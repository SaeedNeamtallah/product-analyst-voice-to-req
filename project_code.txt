================================================================================
  PROJECT CODE DUMP
  Generated: 2026-02-27 21:25:54
  Root: C:\Users\saeid\OneDrive\Desktop\product-analyst-voice-to-req
  Total files: 84
================================================================================

TABLE OF CONTENTS
----------------------------------------
    1. .env.example
    2. README.md
    3. app_config.json
    4. bot_config.json
    5. check_extension.py
    6. create_database.sql
    7. docker-compose.yml
    8. health_check.json
    9. health_check_utf8.json
   10. init-db.sql
   11. list_models.py
   12. reproduce_issue.py
   13. restore_env.py
   14. setup.bat
   15. start.bat
   16. start_backend.bat
   17. start_backend.ps1
   18. start_docker.bat
   19. start_telegram_bot.bat
   20. stop_docker.bat
   21. stop_telegram_bot.bat
   22. tempCodeRunnerFile.bat
   23. test_live_patch.py
   24. update_token.py
   25. backend\__init__.py
   26. backend\config.py
   27. backend\errors.py
   28. backend\init_database.py
   29. backend\main.py
   30. backend\requirements.txt
   31. backend\runtime_config.py
   32. backend\controllers\__init__.py
   33. backend\controllers\project_controller.py
   34. backend\controllers\query_controller.py
   35. backend\database\__init__.py
   36. backend\database\connection.py
   37. backend\database\models.py
   38. backend\providers\__init__.py
   39. backend\providers\llm\__init__.py
   40. backend\providers\llm\cohere_provider.py
   41. backend\providers\llm\factory.py
   42. backend\providers\llm\gemini_provider.py
   43. backend\providers\llm\hf_bge_m3_provider.py
   44. backend\providers\llm\interface.py
   45. backend\providers\llm\openai_compat_provider.py
   46. backend\providers\llm\voyage_provider.py
   47. backend\routes\__init__.py
   48. backend\routes\app_config.py
   49. backend\routes\auth.py
   50. backend\routes\bot_config.py
   51. backend\routes\documents.py
   52. backend\routes\handoff.py
   53. backend\routes\health.py
   54. backend\routes\interview.py
   55. backend\routes\judge.py
   56. backend\routes\messages.py
   57. backend\routes\projects.py
   58. backend\routes\query.py
   59. backend\routes\srs.py
   60. backend\routes\stats.py
   61. backend\routes\stt.py
   62. backend\services\__init__.py
   63. backend\services\agent_telemetry.py
   64. backend\services\answer_service.py
   65. backend\services\constraints_checker.py
   66. backend\services\file_service.py
   67. backend\services\interview_service.py
   68. backend\services\judging_service.py
   69. backend\services\live_patch_service.py
   70. backend\services\resilience_service.py
   71. backend\services\runtime_metrics.py
   72. backend\services\srs_pdf_html_renderer.py
   73. backend\services\srs_service.py
   74. backend\services\srs_snapshot_cache.py
   75. backend\services\stt_service.py
   76. backend\services\telemetry_service.py
   77. backend\tools\package.json
   78. frontend\app.js
   79. frontend\index.html
   80. frontend\style.css
   81. telegram_bot\__init__.py
   82. telegram_bot\bot.py
   83. telegram_bot\config.py
   84. telegram_bot\handlers.py

================================================================================

################################################################################
# FILE: .env.example
################################################################################

# Tawasul Configuration Template
# Copy this file to .env and modify the values

# ========================================
# Database Configuration (Docker)
# ========================================
# Default Docker database URL (no changes needed if using docker-compose)
DATABASE_URL=postgresql+asyncpg://tawasul:tawasul123@localhost:5555/tawasul

# ========================================
# LLM Provider Configuration
# ========================================
# Get your API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=YOUR_GEMINI_API_KEY_HERE
LLM_PROVIDER=gemini
GEMINI_MODEL=gemini-2.5-flash
GEMINI_LITE_MODEL=gemini-2.5-lite-flash
OPENROUTER_API_KEY=
GROQ_API_KEY=
CEREBRAS_API_KEY=

# ========================================
# Storage Configuration
# ========================================
UPLOAD_DIR=./uploads
MAX_FILE_SIZE_MB=50

# ========================================
# API Configuration
# ========================================
API_HOST=127.0.0.1
API_PORT=8500
API_TITLE=Tawasul API
API_VERSION=1.0.0

# ========================================
# Telegram Bot Configuration (Optional)
# ========================================
# Get bot token from @BotFather on Telegram
TELEGRAM_BOT_TOKEN=
# Your Telegram user ID (get from @userinfobot)
TELEGRAM_ADMIN_ID=

# ========================================
# CORS Configuration
# ========================================
CORS_ORIGINS=["http://localhost:3000", "http://localhost:8500", "http://127.0.0.1:3000", "http://127.0.0.1:8500"]

# ========================================
# Logging Configuration
# ========================================
# Options: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# ========================================
# Authentication (JWT)
# ========================================
# IMPORTANT: Set a stable secret (tokens break after restart if JWT_SECRET is empty)
JWT_SECRET=CHANGE_ME_TO_A_LONG_RANDOM_SECRET
JWT_ALGORITHM=HS256
JWT_EXPIRY_HOURS=72

# ========================================
# Speech-to-Text Configuration (Optional)
# ========================================
# Groq API key for Whisper STT (also used by optional LLM provider)
STT_MAX_FILE_SIZE_MB=500

################################################################################
# FILE: README.md
################################################################################

# ðŸ“˜ Tawasul (SpecWise AI) Documentation

**Version:** 1.0.0  
**Last Updated:** February 2026  
**Classification:** Technical Architecture & System Design (Confidential)

---

## 1) Vision

**Tawasul** is a voice-first AI agent platform that automates software requirement elicitation for non-technical stakeholders. It transforms free-form text/audio conversations into structured, evolving SRS outputs.

The platform is designed to:
- Reduce dependency on traditional manual requirement interviews.
- Extract requirements incrementally through intelligent clarification loops.
- Support Arabic (including Egyptian dialect) and English.
- Provide a unified workflow across web and Telegram channels.

---

## 2) Product Scope

Current product scope includes:
- User and project management.
- Interactive interview loop with continuously updated SRS drafts.
- Voice input through STT with post-processing correction.
- SRS export to HTML/PDF.
- Multi-session Telegram bot integration.

---

## 3) Tech Stack

### Backend
- Python 3.10+
- FastAPI (async)
- SQLAlchemy Async + asyncpg

### Data & Storage
- PostgreSQL (core entities)
- Local/S3 object storage for uploaded assets

### AI Layer
- LLM Factory with multiple providers:
  - Gemini
  - OpenRouter
  - Groq
  - Cerebras
- STT providers:
  - Groq Whisper
  - OpenAI Whisper
- LLM post-processing for STT cleanup

### Frontend & Channels
- Vanilla JavaScript + HTML + CSS
- Telegram bot (pyTelegramBotAPI)

---

## 4) High-Level Architecture

The system is implemented as a **modular monolith**, with a clear path to service decomposition when scaling requires it.

### Core Layers
1. **API Layer (FastAPI Routers)**
   - auth, projects, documents, query, interview, stt, srs, messages, judge, stats, health, handoff
2. **Service Layer**
   - interview orchestration, SRS lifecycle, resilience/failover, telemetry, locking
3. **Provider Layer**
   - LLM provider factory + OpenAI-compatible providers
4. **Persistence Layer**
  - PostgreSQL models (single source of truth)
5. **Channel Layer**
   - Web client + Telegram bot sharing the same backend business logic

---

## 5) Core Modules

### 5.1 Interview Orchestrator
**File:** `backend/services/interview_service.py`

- Core analytical and orchestration engine.
- Drives stateful interview progression through stages:
  - discovery
  - scope
  - users
  - features
  - constraints
- Uses patch-like partial updates to SRS drafts instead of full rewrites.
- Integrates resilient multi-provider LLM execution with failover.

### 5.2 LLM Provider Factory
**File:** `backend/providers/llm/factory.py`

- Decouples business logic from a single LLM vendor.
- Selects providers dynamically based on configuration and key availability.
- Supports fallback behavior through `run_with_failover` patterns.

### 5.3 STT Pipeline
**File:** `backend/routes/stt.py`

Processing flow:
1. Receive audio input.
2. Transcribe audio via Whisper provider.
3. Pass raw transcript to an LLM post-processor (low temperature) for cleanup.
4. Return cleaned text into the interview flow.

### 5.4 Document Processing
**Files:**
- `backend/routes/documents.py`
- `backend/services/file_service.py`

- Upload and manage project-linked assets.
- Extract text from supported document formats.
- Support local or S3-backed storage.

### 5.5 Telegram Multi-Session Bot
**Files:**
- `telegram_bot/bot.py`
- `telegram_bot/handlers.py`

- Adds a production channel fully integrated with backend flows.
- Uses local instance locking to prevent duplicate bot processes on one machine.

---

## 6) Data Workflow

### A) Elicitation Loop
1. **Input**: user text or voice.
2. **(Optional) STT Cleanup**: normalize noisy transcript output.
3. **Context**: load latest SRS draft + chat history.
4. **LLM Turn**: generate clarification response + state updates.
5. **Persist**: store chat turn and update draft/version.
6. **Return**: deliver next prompt/question to continue elicitation.

### B) SRS Export Loop
1. Aggregate final structured draft content.
2. Apply engineering-style formatting.
3. Generate HTML then PDF.

---

## 7) Database Schema Highlights

**Reference file:** `backend/database/models.py`

Primary tables in the current implementation:
- `users`: authentication and user profile data
- `projects`: top-level container for requirement elicitation lifecycle
- `assets`: uploaded documents and processing status
- `chat_messages`: project-level conversation history
- `srs_drafts`: versioned SRS draft snapshots

> Note: Some older design artifacts may reference logical entities such as `Requirement` or `Document`. In the current codebase, those concerns are represented through `srs_drafts` and `assets`.

---

## 8) Reliability & Resilience

### Failover
- LLM/STT calls support automatic fallback across configured providers.
- If one provider fails or is rate-limited, execution shifts to alternatives.

### Backend-managed State
- Runtime and interview state are persisted in PostgreSQL.
- This avoids cross-worker drift and removes external cache coupling.

### Observability
- Middleware injects `X-Request-ID` and `X-Response-Time-ms`.
- `/metrics` endpoint is Prometheus-compatible when enabled.

---

## 9) Risk Matrix (Compact FMEA)

| Domain | Risk | Mitigation Strategy |
|---|---|---|
| Interview State | Concurrent update collisions | PostgreSQL row locking + backend-managed state |
| LLM Quality | Hallucination or over-generalization | Incremental update style + context grounding + low temperature where needed |
| STT Accuracy | Dialect/noise/transcription drift | Post-processing layer before downstream analysis |
| Provider Availability | Single-vendor outage | Multi-provider failover |
| Runtime Consistency | Worker state divergence | DB-backed state and transactional updates |

---

## 10) Local Runbook

### Prerequisites
- Python 3.10+
- Docker Desktop
- PowerShell (Windows)

### Quick Start
- Start everything: `start.bat`
- Start Docker services only: `start_docker.bat`
- Start backend: `start_backend.bat`
- Start Telegram bot: `start_telegram_bot.bat`

### Current Docker Services
**File:** `docker-compose.yml`
- PostgreSQL on host port `5555` (default)

---

## 11) Environment Variables

Baseline example:

```env
# Core
DATABASE_URL=postgresql+asyncpg://tawasul:tawasul123@localhost:5555/tawasul

# LLM / STT
LLM_PROVIDER=gemini
GEMINI_API_KEY=...
OPENROUTER_API_KEY=...
GROQ_API_KEY=...
CEREBRAS_API_KEY=...
OPENAI_API_KEY=...

# App
API_HOST=127.0.0.1
API_PORT=8500
ENVIRONMENT=development
JWT_SECRET=...

# Telegram
TELEGRAM_BOT_TOKEN=...
```

> Full supported configuration keys are defined in `backend/config.py`.

---

## 12) Main REST Surface

- Health: `/health`
- Auth: `/auth/*`
- Projects: `/projects/*`
- Documents: `/projects/{project_id}/documents*`
- Interview: `/interview/*`
- STT: `/stt/*`
- SRS: `/srs/*`
- Metrics: `/metrics`

> See `backend/main.py` for the currently enabled router list.

---

## 13) Project Structure (Condensed)

```text
backend/
  main.py
  config.py
  database/
    models.py
  routes/
    auth.py, projects.py, documents.py, query.py, interview.py, stt.py, srs.py, ...
  services/
    interview_service.py, srs_service.py, stt_service.py, resilience_service.py, ...
  providers/llm/
    factory.py, gemini_provider.py, openai_compat_provider.py, ...
frontend/
  index.html, app.js, style.css
telegram_bot/
  bot.py, handlers.py, config.py
docker-compose.yml
start*.bat
```

---

## 14) Important Architecture Notes

- This repository reflects a product-oriented implementation with clear extensibility points.
- Some earlier presentations may mention Qdrant or separate RAG services; the current active path is centered on:
  - PostgreSQL (single source of truth)
  - Document extraction + conversational elicitation
  - LLM/STT failover
- Future RAG/vector capabilities can be added as an additional layer on top of the existing interview orchestration.

---

## 15) Suggested Engineering Roadmap

1. Formalize migration workflows (stronger Alembic + CI checks).
2. Expand integration tests for interview/STT critical paths.
3. Introduce worker queue isolation for heavy document processing.
4. Unify distributed tracing (OpenTelemetry) across key paths.
5. Publish deeper OpenAPI consumer documentation.

---

## 16) Executive Summary

**Tawasul** is not just a chatbot; it is an **agentic requirements platform** combining:
- multi-stage requirement elicitation,
- practical Arabic voice processing,
- resilient multi-provider AI execution,
- and fast local operability with enterprise growth potential.

This document serves as a single technical reference for engineering teams, technical leadership, and investment stakeholders.

################################################################################
# FILE: app_config.json
################################################################################

{
  "llm_provider": "groq-llama-3.3-70b-versatile",
  "embedding_provider": "gemini",
  "retrieval_top_k": 20,
  "chunk_strategy": "parent_child",
  "chunk_size": 600,
  "chunk_overlap": 100,
  "parent_chunk_size": 4000,
  "parent_chunk_overlap": 600,
  "retrieval_candidate_k": 20,
  "retrieval_hybrid_enabled": true,
  "retrieval_hybrid_alpha": 0.7,
  "retrieval_rerank_enabled": true,
  "retrieval_rerank_top_k": 10,
  "query_rewrite_enabled": true,
  "voyage_output_dimension": 1024
}

################################################################################
# FILE: bot_config.json
################################################################################

{"active_project_id": 3}

################################################################################
# FILE: check_extension.py
################################################################################

import psycopg2
from backend.config import settings

def check_extension():
    db_url = settings.database_url.replace("postgresql+asyncpg://", "")
    auth, rest = db_url.split("@")
    user, password = auth.split(":")
    host_port, db_name = rest.split("/")
    host = host_port.split(":")[0]
    port = host_port.split(":")[1] if ":" in host_port else "5432"
    
    try:
        conn = psycopg2.connect(
            dbname=db_name,
            user=user,
            password=password,
            host=host,
            port=port
        )
        cur = conn.cursor()
        cur.execute("SELECT * FROM pg_available_extensions WHERE name = 'vector'")
        extension = cur.fetchone()
        if extension:
            print(f"Extension 'vector' is available. Version: {extension[2]}")
        else:
            print("Extension 'vector' is NOT available in pg_available_extensions.")
        cur.close()
        conn.close()
    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    check_extension()

################################################################################
# FILE: create_database.sql
################################################################################

-- Tawasul Database Creation Script
-- Run this in PostgreSQL before starting the application

-- Create database
CREATE DATABASE tawasul;

-- Connect to the tawasul database (in psql: \c tawasul)
\c tawasul

################################################################################
# FILE: docker-compose.yml
################################################################################

services:
  postgres:
    image: postgres:16-alpine
    container_name: tawasul-postgres
    environment:
      POSTGRES_USER: tawasul
      POSTGRES_PASSWORD: tawasul123
      POSTGRES_DB: tawasul
      POSTGRES_HOST_AUTH_METHOD: trust
    ports:
      - "${POSTGRES_HOST_PORT:-5555}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U tawasul -d tawasul"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

volumes:
  postgres_data:

################################################################################
# FILE: health_check.json
################################################################################

ï¿½ï¿½{ 
 
         " s t a t u s " :     " h e a l t h y " , 
 
         " d a t a b a s e " :     " c o n n e c t e d " , 
 
         " l l m _ p r o v i d e r " :     " g e m i n i " 
 
 } 
 
 

################################################################################
# FILE: health_check_utf8.json
################################################################################

ï»¿{
    "status":  "healthy",
    "database":  "connected",
    "llm_provider":  "gemini"
}

################################################################################
# FILE: init-db.sql
################################################################################

-- Tawasul Database Initialization Script
-- This script runs automatically when the PostgreSQL container starts

-- Grant privileges
GRANT ALL PRIVILEGES ON DATABASE tawasul TO tawasul;

################################################################################
# FILE: list_models.py
################################################################################

import os
import google.generativeai as genai
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")

if not api_key:
    try:
        from backend.config import settings
        api_key = settings.gemini_api_key
        print(f"Using API key from settings")
    except ImportError:
        print("Could not find API key.")
        exit(1)

genai.configure(api_key=api_key)

print("Listing available models...")
try:
    for m in genai.list_models():
        print(f"Name: {m.name}, Supported Methods: {m.supported_generation_methods}")
except Exception as e:
    print(f"Error listing models: {e}")

################################################################################
# FILE: reproduce_issue.py
################################################################################

import os
import google.generativeai as genai
from dotenv import load_dotenv
import asyncio

# Load environment variables
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")

if not api_key:
    print("Error: GEMINI_API_KEY not found in .env")
    # Try to read from config.py default if not in env
    try:
        from backend.config import settings
        api_key = settings.gemini_api_key
        print(f"Using API key from settings: {api_key[:5]}...")
    except ImportError:
        print("Could not import settings to fallback.")
        exit(1)

genai.configure(api_key=api_key)

async def test_model(model_name):
    print(f"Testing model: {model_name}")
    try:
        model = genai.GenerativeModel(model_name)
        # Run in executor to match provider implementation behavior, though simple generate is sync-ish in some versions
        # but let's just call it directly for simplicity of the script
        response = model.generate_content("Hello")
        print(f"Success! Response: {response.text}")
        return True
    except Exception as e:
        print(f"Failed: {e}")
        return False

async def main():
    print("--- Reproducing Issue ---")
    await test_model("gemma-3-12b-it")

    print("\n--- Verifying Fix ---")
    await test_model("gemma-3-12b-it")

if __name__ == "__main__":
    asyncio.run(main())

################################################################################
# FILE: restore_env.py
################################################################################

with open('.env', 'w', encoding='utf-8') as f:
    f.write('DATABASE_URL=postgresql+asyncpg://tawasul:tawasul123@localhost:5555/tawasul')
    f.write('GEMINI_API_KEY=AIzaSyD2N-rsmfER9P2dZznBh4wXKAFZRajJ0eU\n')
    f.write('LLM_PROVIDER=gemini\n')
    f.write('GEMINI_MODEL=gemini-2.0-flash\n')
    f.write('UPLOAD_DIR=./uploads\n')
    f.write('MAX_FILE_SIZE_MB=50\n')
    f.write('CHUNK_SIZE=30000\n')
    f.write('CHUNK_OVERLAP=200\n')
    f.write('PARENT_CHUNK_SIZE=300000\n')
    f.write('PARENT_CHUNK_OVERLAP=600\n')
    f.write('CHUNK_STRATEGY=parent_child\n')
    f.write('RETRIEVAL_CANDIDATE_K=20\n')
    f.write('RETRIEVAL_HYBRID_ENABLED=false\n')
    f.write('RETRIEVAL_HYBRID_ALPHA=0.7\n')
    f.write('RETRIEVAL_RERANK_ENABLED=false\n')
    f.write('RETRIEVAL_RERANK_TOP_K=10\n')
    f.write('QUERY_REWRITE_ENABLED=false\n')
    f.write('API_HOST=127.0.0.1\n')
    f.write('API_PORT=8500\n')
    f.write('TELEGRAM_BOT_TOKEN=n')
    f.write('TELEGRAM_ADMIN_ID=')
    f.write('CORS_ORIGINS=["http://localhost:3000", "http://localhost:8500"]\n')
    f.write('LOG_LEVEL=INFO\n')

################################################################################
# FILE: setup.bat
################################################################################

@echo off
chcp 65001 >nul
color 0A
echo ========================================
echo    Tawasul - Setup Script
echo    Installing all project dependencies
echo ========================================
echo.

:: Check if uv is installed
uv --version >nul 2>&1
if errorlevel 1 (
    echo [ERROR] uv is not installed on the system!
    echo Please install uv from: https://docs.astral.sh/uv/getting-started/installation/
    echo You can install it with: pip install uv or winget install --id=astral-sh.uv
    pause
    exit /b 1
)

echo [âœ“] uv is installed
uv --version
echo.

:: Check if Docker is available (recommended for database)
echo [INFO] This project uses Docker for PostgreSQL database (recommended)
echo        Install Docker Desktop from: https://www.docker.com/products/docker-desktop
echo.

:: Create virtual environment
echo ========================================
echo 1. Create virtual environment (Virtual Environment)
echo ========================================
if exist "venv\" (
    echo [!] Virtual environment already exists
    choice /C YN /M "Do you want to recreate it?"
    if errorlevel 2 goto skip_venv
    if errorlevel 1 (
        echo [INFO] Removing old virtual environment...
        rmdir /s /q venv
    )
)

echo [INFO] Creating a new virtual environment...
uv venv venv
if errorlevel 1 (
    echo [ERROR] Failed to create virtual environment!
    pause
    exit /b 1
)
echo [âœ“] Virtual environment created successfully
echo.

:skip_venv

:: Activate virtual environment
echo ========================================
echo 2. Activate virtual environment
echo ========================================
call venv\Scripts\activate.bat
if errorlevel 1 (
    echo [ERROR] Failed to activate the virtual environment!
    pause
    exit /b 1
)
echo [âœ“] Virtual environment activated
echo.

:: Install dependencies with uv
echo ========================================
echo 3. Install dependencies with uv (much faster!)
echo ========================================
uv pip install --python venv\Scripts\python.exe -r backend\requirements.txt
if errorlevel 1 (
    echo [ERROR] Failed to install dependencies!
    echo Please check requirements.txt and your internet connection
    pause
    exit /b 1
)
echo [âœ“] All dependencies installed successfully

echo [INFO] Verifying critical Python imports...
venv\Scripts\python.exe -c "import dotenv, pydantic_settings, fastapi, sqlalchemy; print('imports-ok')" >nul 2>&1
if errorlevel 1 (
    echo [WARNING] Dependency verification failed. Reinstalling critical packages in the same venv...
    uv pip install --python venv\Scripts\python.exe --reinstall python-dotenv pydantic-settings fastapi sqlalchemy
    if errorlevel 1 (
        echo [ERROR] Could not repair dependencies in venv.
        echo Please run: venv\Scripts\python.exe -m pip install --force-reinstall python-dotenv pydantic-settings fastapi sqlalchemy
        pause
        exit /b 1
    )
)
echo [âœ“] Dependency verification passed
echo.

:: Create .env file if not exists
echo ========================================
echo 4. Create configuration file (.env)
echo ========================================
if exist ".env" (
    echo [!] .env file already exists
    choice /C YN /M "Do you want to recreate it from the template?"
    if errorlevel 2 goto skip_env
)

if exist ".env.example" (
    echo [INFO] Copying settings from .env.example...
    copy .env.example .env >nul
    echo [âœ“] .env file created
    echo [!] Please edit .env and add:
    echo     - DATABASE_URL
    echo     - GEMINI_API_KEY
    echo     - GROQ_API_KEY (for speech-to-text)
    echo     - JWT_SECRET (change for production!)
    echo     - TELEGRAM_BOT_TOKEN (optional)
) else (
    echo [WARNING] .env.example not found
    echo Please create a .env file manually and add the required settings
)
echo.

:skip_env

:: Create uploads directory
echo ========================================
echo 5. Create project directories
echo ========================================
if not exist "uploads\" mkdir uploads
echo [âœ“] uploads directory created
echo.

:: Database setup with Docker
echo ========================================
echo 6. Database setup (Docker)
echo ========================================
echo.

REM Check if Docker is installed
docker --version >nul 2>&1
if errorlevel 1 (
    echo [WARNING] Docker is not installed!
    echo Please install Docker Desktop from: https://www.docker.com/products/docker-desktop
    echo.
    echo Alternatively, set up PostgreSQL manually:
    echo   1. Install PostgreSQL
    echo   2. Create database: CREATE DATABASE tawasul;
    echo   3. Update DATABASE_URL in .env
    echo.
    goto skip_docker
)

REM Check if Docker is running
docker info >nul 2>&1
if errorlevel 1 (
    echo [WARNING] Docker is not running!
    echo Please start Docker Desktop and run start_docker.bat
    goto skip_docker
)

echo [âœ“] Docker is available
choice /C YN /M "Do you want to start the database with Docker?"
if errorlevel 2 goto skip_docker
if errorlevel 1 (
    echo [INFO] Starting Docker services...
    docker-compose up -d
    if errorlevel 1 (
        echo [ERROR] Failed to start Docker services!
    ) else (
        echo [âœ“] PostgreSQL container started
        echo.
        echo Waiting for database to be ready...
        timeout /t 5 /nobreak >nul
        echo [âœ“] Database is ready
    )
)
echo.

:skip_docker

:: Initialize database
echo ========================================
echo 7. Initialize database tables
echo ========================================
choice /C YN /M "Do you want to initialize the database tables now?"
if errorlevel 2 (
    echo [!] You can initialize the database later with:
    echo     python -m backend.init_database
) else (
    echo [INFO] Initializing database...
    python -m backend.init_database
    if errorlevel 1 (
        echo [ERROR] Failed to initialize the database
        echo Please check:
        echo   - PostgreSQL connection
        echo   - DATABASE_URL in .env
    ) else (
        echo [âœ“] Database initialized successfully
    )
)
echo.

:: Summary
echo ========================================
echo âœ… Setup complete!
echo ========================================
echo.
echo Next steps:
echo ----------------
echo 1. Start the database (if not already running):
echo    - Run: start_docker.bat
echo.
echo 2. Edit the .env file and add:
echo    - GEMINI_API_KEY (required)
echo    - GROQ_API_KEY (for speech-to-text)
echo    - JWT_SECRET (change in production!)
echo    - TELEGRAM_BOT_TOKEN (optional)
echo    - DATABASE_URL is pre-configured for Docker
echo.
echo 3. To run the project:
echo    - Start Backend: start_backend.bat
echo    - Start Telegram Bot: start_telegram_bot.bat (optional)
echo.
echo 4. Open your browser at: http://localhost:3000
echo.
echo ========================================
echo ðŸ“š For more information, see README.md
echo ========================================
echo.
pause

################################################################################
# FILE: start.bat
################################################################################

@echo off
REM Tawasul One-Click Startup (Docker + Backend + Frontend)

echo ========================================
echo    Tawasul One-Click Start
echo ========================================
echo.

REM Start Docker services (Postgres)
set SKIP_DOCKER_PAUSE=1
call start_docker.bat
if errorlevel 1 (
	echo [ERROR] Docker services failed to start.
	pause
	exit /b 1
)

REM Start backend (also starts frontend)
call start_backend.bat
if errorlevel 1 (
	echo [ERROR] Backend failed to start.
	pause
	exit /b 1
)

################################################################################
# FILE: start_backend.bat
################################################################################

@echo off
setlocal
REM Tawasul Backend Startup Script (delegates to PowerShell)

echo ========================================
echo    Tawasul Backend Server
echo ========================================
echo.

set "PS_SCRIPT=%~dp0start_backend.ps1"

if not exist "%PS_SCRIPT%" (
    echo [ERROR] Missing PowerShell startup script:
    echo         %PS_SCRIPT%
    pause
    exit /b 1
)

where powershell >nul 2>&1
if errorlevel 1 (
    echo [ERROR] powershell.exe is not available in PATH.
    pause
    exit /b 1
)

echo [INFO] Launching PowerShell startup flow...
echo [INFO] Frontend mode: legacy frontend/ only
powershell -ExecutionPolicy Bypass -File "%PS_SCRIPT%"
set "EXIT_CODE=%ERRORLEVEL%"

if not "%EXIT_CODE%"=="0" (
    echo.
    echo [ERROR] Startup failed with exit code %EXIT_CODE%.
    pause
)

exit /b %EXIT_CODE%

################################################################################
# FILE: start_backend.ps1
################################################################################

$ErrorActionPreference = 'Stop'

Write-Host '========================================' -ForegroundColor Cyan
Write-Host '   Tawasul Backend Server (PowerShell)' -ForegroundColor Cyan
Write-Host '========================================' -ForegroundColor Cyan
Write-Host ''

function Invoke-NativeChecked {
    param([Parameter(Mandatory = $true)][scriptblock]$Command)

    & $Command
    if ($LASTEXITCODE -ne 0) {
        throw "Command failed with exit code $LASTEXITCODE"
    }
}

function Test-CommandExists {
    param([Parameter(Mandatory = $true)][string]$Name)
    return [bool](Get-Command $Name -ErrorAction SilentlyContinue)
}

function Wait-PostgresReady {
    param(
        [int]$MaxAttempts = 30,
        [int]$SleepSeconds = 2
    )

    for ($attempt = 1; $attempt -le $MaxAttempts; $attempt++) {
        try {
            $postgresContainerId = Get-PostgresContainerId
            $health = docker inspect --format='{{.State.Health.Status}}' $postgresContainerId 2>$null
            if ($LASTEXITCODE -eq 0 -and $health -eq 'healthy') {
                Write-Host '[âœ“] PostgreSQL container is healthy' -ForegroundColor Green
                return
            }
        } catch {
            # ignore and retry
        }

        Write-Host "[INFO] Waiting for PostgreSQL readiness ($attempt/$MaxAttempts)..." -ForegroundColor Yellow
        Start-Sleep -Seconds $SleepSeconds
    }

    throw 'PostgreSQL container did not become healthy in time.'
}

function Wait-PostgresHostPort {
    param(
        [int]$Port,
        [int]$MaxAttempts = 30,
        [int]$SleepSeconds = 2
    )

    for ($attempt = 1; $attempt -le $MaxAttempts; $attempt++) {
        try {
            $ok = Test-NetConnection -ComputerName '127.0.0.1' -Port $Port -WarningAction SilentlyContinue
            if ($ok.TcpTestSucceeded) {
                Write-Host "[OK] PostgreSQL host port 127.0.0.1:$Port is reachable" -ForegroundColor Green
                return
            }
        } catch {
            # ignore and retry
        }

        Write-Host "[INFO] Waiting for host port 127.0.0.1:$Port ($attempt/$MaxAttempts)..." -ForegroundColor Yellow
        Start-Sleep -Seconds $SleepSeconds
    }

    throw "PostgreSQL host port 127.0.0.1:$Port is not reachable."
}

function Wait-LocalPortReady {
    param(
        [int]$Port,
        [int]$MaxAttempts = 30,
        [int]$SleepSeconds = 1
    )

    for ($attempt = 1; $attempt -le $MaxAttempts; $attempt++) {
        try {
            $ok = Test-NetConnection -ComputerName '127.0.0.1' -Port $Port -WarningAction SilentlyContinue
            if ($ok.TcpTestSucceeded) {
                Write-Host "[OK] Local port 127.0.0.1:$Port is reachable" -ForegroundColor Green
                return
            }
        } catch {
            # ignore and retry
        }

        Start-Sleep -Seconds $SleepSeconds
    }

    throw "Port 127.0.0.1:$Port did not become reachable in time."
}

function Ensure-PostgresPortBinding {
    param([int]$ExpectedHostPort)

    $postgresContainerId = Get-PostgresContainerId
    $portsJson = docker inspect $postgresContainerId --format='{{json .NetworkSettings.Ports}}' 2>$null
    if ($LASTEXITCODE -ne 0 -or [string]::IsNullOrWhiteSpace($portsJson)) {
        throw 'Could not inspect postgres container network settings.'
    }

    $ports = $portsJson | ConvertFrom-Json
    $boundEntries = $ports.'5432/tcp'
    $hasExpectedBinding = $false

    if ($boundEntries) {
        foreach ($entry in $boundEntries) {
            if ($entry.HostPort -eq "$ExpectedHostPort") {
                $hasExpectedBinding = $true
                break
            }
        }
    }

    if (-not $hasExpectedBinding) {
        Write-Host "[WARN] postgres service is missing host port binding on $ExpectedHostPort. Recreating postgres service..." -ForegroundColor Yellow
        Invoke-NativeChecked { docker compose up -d --force-recreate --no-deps postgres }
    }
}

function Get-PostgresContainerId {
    $composeRaw = docker compose ps -q postgres 2>$null | Select-Object -First 1
    $containerId = if ($null -eq $composeRaw) { '' } else { "$composeRaw".Trim() }
    if ($LASTEXITCODE -eq 0 -and -not [string]::IsNullOrWhiteSpace($containerId)) {
        return $containerId
    }

    $fallbackRaw = docker ps --filter "name=tawasul-postgres" --format "{{.ID}}" 2>$null | Select-Object -First 1
    $containerId = if ($null -eq $fallbackRaw) { '' } else { "$fallbackRaw".Trim() }
    if (-not [string]::IsNullOrWhiteSpace($containerId)) {
        return $containerId
    }

    throw 'Could not resolve postgres container ID from docker compose.'
}

function Get-DatabaseHostPort {
    $databaseUrl = $env:DATABASE_URL

    if ([string]::IsNullOrWhiteSpace($databaseUrl) -and (Test-Path '.env')) {
        $line = Get-Content '.env' | Where-Object { $_ -match '^\s*DATABASE_URL\s*=' } | Select-Object -First 1
        if ($line) {
            $databaseUrl = ($line -split '=', 2)[1].Trim()
        }
    }

    if ([string]::IsNullOrWhiteSpace($databaseUrl)) {
        return 5555
    }

    if ($databaseUrl -match '@[^:]+:(\d+)/') {
        return [int]$Matches[1]
    }

    return 5432
}

function Test-LocalPortAvailable {
    param([int]$Port)

    try {
        $listener = [System.Net.Sockets.TcpListener]::new([System.Net.IPAddress]::Loopback, $Port)
        $listener.Start()
        $listener.Stop()
        return $true
    } catch {
        return $false
    }
}

function Get-AvailablePort {
    param([int[]]$Candidates)

    foreach ($candidate in $Candidates) {
        if (Test-LocalPortAvailable -Port $candidate) {
            return $candidate
        }
    }

    throw 'No available PostgreSQL host port found in candidate list.'
}

function Set-DatabaseUrlPort {
    param(
        [string]$DatabaseUrl,
        [int]$Port
    )

    if ([string]::IsNullOrWhiteSpace($DatabaseUrl)) {
        $dbUser = if ([string]::IsNullOrWhiteSpace($env:POSTGRES_USER)) { 'tawasul' } else { $env:POSTGRES_USER }
        $dbPass = if ([string]::IsNullOrWhiteSpace($env:POSTGRES_PASSWORD)) { '' } else { $env:POSTGRES_PASSWORD }
        $auth = if ([string]::IsNullOrWhiteSpace($dbPass)) { $dbUser } else { "{0}:{1}" -f $dbUser, $dbPass }
        return "postgresql+asyncpg://$auth@localhost:$Port/tawasul"
    }

    if ($DatabaseUrl -match '@([^/:]+):(\d+)/(.*)$') {
        $dbHost = $Matches[1]
        $replacement = "@{0}:{1}/" -f $dbHost, $Port
        return ($DatabaseUrl -replace '@[^/]+/', $replacement)
    }

    return $DatabaseUrl
}

function Get-DatabaseUrlRaw {
    $databaseUrl = $env:DATABASE_URL
    if (-not [string]::IsNullOrWhiteSpace($databaseUrl)) {
        return $databaseUrl
    }

    if (Test-Path '.env') {
        $line = Get-Content '.env' | Where-Object { $_ -match '^\s*DATABASE_URL\s*=' } | Select-Object -First 1
        if ($line) {
            return ($line -split '=', 2)[1].Trim()
        }
    }

    return ''
}

function Get-DatabaseIdentityFromUrl {
    param([string]$DatabaseUrl)

    if ([string]::IsNullOrWhiteSpace($DatabaseUrl)) {
        return $null
    }

    $pattern = 'postgresql\+asyncpg://([^:]+):([^@]+)@[^/]+/([^\s]+)'
    if ($DatabaseUrl -notmatch $pattern) {
        return $null
    }

    return @{
        User = $Matches[1]
        Password = $Matches[2]
        Database = $Matches[3]
    }
}

function Invoke-PostgresQuery {
    param(
        [Parameter(Mandatory = $true)][string]$ContainerId,
        [Parameter(Mandatory = $true)][string]$User,
        [Parameter(Mandatory = $true)][AllowEmptyString()][string]$Password,
        [Parameter(Mandatory = $true)][string]$Query
    )

    $escapedQuery = $Query.Replace('"', '""')
    $passwordEnvPart = if ([string]::IsNullOrEmpty($Password)) { '' } else { "-e PGPASSWORD=$Password" }
    $commandLine = "docker exec $passwordEnvPart $ContainerId psql -U $User -d postgres -tAc `"$escapedQuery`""

    $previousErrorActionPreference = $ErrorActionPreference
    try {
        $ErrorActionPreference = 'Continue'
        $output = cmd /c "$commandLine 2>nul"
        $exitCode = $LASTEXITCODE
    }
    finally {
        $ErrorActionPreference = $previousErrorActionPreference
    }

    if ($null -ne $output) {
        $output = @($output | Where-Object {
            $line = "$_"
            ($line -notmatch 'has no actual collation version') -and ($line -notmatch '^WARNING:')
        })
    }

    if ($exitCode -ne 0) {
        return $null
    }

    return "$output".Trim()
}

function Ensure-DatabaseIdentity {
    param([string]$DatabaseUrl)

    $identity = Get-DatabaseIdentityFromUrl -DatabaseUrl $DatabaseUrl
    if ($null -eq $identity) {
        Write-Host '[WARN] Could not parse DATABASE_URL for role/db bootstrap. Skipping identity check.' -ForegroundColor Yellow
        return
    }

    $containerId = Get-PostgresContainerId
    $desiredUser = $identity.User
    $desiredPassword = $identity.Password
    $desiredDatabase = $identity.Database

    $adminCandidates = @(
        @{ User = $desiredUser; Password = $desiredPassword },
        @{ User = 'tawasul'; Password = 'tawasul123' },
        @{ User = 'postgres'; Password = 'postgres' },
        @{ User = 'postgres'; Password = '' }
    )

    $authenticatedAccounts = @()
    foreach ($candidate in $adminCandidates) {
        $probe = Invoke-PostgresQuery -ContainerId $containerId -User $candidate.User -Password $candidate.Password -Query 'SELECT 1'
        if ($probe -eq '1') {
            $authenticatedAccounts += ,$candidate
        }
    }

    if ($authenticatedAccounts.Count -eq 0) {
        Write-Host '[WARN] Could not authenticate to PostgreSQL for bootstrap. Skipping role/db auto-fix.' -ForegroundColor Yellow
        return
    }

    $adminAccount = $authenticatedAccounts[0]

    $desiredUserLiteral = $desiredUser.Replace("'", "''")
    $desiredDatabaseLiteral = $desiredDatabase.Replace("'", "''")

    $roleExists = Invoke-PostgresQuery -ContainerId $containerId -User $adminAccount.User -Password $adminAccount.Password -Query "SELECT 1 FROM pg_roles WHERE rolname = '$desiredUserLiteral'"
    if ($roleExists -ne '1') {
        $escapedPasswordSql = $desiredPassword.Replace("'", "''")
        $createRole = "CREATE ROLE `"$desiredUser`" LOGIN PASSWORD '$escapedPasswordSql'"
        $created = $false
        foreach ($candidate in $authenticatedAccounts) {
            $createResult = Invoke-PostgresQuery -ContainerId $containerId -User $candidate.User -Password $candidate.Password -Query $createRole
            if ($null -ne $createResult) {
                $created = $true
                $adminAccount = $candidate
                break
            }
        }

        if (-not $created) {
            $triedUsers = (($authenticatedAccounts | ForEach-Object { $_.User }) -join ', ')
            throw "Failed to create PostgreSQL role '$desiredUser'. Tried authenticated users: $triedUsers"
        }
        Write-Host "[INFO] Created PostgreSQL role '$desiredUser'." -ForegroundColor Yellow
    }

    $dbExists = Invoke-PostgresQuery -ContainerId $containerId -User $adminAccount.User -Password $adminAccount.Password -Query "SELECT 1 FROM pg_database WHERE datname = '$desiredDatabaseLiteral'"
    if ($dbExists -ne '1') {
        $createDb = "CREATE DATABASE `"$desiredDatabase`" OWNER `"$desiredUser`""
        $createdDb = $false
        foreach ($candidate in $authenticatedAccounts) {
            $createDbResult = Invoke-PostgresQuery -ContainerId $containerId -User $candidate.User -Password $candidate.Password -Query $createDb
            if ($null -ne $createDbResult) {
                $createdDb = $true
                $adminAccount = $candidate
                break
            }
        }

        if (-not $createdDb) {
            $triedUsers = (($authenticatedAccounts | ForEach-Object { $_.User }) -join ', ')
            throw "Failed to create PostgreSQL database '$desiredDatabase'. Tried authenticated users: $triedUsers"
        }
        Write-Host "[INFO] Created PostgreSQL database '$desiredDatabase' (owner '$desiredUser')." -ForegroundColor Yellow
    }
}

if (-not (Test-CommandExists -Name 'docker')) {
    throw 'Docker is not installed. Please install Docker Desktop first.'
}

try {
    docker info | Out-Null
} catch {
    throw 'Docker Desktop is not running. Start Docker Desktop and retry.'
}

Write-Host '[INFO] Resolving PostgreSQL host port...' -ForegroundColor Yellow
$databaseUrlRaw = Get-DatabaseUrlRaw
$requestedDbPort = Get-DatabaseHostPort
$fallbackPorts = @(5544, 5545, 5546, 5547, 5550) + (5700..5720)
$dbPort = Get-AvailablePort -Candidates (@($requestedDbPort) + $fallbackPorts)
if ($dbPort -ne $requestedDbPort) {
    Write-Host "[WARN] Requested PostgreSQL host port $requestedDbPort is not available. Using $dbPort instead." -ForegroundColor Yellow
}

$env:POSTGRES_HOST_PORT = "$dbPort"
$env:DATABASE_URL = Set-DatabaseUrlPort -DatabaseUrl $databaseUrlRaw -Port $dbPort

Write-Host '[INFO] Ensuring Docker services are running...' -ForegroundColor Yellow
Invoke-NativeChecked { docker compose up -d }

Ensure-PostgresPortBinding -ExpectedHostPort $dbPort
Wait-PostgresReady
Wait-PostgresHostPort -Port $dbPort
Ensure-DatabaseIdentity -DatabaseUrl $env:DATABASE_URL
Write-Host ''

if (-not (Test-CommandExists -Name 'uv')) {
    throw 'uv is not installed. Install from https://docs.astral.sh/uv/getting-started/installation/'
}

if (-not (Test-Path -Path 'venv')) {
    Write-Host '[INFO] Creating virtual environment...' -ForegroundColor Yellow
    Invoke-NativeChecked { uv venv venv }
}

Write-Host '[INFO] Activating virtual environment...' -ForegroundColor Yellow
& .\venv\Scripts\Activate.ps1

Write-Host '[INFO] Installing/updating backend dependencies...' -ForegroundColor Yellow
Invoke-NativeChecked { uv pip install --python .\venv\Scripts\python.exe -r .\backend\requirements.txt }

Write-Host '[INFO] Verifying critical imports...' -ForegroundColor Yellow
Invoke-NativeChecked { .\venv\Scripts\python.exe -c "import dotenv, pydantic_settings, fastapi, sqlalchemy; print('imports-ok')" }

Write-Host '[INFO] Initializing database schema...' -ForegroundColor Yellow
Invoke-NativeChecked { python .\backend\init_database.py }

Write-Host '[INFO] Cleaning old listeners on ports 8500 and 3000...' -ForegroundColor Yellow
$ports = @(8500, 3000)
foreach ($port in $ports) {
    $connections = Get-NetTCPConnection -LocalPort $port -State Listen -ErrorAction SilentlyContinue
    foreach ($conn in $connections) {
        try {
            Stop-Process -Id $conn.OwningProcess -Force -ErrorAction SilentlyContinue
        } catch {
            # ignore failures for already terminated processes
        }
    }
}

Write-Host '[INFO] Starting frontend on http://localhost:3000' -ForegroundColor Yellow
$legacyFrontendPath = Join-Path $PSScriptRoot 'frontend'
Start-Process powershell -ArgumentList '-NoExit', '-Command', "Set-Location -LiteralPath '$legacyFrontendPath'; python -m http.server 3000"
Write-Host '[âœ“] Legacy frontend started from frontend/ (python http.server).' -ForegroundColor Green

Wait-LocalPortReady -Port 3000

Start-Process 'http://localhost:3000'

Write-Host ''
Write-Host 'Starting FastAPI server on http://127.0.0.1:8500' -ForegroundColor Green
Write-Host 'Docs: http://127.0.0.1:8500/docs' -ForegroundColor Green
python -m uvicorn backend.main:app --host 127.0.0.1 --port 8500

################################################################################
# FILE: start_docker.bat
################################################################################

@echo off
REM Tawasul Docker Services Startup Script

echo ========================================
echo    Tawasul - Docker Services
echo ========================================
echo.

REM Check if Docker is installed
docker --version >nul 2>&1
if errorlevel 1 (
    echo [ERROR] Docker is not installed!
    echo Please install Docker Desktop from: https://www.docker.com/products/docker-desktop
    pause
    exit /b 1
)

REM Check if Docker is running
docker info >nul 2>&1
if errorlevel 1 (
    echo [ERROR] Docker is not running!
    echo Please start Docker Desktop and try again.
    pause
    exit /b 1
)

echo [âœ“] Docker is running
echo.

REM Clean up any existing containers to avoid name conflicts
echo [INFO] Stopping any old Tawasul containers...
docker compose down >nul 2>&1

REM Also remove orphan containers with the same names (from previous runs or manual creates)
for %%C in (tawasul-postgres) do (
    docker rm -f %%C >nul 2>&1
)

REM Start services
echo Starting PostgreSQL container...
docker compose up -d

if errorlevel 1 (
    echo [ERROR] Failed to start Docker services!
    pause
    exit /b 1
)

echo.
echo ========================================
echo [âœ“] Docker services started successfully!
echo ========================================
echo.
echo Services running:
echo   - PostgreSQL: localhost:5555
echo.
echo Database connection string:
echo   postgresql://tawasul:tawasul123@localhost:5555/tawasul
echo.
echo To stop services: stop_docker.bat
echo To view logs: docker-compose logs -f
echo.
if not defined SKIP_DOCKER_PAUSE (
    pause
)

################################################################################
# FILE: start_telegram_bot.bat
################################################################################

@echo off
setlocal
REM Tawasul Telegram Bot Startup Script

set "SCRIPT_DIR=%~dp0"

echo ========================================
echo    Tawasul Telegram Bot
echo ========================================
echo.

REM Activate virtual environment
set "VENV_DIR="
if exist "%SCRIPT_DIR%.venv-3\Scripts\activate.bat" set "VENV_DIR=.venv-3"
if not defined VENV_DIR if exist "%SCRIPT_DIR%.venv-2\Scripts\activate.bat" set "VENV_DIR=.venv-2"
if not defined VENV_DIR if exist "%SCRIPT_DIR%venv\Scripts\activate.bat" set "VENV_DIR=venv"

if not defined VENV_DIR (
    echo ERROR: Virtual environment not found!
    echo Expected one of: .venv-3 or .venv-2 or venv
    echo Please create a virtual environment first.
    pause
    exit /b 1
)

echo Activating virtual environment: %VENV_DIR%
call "%SCRIPT_DIR%%VENV_DIR%\Scripts\activate.bat"
echo.

set "PYTHON_EXE=%SCRIPT_DIR%%VENV_DIR%\Scripts\python.exe"
if not exist "%PYTHON_EXE%" (
    echo ERROR: Python executable not found at:
    echo %PYTHON_EXE%
    pause
    exit /b 1
)

REM Check if bot token is configured
echo Checking configuration...
"%PYTHON_EXE%" -c "from telegram_bot.config import bot_settings; assert bot_settings.telegram_bot_token != 'your_telegram_bot_token_here', 'Please configure TELEGRAM_BOT_TOKEN in .env file'" 2>nul
if errorlevel 1 (
    echo.
    echo ERROR: Telegram bot token not configured!
    echo Please set TELEGRAM_BOT_TOKEN in the .env file
    echo.
    pause
    exit /b 1
)

REM Start bot
echo Starting Telegram bot...
echo.
python -m telegram_bot.bot

################################################################################
# FILE: stop_docker.bat
################################################################################

@echo off
REM Tawasul Docker Services Stop Script

echo ========================================
echo    Tawasul - Stop Docker Services
echo ========================================
echo.

REM Check if Docker is running
docker info >nul 2>&1
if errorlevel 1 (
    echo [ERROR] Docker is not running!
    pause
    exit /b 1
)

echo Stopping Docker services...
docker-compose down

if errorlevel 1 (
    echo [ERROR] Failed to stop Docker services!
    pause
    exit /b 1
)

echo.
echo ========================================
echo [âœ“] Docker services stopped successfully!
echo ========================================
echo.
echo Note: Data is preserved in Docker volumes.
echo To remove all data: docker-compose down -v
echo.
pause

################################################################################
# FILE: stop_telegram_bot.bat
################################################################################

@echo off
setlocal

echo ========================================
echo    Stop Tawasul Telegram Bot
echo ========================================
echo.

powershell -NoProfile -ExecutionPolicy Bypass -Command "$procs = Get-CimInstance Win32_Process | Where-Object { $_.Name -match 'python' -and $_.CommandLine -like '*telegram_bot.bot*' }; if(-not $procs){ Write-Host 'No running telegram bot process found.'; exit 0 }; $count = 0; foreach($p in $procs){ try { Stop-Process -Id $p.ProcessId -Force -ErrorAction Stop; $count++; Write-Host ('Stopped PID ' + $p.ProcessId) } catch { Write-Host ('Failed to stop PID ' + $p.ProcessId) } }; Write-Host ('Stopped ' + $count + ' bot process(es).')"

echo.
echo Done.

################################################################################
# FILE: tempCodeRunnerFile.bat
################################################################################

"%PYTHON_EXE%" -m telegram_bot.bot

################################################################################
# FILE: test_live_patch.py
################################################################################

import asyncio
import json
import os
from pprint import pprint

from backend.database.models import ChatMessage
from backend.services.live_patch_service import LivePatchService, LivePatchExtraction

async def main():
    messages = [
        ChatMessage(role="assistant", content="ØªÙ…Ø§Ù…ØŒ ÙÙ‡Ù…Øª Ø¥Ù†Ùƒ Ù…Ø­ØªØ§Ø¬ Ù…ÙˆÙ‚Ø¹ Ù„Ù…Ø·Ø¹Ù…Ùƒ. Ø¹Ø´Ø§Ù† Ø£Ù‚Ø¯Ø± Ø£Ø·ÙˆÙ† Ù…Ø¹Ø§ÙƒØŒ Ø¥ÙŠÙ‡ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…Ø·Ø¹Ù…ØŸ"),
        ChatMessage(role="user", content='''Ø¹Ø´Ø§Ù† ØªØ¨Ù†ÙŠ Ù…Ø´Ø±ÙˆØ¹ Ù…Ø·Ø¹Ù… Ù†Ø§Ø¬Ø­ØŒ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø¨ØªÙ†Ù‚Ø³Ù… Ù„ÙƒØ°Ø§ Ø¬Ø§Ù†Ø¨ Ø£Ø³Ø§Ø³ÙŠ:
1. Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©: Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ ÙˆØ§Ù„Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠØ©.
2. Ø§Ù„ØªØ´ØºÙŠÙ„ÙŠØ©: Ù…Ø¹Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ø¨Ø®ØŒ Ù†Ø¸Ø§Ù… POSØŒ ÙƒØ±Ø§Ø³ÙŠ Ù…Ø±ÙŠØ­Ø©.
3. Ø§Ù„Ø¨Ø´Ø±ÙŠØ©: Ø§Ù„Ø´ÙŠÙ Ø§Ù„Ø¹Ù…ÙˆÙ…ÙŠØŒ Ø·Ø§Ù‚Ù… Ø§Ù„ØµØ§Ù„Ø©.
4. Ø§Ù„Ø¬ÙˆØ¯Ø©: Ù†Ø¸Ø§Ù… HACCPØŒ Ø³ÙŠØ³ØªÙ… ØªÙˆØ±ÙŠØ¯.
5. Ø§Ù„ØªØ³ÙˆÙŠÙ‚: Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø¨ØµØ±ÙŠØ©ØŒ Ø¥Ù†Ø³ØªØ¬Ø±Ø§Ù… ÙˆÙÙŠØ³Ø¨ÙˆÙƒØŒ Ø·Ù„Ø¨Ø§Øª.''')
    ]
    
    print("Testing extraction...")
    result = await LivePatchService.build_from_messages(
        language="ar",
        messages=messages,
        last_summary={},
        last_coverage={},
    )
    
    print("\n--- EXTRACTED RESULT ---")
    pprint(result)

if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()
    asyncio.run(main())

################################################################################
# FILE: update_token.py
################################################################################

import os
from pathlib import Path

env_file = Path(".env")
new_token = str(os.getenv("TELEGRAM_BOT_TOKEN", "")).strip()

if not new_token:
    print("âœ— TELEGRAM_BOT_TOKEN is not set in environment")
    raise SystemExit(1)

# Read current content
if env_file.exists():
    with open(env_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # Update or add TELEGRAM_BOT_TOKEN
    token_found = False
    for i, line in enumerate(lines):
        if line.startswith('TELEGRAM_BOT_TOKEN='):
            lines[i] = f'TELEGRAM_BOT_TOKEN={new_token}\n'
            token_found = True
            break
    
    if not token_found:
        lines.append(f'TELEGRAM_BOT_TOKEN={new_token}\n')
    
    # Write back
    with open(env_file, 'w', encoding='utf-8') as f:
        f.writelines(lines)
    
    print("âœ“ Token updated successfully!")
else:
    print("âœ— .env file not found!")

################################################################################
# FILE: backend\__init__.py
################################################################################

"""Backend package initialization."""
__version__ = "1.0.0"

################################################################################
# FILE: backend\config.py
################################################################################

"""
Configuration management using Pydantic Settings.
Loads environment variables from .env file.
"""
from pydantic_settings import BaseSettings, SettingsConfigDict
from pydantic import Field, field_validator
from typing import List
import json
import secrets
import warnings
import os


class Settings(BaseSettings):
    """Application settings loaded from environment variables."""
    
    # Database Configuration (Docker on port 5555)
    database_url: str = Field(
        default="postgresql+asyncpg://tawasul:tawasul123@localhost:5555/tawasul",
        alias="DATABASE_URL"
    )
    
    # LLM Provider Configuration
    gemini_api_key: str = Field(
        default="",
        alias="GEMINI_API_KEY"
    )
    llm_provider: str = Field(default="groq", alias="LLM_PROVIDER")
    gemini_model: str = Field(default="gemini-2.5-flash", alias="GEMINI_MODEL")
    gemini_lite_model: str = Field(default="gemini-2.5-lite-flash", alias="GEMINI_LITE_MODEL")
    # OpenAI-compatible LLM providers
    openrouter_api_key: str = Field(default="", alias="OPENROUTER_API_KEY")
    openrouter_base_url: str = Field(
        default="https://openrouter.ai/api/v1",
        alias="OPENROUTER_BASE_URL"
    )
    openrouter_gemini_2_flash_model: str = Field(
        default="google/gemini-2.0-flash-001",
        alias="OPENROUTER_GEMINI_2_FLASH_MODEL"
    )
    openrouter_free_model: str = Field(
        default="openrouter/free",
        alias="OPENROUTER_FREE_MODEL"
    )
    openrouter_site_url: str = Field(default="", alias="OPENROUTER_SITE_URL")
    openrouter_app_name: str = Field(default="", alias="OPENROUTER_APP_NAME")

    groq_api_key: str = Field(default="", alias="GROQ_API_KEY")
    groq_base_url: str = Field(
        default="https://api.groq.com/openai/v1",
        alias="GROQ_BASE_URL"
    )
    openai_api_key: str = Field(default="", alias="OPENAI_API_KEY")
    openai_base_url: str = Field(
        default="https://api.openai.com/v1",
        alias="OPENAI_BASE_URL"
    )
    openai_stt_model: str = Field(default="whisper-1", alias="OPENAI_STT_MODEL")
    groq_llama_3_3_70b_versatile_model: str = Field(
        default="llama-3.3-70b-versatile",
        alias="GROQ_LLAMA_3_3_70B_VERSATILE_MODEL"
    )

    cerebras_api_key: str = Field(default="", alias="CEREBRAS_API_KEY")
    cerebras_base_url: str = Field(
        default="https://api.cerebras.ai/v1",
        alias="CEREBRAS_BASE_URL"
    )
    cerebras_llama_3_3_70b_model: str = Field(
        default="llama3.3-70b",
        alias="CEREBRAS_LLAMA_3_3_70B_MODEL"
    )
    cerebras_llama_3_1_8b_model: str = Field(
        default="llama3.1-8b",
        alias="CEREBRAS_LLAMA_3_1_8B_MODEL"
    )
    
    # Storage Configuration
    upload_dir: str = Field(default="./uploads", alias="UPLOAD_DIR")
    max_file_size_mb: int = Field(default=50, alias="MAX_FILE_SIZE_MB")
    stt_max_file_size_mb: int = Field(default=500, alias="STT_MAX_FILE_SIZE_MB")
    object_storage_provider: str = Field(default="local", alias="OBJECT_STORAGE_PROVIDER")
    aws_s3_bucket: str = Field(default="", alias="AWS_S3_BUCKET")
    aws_s3_region: str = Field(default="us-east-1", alias="AWS_S3_REGION")
    aws_access_key_id: str = Field(default="", alias="AWS_ACCESS_KEY_ID")
    aws_secret_access_key: str = Field(default="", alias="AWS_SECRET_ACCESS_KEY")
    aws_s3_endpoint_url: str = Field(default="", alias="AWS_S3_ENDPOINT_URL")
    aws_s3_presign_expiry_seconds: int = Field(default=900, alias="AWS_S3_PRESIGN_EXPIRY_SECONDS")
    
    # Chunking Configuration
    chunk_size: int = Field(default=30000, alias="CHUNK_SIZE")
    chunk_overlap: int = Field(default=200, alias="CHUNK_OVERLAP")
    parent_chunk_size: int = Field(default=300000, alias="PARENT_CHUNK_SIZE")
    parent_chunk_overlap: int = Field(default=600, alias="PARENT_CHUNK_OVERLAP")
    chunk_strategy: str = Field(default="parent_child", alias="CHUNK_STRATEGY")

    # API Configuration
    api_host: str = Field(default="127.0.0.1", alias="API_HOST")
    api_port: int = Field(default=8500, alias="API_PORT")
    api_title: str = Field(default="Tawasul API", alias="API_TITLE")
    api_version: str = Field(default="1.0.0", alias="API_VERSION")
    environment: str = Field(default="development", alias="ENVIRONMENT")
    strict_startup_checks: bool = Field(default=False, alias="STRICT_STARTUP_CHECKS")

    # Telegram Bot Configuration
    telegram_bot_token: str = Field(default="", alias="TELEGRAM_BOT_TOKEN")
    telegram_admin_id: str = Field(default="", alias="TELEGRAM_ADMIN_ID")
    
    # CORS Configuration
    cors_origins: List[str] = Field(
        default=[
            "http://localhost:3000",
            "http://localhost:8500",
            "http://127.0.0.1:3000",
            "http://127.0.0.1:8500",
        ],
        alias="CORS_ORIGINS"
    )

    @field_validator("cors_origins", mode="after")
    @classmethod
    def _normalize_cors_origins(cls, value: List[str]) -> List[str]:
        required = [
            "http://localhost:3000",
            "http://localhost:8500",
            "http://127.0.0.1:3000",
            "http://127.0.0.1:8500",
        ]
        normalized = list(value or [])
        for origin in required:
            if origin not in normalized:
                normalized.append(origin)
        return normalized
    
    # Logging
    log_level: str = Field(default="INFO", alias="LOG_LEVEL")

    # Authentication
    jwt_secret: str = Field(default="", alias="JWT_SECRET")
    jwt_algorithm: str = Field(default="HS256", alias="JWT_ALGORITHM")
    jwt_expiry_hours: int = Field(default=72, alias="JWT_EXPIRY_HOURS")

    @field_validator("jwt_secret", mode="after")
    @classmethod
    def _ensure_jwt_secret(cls, v: str) -> str:
        _INSECURE_DEFAULT = "tawasul-secret-change-me-in-production"
        if not v or v == _INSECURE_DEFAULT:
            warnings.warn(
                "JWT_SECRET not set or using insecure default. "
                "A random secret has been generated for this session. "
                "Set JWT_SECRET in .env for persistent tokens across restarts.",
                stacklevel=2,
            )
            return secrets.token_urlsafe(64)
        return v

    @field_validator("environment", mode="after")
    @classmethod
    def _normalize_environment(cls, value: str) -> str:
        env = str(value or "development").strip().lower()
        if env not in {"development", "staging", "production"}:
            return "development"
        return env
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore"
    )

    def _validate_production_basics(self, errors_list: List[str], jwt_secret_from_env: str) -> None:
        if not jwt_secret_from_env:
            errors_list.append("JWT_SECRET must be explicitly set in production.")
        if "*" in self.cors_origins:
            errors_list.append("CORS_ORIGINS must not include '*' in production.")

    def _validate_provider_configuration(
        self,
        env: str,
        warnings_list: List[str],
        errors_list: List[str],
    ) -> None:
        selected_provider = str(self.llm_provider or "").strip().lower()

        def emit(message: str) -> None:
            (errors_list if env == "production" else warnings_list).append(message)

        if selected_provider == "gemini" and not str(self.gemini_api_key or "").strip():
            emit("LLM_PROVIDER=gemini but GEMINI_API_KEY is empty.")
        if selected_provider.startswith("openrouter") and not str(self.openrouter_api_key or "").strip():
            emit("OpenRouter provider selected but OPENROUTER_API_KEY is empty.")
        if selected_provider.startswith("groq") and not str(self.groq_api_key or "").strip():
            emit("Groq provider selected but GROQ_API_KEY is empty.")

    def startup_issues(self) -> tuple[List[str], List[str]]:
        """Return (warnings, errors) for startup hardening checks."""
        warnings_list: List[str] = []
        errors_list: List[str] = []

        env = self.environment
        jwt_secret_from_env = str(os.getenv("JWT_SECRET", "")).strip()
        if env == "production":
            self._validate_production_basics(errors_list, jwt_secret_from_env)

        self._validate_provider_configuration(env, warnings_list, errors_list)

        return warnings_list, errors_list

    def validate_startup_or_raise(self) -> None:
        warnings_list, errors_list = self.startup_issues()
        for msg in warnings_list:
            warnings.warn(msg, stacklevel=2)

        must_fail = bool(errors_list) and (self.environment == "production" or self.strict_startup_checks)
        if must_fail:
            joined = " | ".join(errors_list)
            raise RuntimeError(f"Startup validation failed: {joined}")


# Global settings instance - use default values if .env not found
try:
    settings = Settings()
except Exception as e:
    # If .env is missing, use defaults
    import warnings
    warnings.warn(f".env file not found or invalid, using default settings: {str(e)}")
    # In Pydantic v2, we can't just pass _env_file=None to the constructor easily if it fails
    # We'll try to create a default instance without loading from env
    try:
        settings = Settings(_env_file=None)
    except Exception:
        # Fallback to a very basic settings if even that fails
        settings = Settings()

################################################################################
# FILE: backend\errors.py
################################################################################

"""
Application-level error helpers.
"""
from __future__ import annotations

from typing import Iterator

from fastapi import HTTPException
from sqlalchemy.exc import DBAPIError, OperationalError


def _iter_exception_chain(exc: BaseException) -> Iterator[BaseException]:
    current: BaseException | None = exc
    while current is not None:
        yield current
        current = current.__cause__ or current.__context__


def is_database_unavailable_error(exc: BaseException) -> bool:
    """Return True when an exception indicates DB connectivity is unavailable."""
    connectivity_markers = (
        "connection refused",
        "could not connect to server",
        "is the server running",
        "connection reset",
        "server closed the connection",
        "connection timed out",
        "timeout expired",
        "temporary failure in name resolution",
        "nodename nor servname provided",
        "no route to host",
    )

    for err in _iter_exception_chain(exc):
        if isinstance(err, (OperationalError, ConnectionRefusedError, TimeoutError, OSError)):
            return True
        if isinstance(err, DBAPIError) and getattr(err, "connection_invalidated", False):
            return True

        message = str(err).lower()
        if any(marker in message for marker in connectivity_markers):
            return True

    return False


def db_unavailable_http_exception() -> HTTPException:
    return HTTPException(
        status_code=503,
        detail="Database service unavailable. Start PostgreSQL (or Docker services) and try again.",
    )

################################################################################
# FILE: backend\init_database.py
################################################################################

"""
Database initialization script.
Creates database and tables.
"""
import asyncio
import sys
import os
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from backend.database import init_db
from backend.database.models import User
from backend.config import settings
import bcrypt
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def create_database_if_not_exists():
    """Create the database if it doesn't exist."""
    import psycopg2
    from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT
    
    # Parse connection string to get credentials for default 'postgres' db
    # Example: postgresql+asyncpg://tawasul:tawasul123@localhost:5555/tawasul
    db_url = settings.database_url.replace("postgresql+asyncpg://", "")
    auth, rest = db_url.split("@")
    user, password = auth.split(":")
    host_port, db_name = rest.split("/")
    host = host_port.split(":")[0]
    port = host_port.split(":")[1] if ":" in host_port else "5432"
    
    try:
        # Connect to default 'postgres' database
        conn = psycopg2.connect(
            dbname='postgres',
            user=user,
            password=password,
            host=host,
            port=port
        )
        conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
        cur = conn.cursor()
        
        # Check if database exists
        cur.execute(f"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{db_name}'")
        exists = cur.fetchone()
        
        if not exists:
            logger.info(f"Creating database {db_name}...")
            cur.execute(f"CREATE DATABASE {db_name}")
            logger.info(f"Database {db_name} created successfully")
        else:
            logger.info(f"Database {db_name} already exists")
            
        cur.close()
        conn.close()
    except Exception as e:
        logger.error(f"Error creating database: {str(e)}")
        # We continue anyway, maybe it exists but we couldn't check


async def create_default_user():
    """Create a default admin user if it doesn't exist."""
    from sqlalchemy import select
    from backend.database.connection import async_session_maker

    async with async_session_maker() as session:
        result = await session.execute(select(User).where(User.email == "admin@tawasul.com"))
        if result.scalar_one_or_none() is not None:
            logger.info("Default admin user already exists")
            return

        password_hash = bcrypt.hashpw("admin123".encode("utf-8"), bcrypt.gensalt()).decode("utf-8")
        user = User(
            name="Admin",
            email="admin@tawasul.com",
            password_hash=password_hash,
            role="admin"
        )
        session.add(user)
        await session.commit()
        logger.info("âœ… Default admin user created (admin@tawasul.com / admin123)")


async def main():
    """Initialize database."""
    try:
        # Step 1: Create database if needed
        await create_database_if_not_exists()
        
        logger.info(f"Connecting to database: {settings.database_url.split('@')[1]}")
        
        # Step 2: Initialize tables
        await init_db()

        logger.info("âœ… Database initialized successfully!")
        logger.info("Tables created:")
        logger.info("  - projects")
        logger.info("  - assets")
        logger.info("  - chat_messages")
        logger.info("  - srs_drafts")

        # Step 3: Create default admin user if no users exist
        await create_default_user()
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ Failed to initialize database: {str(e)}")
        return 1
    
    finally:
        from backend.database import close_db
        await close_db()


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

################################################################################
# FILE: backend\main.py
################################################################################

"""
Main FastAPI Application.
Entry point for the Tawasul backend API.
"""
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import Response
from contextlib import asynccontextmanager
import time
import uuid

try:
    from fastapi.responses import ORJSONResponse
    _default_response = ORJSONResponse
except ImportError:
    _default_response = None
# Configure logging
from backend.config import settings
import logging

logging.basicConfig(
    level=getattr(logging, settings.log_level),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST

    _metrics_enabled = True
    _request_counter = Counter(
        "tawasul_http_requests_total",
        "Total HTTP requests",
        ["method", "path", "status"],
    )
    _request_latency = Histogram(
        "tawasul_http_request_duration_seconds",
        "HTTP request latency in seconds",
        ["method", "path"],
        buckets=(0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0),
    )
except Exception:  # noqa: BLE001
    _metrics_enabled = False
    _request_counter = None
    _request_latency = None

from backend.database import init_db, close_db
from backend.routes import projects, documents, query, health, stats, bot_config, app_config, stt, srs, messages, interview, judge
from backend.routes import auth
from backend.routes import handoff


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler."""
    # Startup
    logger.info("Starting Tawasul API...")
    settings.validate_startup_or_raise()
    try:
        await init_db()
        logger.info("Database initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize database: {str(e)}")
        raise

    yield
    
    # Shutdown
    logger.info("Shutting down Tawasul API...")
    await close_db()
    logger.info("Database connections closed")


# Create FastAPI app
_app_kwargs = {
    "title": settings.api_title,
    "version": settings.api_version,
    "description": """
    Tawasul - AI Requirements Engineering Platform
    
    A transcript-first requirements and Q&A API using:
    - Google Gemini 2.5 Flash for LLM capabilities
    - PostgreSQL for project data, chat history, and SRS drafts
    - Document transcript extraction (PDF, DOCX, TXT)
    
    ## Features
    - Project-based document organization
    - Multi-format document support (PDF, TXT, DOCX)
    - Transcript-grounded question answering with sources
    - Guided interview workflow for requirements gathering
    - Automated SRS draft generation and PDF export
    - Multi-language support (Arabic/English)
    """,
    "lifespan": lifespan,
}
if _default_response:
    _app_kwargs['default_response_class'] = _default_response
app = FastAPI(**_app_kwargs)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# GZip compression for responses
app.add_middleware(GZipMiddleware, minimum_size=500)


@app.middleware("http")
async def request_observability_middleware(request, call_next):
    request_id = request.headers.get("X-Request-ID") or str(uuid.uuid4())
    started = time.perf_counter()
    response = await call_next(request)
    elapsed_ms = round((time.perf_counter() - started) * 1000, 2)
    elapsed_seconds = max(0.0, elapsed_ms / 1000.0)

    route_path = request.url.path
    route = request.scope.get("route")
    if route is not None:
        normalized = getattr(route, "path", None)
        if isinstance(normalized, str) and normalized:
            route_path = normalized

    response.headers["X-Request-ID"] = request_id
    response.headers["X-Response-Time-ms"] = str(elapsed_ms)

    if _metrics_enabled and _request_counter is not None and _request_latency is not None:
        method = request.method
        status = str(response.status_code)
        _request_counter.labels(method=method, path=route_path, status=status).inc()
        _request_latency.labels(method=method, path=route_path).observe(elapsed_seconds)

    logger.info(
        "request_id=%s method=%s path=%s status=%s duration_ms=%s",
        request_id,
        request.method,
        request.url.path,
        response.status_code,
        elapsed_ms,
    )
    return response


@app.get("/metrics", include_in_schema=False)
async def metrics_endpoint():
    if not _metrics_enabled:
        return Response(
            content="# metrics_disabled prometheus_client_not_installed\n",
            media_type="text/plain; version=0.0.4; charset=utf-8",
            status_code=503,
        )
    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)

# Include routers
app.include_router(health.router)
app.include_router(auth.router)
app.include_router(projects.router)
app.include_router(documents.router)
app.include_router(query.router)
app.include_router(stats.router)
app.include_router(bot_config.router)
app.include_router(app_config.router)
app.include_router(stt.router)
app.include_router(srs.router)
app.include_router(messages.router)
app.include_router(interview.router)
app.include_router(judge.router)
app.include_router(handoff.router)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "backend.main:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=True
    )

################################################################################
# FILE: backend\requirements.txt
################################################################################

# FastAPI and server
fastapi==0.109.0
uvicorn[standard]==0.27.0
python-multipart==0.0.6

# Database
sqlalchemy==2.0.25
asyncpg==0.29.0
psycopg2-binary==2.9.9
alembic==1.13.1

# Document Processing
pypdf>=4.0.0
python-docx>=1.1.0

# Speech-to-Text (Groq Whisper chunking for files > 24MB)
# pydub>=0.25.1

# Google Gemini
google-generativeai>=0.8.0

# Telegram Bot
python-telegram-bot>=21.0

# Utilities
python-dotenv==1.0.0
pydantic>=2.7.4
pydantic-settings>=2.10.1
aiofiles==23.2.1
orjson>=3.9.0
json-repair>=0.30.0
fpdf2>=2.7.8
prometheus-client>=0.20.0
boto3>=1.34.0

# HTTP Client
httpx>=0.27.0
pyTelegramBotAPI

# Authentication
PyJWT>=2.8.0
bcrypt>=4.1.0

################################################################################
# FILE: backend\runtime_config.py
################################################################################

"""
Runtime configuration storage.
Stores provider selections in process-local memory.
"""
from __future__ import annotations

from typing import Any, Dict


_runtime_config: Dict[str, Any] = {}


def load_runtime_config() -> Dict[str, Any]:
    return dict(_runtime_config)


def save_runtime_config(config: Dict[str, Any]) -> None:
    _runtime_config.clear()
    _runtime_config.update(dict(config or {}))


def update_runtime_config(updates: Dict[str, Any]) -> Dict[str, Any]:
    config = load_runtime_config()
    config.update(dict(updates or {}))
    save_runtime_config(config)
    return config


def get_runtime_value(key: str, default: Any = None) -> Any:
    config = load_runtime_config()
    return config.get(key, default)

################################################################################
# FILE: backend\controllers\__init__.py
################################################################################

"""Controllers package initialization.

Keep this module import-light to avoid side effects and circular imports.
"""

__all__ = []

################################################################################
# FILE: backend\controllers\project_controller.py
################################################################################

"""
Project Controller.
Business logic for project management.
"""
from typing import List, Optional, Dict, Any
from sqlalchemy import select, delete
from sqlalchemy.ext.asyncio import AsyncSession
from backend.database.models import Project, Asset
from backend.services.file_service import FileService
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class ProjectController:
    """Controller for project operations."""
    
    def __init__(self):
        """Initialize project controller."""
        self.file_service = FileService()
    
    async def create_project(
        self,
        db: AsyncSession,
        name: str,
        description: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        user_id: Optional[int] = None
    ) -> Project:
        """
        Create a new project.

        Args:
            db: Database session
            name: Project name
            description: Optional description
            metadata: Optional metadata
            user_id: Owner user ID

        Returns:
            Created project
        """
        try:
            project = Project(
                name=name,
                description=description,
                extra_metadata=metadata or {},
                user_id=user_id
            )
            
            db.add(project)
            await db.commit()
            await db.refresh(project)
            
            logger.info(f"Created project: {project.id} - {project.name}")
            return project
            
        except Exception as e:
            await db.rollback()
            logger.error(f"Error creating project: {str(e)}")
            raise
    
    async def get_project(
        self,
        db: AsyncSession,
        project_id: int,
        user_id: Optional[int] = None
    ) -> Optional[Project]:
        """
        Get project by ID, optionally scoped to a user.

        Args:
            db: Database session
            project_id: Project ID
            user_id: If provided, only return project if owned by this user

        Returns:
            Project or None
        """
        try:
            stmt = select(Project).where(Project.id == project_id)
            if user_id is not None:
                stmt = stmt.where(Project.user_id == user_id)
            result = await db.execute(stmt)
            project = result.scalar_one_or_none()

            return project
            
        except Exception as e:
            logger.error(f"Error getting project: {str(e)}")
            raise
    
    async def list_projects(
        self,
        db: AsyncSession,
        skip: int = 0,
        limit: int = 100,
        user_id: Optional[int] = None
    ) -> List[Project]:
        """
        List projects, optionally scoped to a user.

        Args:
            db: Database session
            skip: Number of projects to skip
            limit: Maximum number of projects to return
            user_id: If provided, only return projects owned by this user

        Returns:
            List of projects
        """
        try:
            stmt = select(Project)
            if user_id is not None:
                stmt = stmt.where(Project.user_id == user_id)
            stmt = stmt.offset(skip).limit(limit).order_by(Project.created_at.desc())
            result = await db.execute(stmt)
            projects = result.scalars().all()
            
            return list(projects)
            
        except Exception as e:
            logger.error(f"Error listing projects: {str(e)}")
            raise
    
    async def update_project(
        self,
        db: AsyncSession,
        project_id: int,
        name: Optional[str] = None,
        description: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Optional[Project]:
        """
        Update project.
        
        Args:
            db: Database session
            project_id: Project ID
            name: Optional new name
            description: Optional new description
            metadata: Optional new metadata
            
        Returns:
            Updated project or None
        """
        try:
            project = await self.get_project(db, project_id)
            if not project:
                return None
            
            if name is not None:
                project.name = name
            if description is not None:
                project.description = description
            if metadata is not None:
                project.extra_metadata = metadata
            
            project.updated_at = datetime.utcnow()
            
            await db.commit()
            await db.refresh(project)
            
            logger.info(f"Updated project: {project_id}")
            return project
            
        except Exception as e:
            await db.rollback()
            logger.error(f"Error updating project: {str(e)}")
            raise
    
    async def delete_project(
        self,
        db: AsyncSession,
        project_id: int,
        user_id: Optional[int] = None
    ) -> bool:
        """
        Delete project and all associated data.

        Args:
            db: Database session
            project_id: Project ID
            user_id: If provided, only delete if owned by this user

        Returns:
            True if deleted successfully
        """
        try:
            # Delete files from storage
            await self.file_service.delete_project_files(project_id)

            # Delete from database (cascade will handle assets)
            stmt = delete(Project).where(Project.id == project_id)
            if user_id is not None:
                stmt = stmt.where(Project.user_id == user_id)
            result = await db.execute(stmt)
            await db.commit()
            
            deleted = result.rowcount > 0
            if deleted:
                logger.info(f"Deleted project: {project_id}")
            
            return deleted
            
        except Exception as e:
            await db.rollback()
            logger.error(f"Error deleting project: {str(e)}")
            raise
    
    async def get_project_stats(
        self,
        db: AsyncSession,
        project_id: int
    ) -> Dict[str, Any]:
        """
        Get project statistics.
        
        Args:
            db: Database session
            project_id: Project ID
            
        Returns:
            Statistics dictionary
        """
        try:
            # Get asset count
            asset_stmt = select(Asset).where(Asset.project_id == project_id)
            asset_result = await db.execute(asset_stmt)
            assets = asset_result.scalars().all()
            
            transcript_count = sum(1 for a in assets if (a.extracted_text or "").strip())
            
            return {
                'asset_count': len(assets),
                'transcript_count': transcript_count,
                'total_size': sum(a.file_size for a in assets),
                'completed_assets': sum(1 for a in assets if a.status == 'completed'),
                'processing_assets': sum(1 for a in assets if a.status == 'processing'),
                'failed_assets': sum(1 for a in assets if a.status == 'failed')
            }
            
        except Exception as e:
            logger.error(f"Error getting project stats: {str(e)}")
            raise

################################################################################
# FILE: backend\controllers\query_controller.py
################################################################################

"""
Query Controller.
Business logic for query processing and answer generation.
"""
from typing import Dict, Any, Optional, AsyncIterator
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
import json
import logging

from backend.database.models import Project
from backend.services.srs_snapshot_cache import SRSSnapshotCache

logger = logging.getLogger(__name__)


class QueryController:
    """Controller for query operations."""
    
    def __init__(self):
        """Initialize query controller."""
        from backend.services.answer_service import AnswerService

        self.answer_service = AnswerService()
    
    async def answer_query(
        self,
        db: AsyncSession,
        project_id: int,
        query: str,
        top_k: int = 5,
        language: str = "ar",
        asset_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Process query and generate answer.
        
        Args:
            db: Database session
            project_id: Project ID to search in
            query: User question
            top_k: Number of transcript matches to retrieve
            language: Response language ('ar' or 'en')
            asset_id: Optional specific transcript document to search
            
        Returns:
            Dictionary with answer and metadata
        """
        try:
            logger.info(f"Processing query for project {project_id}: {query[:50]}...")
            project_context = await self._get_project_context(db=db, project_id=project_id)

            result = await self.answer_service.generate_answer_no_context(
                query=query,
                language=language,
                project_context=project_context,
            )
            logger.info("Generated answer for query in context-window mode (no RAG)")
            return result
            
        except Exception as e:
            logger.error(f"Error processing query: {str(e)}")
            raise

    async def answer_query_stream(
        self,
        db: AsyncSession,
        project_id: int,
        query: str,
        top_k: int = 5,
        language: str = "ar",
        asset_id: Optional[int] = None,
    ) -> AsyncIterator[str]:
        """
        Stream SSE events: first the sources, then token-by-token answer,
        then a [DONE] sentinel.
        """
        try:
            logger.info(
                f"Processing streaming query for project {project_id}: {query[:50]}..."
            )
            project_context = await self._get_project_context(db=db, project_id=project_id)

            yield f"data: {json.dumps({'type': 'sources', 'sources': [], 'context_used': 0})}\n\n"
            async for token in self.answer_service.generate_answer_no_context_stream(
                query=query,
                language=language,
                project_context=project_context,
            ):
                yield f"data: {json.dumps({'type': 'token', 'token': token}, ensure_ascii=False)}\n\n"

            yield "data: [DONE]\n\n"

        except Exception as e:
            logger.error(f"Error streaming query: {str(e)}")
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
            yield "data: [DONE]\n\n"

    @staticmethod
    async def _get_project_context(db: AsyncSession, project_id: int) -> Dict[str, Any]:
        project_stmt = select(Project).where(Project.id == project_id)
        project_result = await db.execute(project_stmt)
        project = project_result.scalar_one_or_none()
        srs_snapshot = await SRSSnapshotCache.get_latest_snapshot(db=db, project_id=project_id)

        context: Dict[str, Any] = {
            "project_id": project_id,
            "project_name": str(project.name) if project and project.name else "",
            "project_description": str(project.description) if project and project.description else "",
            "srs": None,
        }

        if isinstance(srs_snapshot, dict):
            context["srs"] = srs_snapshot

        return context

################################################################################
# FILE: backend\database\__init__.py
################################################################################

"""Database package initialization."""
from backend.database.models import Base, User, Project, Asset, ChatMessage, TelegramSession, SRSDraft
from backend.database.connection import engine, async_session_maker, get_db, init_db, close_db

__all__ = [
    "Base",
    "User",
    "Project",
    "Asset",
    "ChatMessage",
    "TelegramSession",
    "SRSDraft",
    "engine",
    "async_session_maker",
    "get_db",
    "init_db",
    "close_db"
]

################################################################################
# FILE: backend\database\connection.py
################################################################################

"""
Database connection management with async SQLAlchemy.
Provides engine and session factory.
"""
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from backend.config import settings
import logging

logger = logging.getLogger(__name__)

# Create async engine
engine = create_async_engine(
    settings.database_url,
    echo=False,
    pool_size=20,
    max_overflow=10,
    pool_pre_ping=True,
    pool_recycle=300,
    pool_timeout=30,
    future=True,
)

# Create async session factory
async_session_maker = async_sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autocommit=False,
    autoflush=False
)


async def get_db() -> AsyncSession:
    """
    Dependency function to get database session.
    Use with FastAPI Depends().

    Callers (routes/controllers) must explicitly ``await db.commit()``
    when they want to persist changes.  The session will auto-rollback
    on unhandled exceptions and always close when the request ends.
    """
    async with async_session_maker() as session:
        yield session


async def init_db():
    """Initialize database - create tables if they don't exist."""
    from backend.database.models import Base
    from sqlalchemy import text

    async def ensure_schema_compat(conn):
        column_exists = await conn.execute(text("""
            SELECT 1
            FROM information_schema.columns
            WHERE table_schema = 'public'
              AND table_name = 'projects'
              AND column_name = 'user_id'
            LIMIT 1
        """))
        if column_exists.scalar_one_or_none() is None:
            logger.warning("Legacy schema detected: adding projects.user_id column")
            await conn.execute(text("ALTER TABLE projects ADD COLUMN user_id INTEGER"))
            await conn.execute(text("CREATE INDEX IF NOT EXISTS ix_projects_user_id ON projects (user_id)"))

    startup_error = None
    try:
        async with engine.begin() as conn:
            # Create all tables
            await conn.run_sync(Base.metadata.create_all)
            await ensure_schema_compat(conn)
            logger.info("Database initialized successfully")
            return
    except Exception as e:
        startup_error = e
        logger.warning(f"Initial database initialization attempt failed: {str(e)}")
        logger.info("Attempting fallback initialization...")
        try:
            async with engine.begin() as conn:
                # Try to create tables one by one or skip those that fail
                await conn.run_sync(Base.metadata.create_all)
                await ensure_schema_compat(conn)
                logger.info("Database tables initialized (some might have failed)")
                return
        except Exception as e2:
            logger.error(f"Failed to initialize database tables: {str(e2)}")
            startup_error = e2

    if startup_error is not None:
        raise RuntimeError("Database initialization failed") from startup_error


async def close_db():
    """Close database connections."""
    await engine.dispose()
    logger.info("Database connections closed")

################################################################################
# FILE: backend\database\models.py
################################################################################

"""
Database models using SQLAlchemy async ORM.
Defines tables for projects, assets, chat messages, and SRS drafts.
"""
from sqlalchemy import Column, Integer, BigInteger, String, Text, DateTime, ForeignKey, JSON, Index
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from datetime import datetime

Base = declarative_base()


class User(Base):
    """User model for authentication."""
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(255), nullable=False)
    email = Column(String(255), nullable=False, unique=True, index=True)
    password_hash = Column(String(255), nullable=False)
    role = Column(String(50), default="admin")  # user, admin
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    def __repr__(self):
        return f"<User(id={self.id}, email='{self.email}')>"


class Project(Base):
    """Project model for organizing documents."""
    __tablename__ = "projects"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id", ondelete="CASCADE"), nullable=True, index=True)
    name = Column(String(255), nullable=False, index=True)
    description = Column(Text, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    # Metadata (renamed to avoid conflict with SQLAlchemy metadata)
    extra_metadata = Column("metadata", JSON, default={})

    # Relationships
    owner = relationship("User", backref="projects")
    assets = relationship("Asset", back_populates="project", cascade="all, delete-orphan")
    
    def __repr__(self):
        return f"<Project(id={self.id}, name='{self.name}')>"


class Asset(Base):
    """Asset model for uploaded documents."""
    __tablename__ = "assets"
    
    id = Column(Integer, primary_key=True, index=True)
    project_id = Column(Integer, ForeignKey("projects.id", ondelete="CASCADE"), nullable=False)
    
    # File information
    filename = Column(String(500), nullable=False)
    original_filename = Column(String(500), nullable=False)
    file_path = Column(String(1000), nullable=False)
    file_size = Column(Integer, nullable=False)  # in bytes
    file_type = Column(String(50), nullable=False)  # pdf, txt, docx
    extracted_text = Column(Text, nullable=True)
    
    # Status
    status = Column(String(50), default="uploaded")  # uploaded, processing, completed, failed
    error_message = Column(Text, nullable=True)
    
    # Timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    processed_at = Column(DateTime(timezone=True), nullable=True)
    
    # Metadata (renamed to avoid conflict)
    extra_metadata = Column("metadata", JSON, default={})
    
    # Relationships
    project = relationship("Project", back_populates="assets")

    __table_args__ = (
        Index('ix_assets_project_status', 'project_id', 'status'),
    )

    def __repr__(self):
        return f"<Asset(id={self.id}, filename='{self.filename}', status='{self.status}')>"


class ChatMessage(Base):
    """Chat message model for project conversations."""
    __tablename__ = "chat_messages"

    id = Column(Integer, primary_key=True, index=True)
    project_id = Column(Integer, ForeignKey("projects.id", ondelete="CASCADE"), nullable=False)
    role = Column(String(50), nullable=False)  # user, assistant, system
    content = Column(Text, nullable=False)
    extra_metadata = Column("metadata", JSON, default={})
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    project = relationship("Project")

    __table_args__ = (
        Index('ix_chat_messages_project_created', 'project_id', 'created_at'),
    )

    def __repr__(self):
        return f"<ChatMessage(id={self.id}, project_id={self.project_id}, role='{self.role}')>"


class SRSDraft(Base):
    """SRS draft model for project requirements summaries."""
    __tablename__ = "srs_drafts"

    id = Column(Integer, primary_key=True, index=True)
    project_id = Column(Integer, ForeignKey("projects.id", ondelete="CASCADE"), nullable=False)
    version = Column(Integer, nullable=False, default=1)
    status = Column(String(50), default="draft")
    language = Column(String(10), default="ar")
    content = Column(JSON, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    project = relationship("Project")

    __table_args__ = (
        Index('ix_srs_drafts_project_version', 'project_id', 'version'),
    )

    def __repr__(self):
        return f"<SRSDraft(id={self.id}, project_id={self.project_id}, version={self.version}, status='{self.status}')>"


class TelegramSession(Base):
    """Telegram session model for linking chat_id to project_id."""
    __tablename__ = "telegram_sessions"

    id = Column(Integer, primary_key=True, index=True)
    chat_id = Column(BigInteger, nullable=False, unique=True, index=True)
    project_id = Column(Integer, ForeignKey("projects.id", ondelete="CASCADE"), nullable=False, index=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    project = relationship("Project")

    def __repr__(self):
        return f"<TelegramSession(chat_id={self.chat_id}, project_id={self.project_id})>"

################################################################################
# FILE: backend\providers\__init__.py
################################################################################

"""Providers package initialization."""

################################################################################
# FILE: backend\providers\llm\__init__.py
################################################################################

"""LLM providers package."""
from backend.providers.llm.interface import LLMInterface
from backend.providers.llm.factory import LLMProviderFactory

__all__ = [
	"LLMInterface",
	"LLMProviderFactory",
]

################################################################################
# FILE: backend\providers\llm\cohere_provider.py
################################################################################

"""
Cohere Embedding Provider Implementation.
Uses Cohere SDK for embeddings.
"""
from typing import List, Optional
import asyncio
import logging
import random
import cohere
from backend.providers.llm.interface import LLMInterface
from backend.config import settings

logger = logging.getLogger(__name__)


class CohereProvider(LLMInterface):
    """Cohere provider implementation (embeddings only)."""

    def __init__(self, api_key: str = None, embed_model: str = None):
        self.api_key = api_key or getattr(settings, "cohere_api_key", "")
        self.embed_model = embed_model or getattr(settings, "cohere_embed_model", "embed-multilingual-v3.0")
        self.client = cohere.Client(self.api_key)
        logger.info(f"Cohere provider initialized with embedding model: {self.embed_model}")

    async def generate_text(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        raise NotImplementedError("CohereProvider does not support text generation in this project.")

    async def generate_embeddings(
        self,
        texts: List[str],
        **kwargs
    ) -> List[List[float]]:
        try:
            if not texts:
                return []

            batch_size = kwargs.get("batch_size")
            max_batch_tokens = kwargs.get("max_batch_tokens")
            max_retries = kwargs.get("max_retries", getattr(settings, "cohere_max_retries", 3))
            base_delay = kwargs.get("base_delay", getattr(settings, "cohere_base_retry_delay", 2.0))

            token_cap = getattr(settings, "cohere_max_batch_tokens", 50000)
            if max_batch_tokens is not None:
                token_cap = min(token_cap, int(max_batch_tokens))

            batches = self._build_batches(
                texts=texts,
                batch_size=batch_size,
                max_batch_tokens=token_cap
            )

            embeddings: List[List[float]] = []
            for idx, batch in enumerate(batches):
                if idx > 0:
                    # Throttle between batches to avoid rate limits
                    await asyncio.sleep(1.0 + random.uniform(0, 0.5))
                response = await self._embed_with_retry(
                    texts=batch,
                    max_retries=max_retries,
                    base_delay=base_delay
                )
                embeddings.extend(response.embeddings)

            return embeddings
        except Exception as e:
            logger.error(f"Error generating embeddings with Cohere: {str(e)}")
            raise

    def get_model_name(self) -> str:
        return self.embed_model

    def get_embedding_dimension(self) -> int:
        model = (self.embed_model or "").lower()
        if "v3" in model:
            return 1024
        return 768

    def _build_batches(
        self,
        texts: List[str],
        batch_size: Optional[int],
        max_batch_tokens: Optional[int]
    ) -> List[List[str]]:
        if not texts:
            return []

        resolved_batch_size = max(1, int(batch_size)) if batch_size else len(texts)
        token_budget = max(1, int(max_batch_tokens)) if max_batch_tokens else None

        batches: List[List[str]] = []
        current_batch: List[str] = []
        current_tokens = 0

        for text in texts:
            estimated_tokens = max(1, len(text) // 4)

            would_exceed_count = len(current_batch) >= resolved_batch_size
            would_exceed_tokens = token_budget is not None and (current_tokens + estimated_tokens) > token_budget

            if current_batch and (would_exceed_count or would_exceed_tokens):
                batches.append(current_batch)
                current_batch = []
                current_tokens = 0

            current_batch.append(text)
            current_tokens += estimated_tokens

        if current_batch:
            batches.append(current_batch)

        return batches

    async def _embed_with_retry(
        self,
        texts: List[str],
        max_retries: int,
        base_delay: float
    ) -> "cohere.types.EmbedResponse":
        attempt = 0
        while True:
            try:
                return self.client.embed(
                    texts=texts,
                    model=self.embed_model,
                    input_type="search_document"
                )
            except cohere.errors.too_many_requests_error.TooManyRequestsError as e:
                attempt += 1
                if attempt > max_retries:
                    raise

                # Exponential backoff + jitter to avoid thundering herd
                delay = base_delay * (2 ** (attempt - 1)) + random.uniform(0, 1.5)
                delay = min(delay, 120)  # Cap at 2 minutes
                logger.warning(
                    "Cohere rate limited, retrying in %.2fs (attempt %s/%s)",
                    delay,
                    attempt,
                    max_retries
                )
                await asyncio.sleep(delay)
            except Exception:
                raise

################################################################################
# FILE: backend\providers\llm\factory.py
################################################################################

"""
LLM Provider Factory.
Creates LLM provider instances based on configuration.
"""
from backend.providers.llm.interface import LLMInterface
from backend.providers.llm.gemini_provider import GeminiProvider
from backend.providers.llm.openai_compat_provider import OpenAICompatProvider
from backend.config import settings
from backend.runtime_config import get_runtime_value
import logging

logger = logging.getLogger(__name__)

GEMINI_PROVIDER = "gemini"
GEMINI_FLASH_PROVIDER = "gemini-2.5-flash"
GEMINI_LITE_PROVIDER = "gemini-2.5-lite-flash"


class LLMProviderFactory:
    """Factory for creating LLM provider instances."""

    _provider_cache: dict[str, LLMInterface] = {}

    @staticmethod
    def _openrouter_headers() -> dict:
        headers = {}
        if settings.openrouter_site_url:
            headers["HTTP-Referer"] = settings.openrouter_site_url
        if settings.openrouter_app_name:
            headers["X-Title"] = settings.openrouter_app_name
        return headers
    
    @staticmethod
    def create_provider(provider_name: str = None) -> LLMInterface:
        """
        Create LLM provider instance.
        
        Args:
            provider_name: Name of provider ('gemini', 'openai', etc.)
                          Defaults to settings.llm_provider
        
        Returns:
            LLM provider instance
            
        Raises:
            ValueError: If provider name is not supported
        """
        provider_name = provider_name or get_runtime_value("llm_provider", settings.llm_provider)
        provider_name = provider_name.lower()

        cached = LLMProviderFactory._provider_cache.get(provider_name)
        if cached is not None:
            return cached

        if provider_name in {GEMINI_PROVIDER, GEMINI_FLASH_PROVIDER}:
            logger.info("Creating Gemini LLM provider: %s", settings.gemini_model)
            provider = GeminiProvider(model_name=settings.gemini_model)
            LLMProviderFactory._provider_cache[provider_name] = provider
            return provider

        if provider_name == GEMINI_LITE_PROVIDER:
            lite_model = getattr(settings, "gemini_lite_model", GEMINI_LITE_PROVIDER)
            logger.info("Creating Gemini LLM provider: %s", lite_model)
            provider = GeminiProvider(model_name=lite_model)
            LLMProviderFactory._provider_cache[provider_name] = provider
            return provider

        openrouter_models = {
            "openrouter-gemini-2.0-flash": settings.openrouter_gemini_2_flash_model,
            "openrouter-free": settings.openrouter_free_model,
        }
        openrouter_model = openrouter_models.get(provider_name)
        if openrouter_model:
            logger.info("Creating OpenRouter provider: %s", provider_name)
            provider = OpenAICompatProvider(
                api_key=settings.openrouter_api_key,
                base_url=settings.openrouter_base_url,
                model_name=openrouter_model,
                provider_label="OpenRouter",
                extra_headers=LLMProviderFactory._openrouter_headers()
            )
            LLMProviderFactory._provider_cache[provider_name] = provider
            return provider

        compat_providers = {
            "groq-llama-3.3-70b-versatile": {
                "api_key": settings.groq_api_key,
                "base_url": settings.groq_base_url,
                "model_name": settings.groq_llama_3_3_70b_versatile_model,
                "label": "Groq",
            },
            "cerebras-llama-3.3-70b": {
                "api_key": settings.cerebras_api_key,
                "base_url": settings.cerebras_base_url,
                "model_name": settings.cerebras_llama_3_3_70b_model,
                "label": "Cerebras",
            },
            "cerebras-llama-3.1-8b": {
                "api_key": settings.cerebras_api_key,
                "base_url": settings.cerebras_base_url,
                "model_name": settings.cerebras_llama_3_1_8b_model,
                "label": "Cerebras",
            },
        }
        compat_config = compat_providers.get(provider_name)
        if compat_config:
            logger.info("Creating %s provider", compat_config["label"])
            provider = OpenAICompatProvider(
                api_key=compat_config["api_key"],
                base_url=compat_config["base_url"],
                model_name=compat_config["model_name"],
                provider_label=compat_config["label"]
            )
            LLMProviderFactory._provider_cache[provider_name] = provider
            return provider

        raise ValueError(f"Unsupported LLM provider: {provider_name}")

    @staticmethod
    def get_available_providers() -> list:
        """Get list of available provider names (only those with API keys configured)."""
        providers = [GEMINI_PROVIDER]  # Gemini is always available if GEMINI_API_KEY is set

        if not settings.gemini_api_key:
            providers = []

        # Gemini lite variant shares the same key
        if settings.gemini_api_key:
            providers.append(GEMINI_FLASH_PROVIDER)
            providers.append(GEMINI_LITE_PROVIDER)

        if settings.openrouter_api_key:
            providers.append("openrouter-gemini-2.0-flash")
            providers.append("openrouter-free")

        if settings.groq_api_key:
            providers.append("groq-llama-3.3-70b-versatile")

        if settings.cerebras_api_key:
            providers.append("cerebras-llama-3.3-70b")
            providers.append("cerebras-llama-3.1-8b")

        return providers

    @staticmethod
    def get_available_embedding_providers() -> list:
        """Embedding providers are disabled in no-RAG mode."""
        return []

    @staticmethod
    def clear_cache() -> None:
        LLMProviderFactory._provider_cache.clear()

################################################################################
# FILE: backend\providers\llm\gemini_provider.py
################################################################################

"""
Google Gemini 2.5 Flash LLM Provider Implementation.
Uses google-generativeai SDK for text generation and embeddings.
"""
from typing import List, Optional, AsyncIterator
import google.generativeai as genai
from backend.providers.llm.interface import LLMInterface
from backend.config import settings
import logging
import asyncio

logger = logging.getLogger(__name__)


class GeminiProvider(LLMInterface):
    """Google Gemini LLM provider implementation."""
    
    def __init__(self, api_key: str = None, model_name: str = None):
        """
        Initialize Gemini provider.
        
        Args:
            api_key: Gemini API key (defaults to settings)
            model_name: Model name (defaults to settings)
        """
        self.api_key = api_key or settings.gemini_api_key
        # Support both flash and lite-flash
        if model_name:
            self.model_name = model_name
        else:
            import backend.runtime_config as rc
            runtime_model = rc.get_runtime_value("gemini_model")
            if runtime_model:
                self.model_name = runtime_model
            else:
                # Use special model if provider is gemini-2.5-lite-flash
                provider = rc.get_runtime_value("llm_provider", settings.llm_provider)
                if provider == "gemini-2.5-lite-flash":
                    self.model_name = getattr(settings, "gemini_lite_model", "gemini-2.5-lite-flash")
                else:
                    self.model_name = settings.gemini_model
        # Configure Gemini
        genai.configure(api_key=self.api_key)
        # Initialize models
        self.chat_model = genai.GenerativeModel(self.model_name)
        self.embedding_model = getattr(settings, "gemini_embed_model", "models/gemini-embedding-001")
        logger.info(f"Gemini provider initialized with model: {self.model_name}")
    
    async def generate_text(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """
        Generate text using Gemini.
        
        Args:
            prompt: User prompt
            system_prompt: System instruction
            temperature: Sampling temperature
            max_tokens: Maximum output tokens
            
        Returns:
            Generated text
        """
        try:
            # Combine system prompt and user prompt
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"
            
            # Configure generation
            generation_config = genai.GenerationConfig(
                temperature=temperature,
                max_output_tokens=max_tokens or 2048,
            )
            
            # Generate response (run in thread pool for async)
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.chat_model.generate_content(
                    full_prompt,
                    generation_config=generation_config
                )
            )
            
            return response.text
            
        except Exception as e:
            logger.error(f"Error generating text with Gemini: {str(e)}")
            raise

    async def generate_text_stream(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncIterator[str]:
        """Stream text token-by-token using Gemini SDK."""
        try:
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"

            generation_config = genai.GenerationConfig(
                temperature=temperature,
                max_output_tokens=max_tokens or 2048,
            )

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.chat_model.generate_content(
                    full_prompt,
                    generation_config=generation_config,
                    stream=True,
                ),
            )

            for chunk in response:
                text = getattr(chunk, "text", None)
                if text:
                    yield text

        except Exception as e:
            logger.error(f"Error streaming text with Gemini: {str(e)}")
            raise
    
    async def generate_embeddings(
        self,
        texts: List[str],
        **kwargs
    ) -> List[List[float]]:
        """
        Generate embeddings using Gemini.
        
        Args:
            texts: List of texts to embed
            
        Returns:
            List of embedding vectors
        """
        try:
            embeddings = []
            
            # Use asyncio.gather for parallel processing with a semaphore to control concurrency
            # This is significantly faster than sequential processing
            MAX_CONCURRENT_REQUESTS = 20
            sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
            
            async def embed_single(text):
                async with sem:
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(
                        None,
                        lambda: genai.embed_content(
                            model=self.embedding_model,
                            content=text,
                            task_type="retrieval_document"
                        )
                    )
                    return result["embedding"]
            
            # Create tasks
            tasks = [embed_single(text) for text in texts]
            
            # Execute in parallel
            embeddings = await asyncio.gather(*tasks)
            
            return list(embeddings)
            
        except Exception as e:
            logger.error(f"Error generating embeddings with Gemini: {str(e)}")
            raise
    
    def get_model_name(self) -> str:
        """Get model name."""
        return self.model_name
    
    def get_embedding_dimension(self) -> int:
        """Get embedding dimension for Gemini (768)."""
        return 768

################################################################################
# FILE: backend\providers\llm\hf_bge_m3_provider.py
################################################################################

"""
Hugging Face BGE-M3 Embedding Provider (local).
Uses sentence-transformers for embeddings.
"""
from typing import List, Optional
import logging
import asyncio

from backend.providers.llm.interface import LLMInterface
from backend.config import settings

logger = logging.getLogger(__name__)


class BgeM3Provider(LLMInterface):
    """Local BGE-M3 provider implementation (embeddings only)."""

    def __init__(self, model_name: str = None, device: str = None):
        self.model_name = model_name or getattr(settings, "hf_embedding_model", "BAAI/bge-m3")
        self.device = device

        try:
            from sentence_transformers import SentenceTransformer
            import torch
        except ImportError as exc:
            raise ImportError(
                "sentence-transformers/torch not installed. Install with: pip install sentence-transformers torch"
            ) from exc

        if self.device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"

        self.model = SentenceTransformer(self.model_name, device=self.device)
        logger.info(f"BGE-M3 provider initialized with model: {self.model_name} on {self.device}")

    async def generate_text(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        raise NotImplementedError("BgeM3Provider does not support text generation in this project.")

    async def generate_embeddings(
        self,
        texts: List[str],
        batch_size: int = 16,
        **kwargs
    ) -> List[List[float]]:
        try:
            loop = asyncio.get_event_loop()
            embeddings = await loop.run_in_executor(
                None,
                lambda: self.model.encode(
                    texts,
                    batch_size=batch_size,
                    normalize_embeddings=True,
                    show_progress_bar=False
                )
            )
            return embeddings.tolist()
        except Exception as e:
            logger.error(f"Error generating embeddings with BGE-M3: {str(e)}")
            raise

    def get_model_name(self) -> str:
        return self.model_name

    def get_embedding_dimension(self) -> int:
        return int(self.model.get_sentence_embedding_dimension())

################################################################################
# FILE: backend\providers\llm\interface.py
################################################################################

"""
Abstract LLM Provider Interface.
Defines the contract that all LLM providers must implement.
"""
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, AsyncIterator


class LLMInterface(ABC):
    """Abstract base class for LLM providers."""
    
    @abstractmethod
    async def generate_text(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """
        Generate text completion from prompt.
        
        Args:
            prompt: User prompt/question
            system_prompt: Optional system instruction
            temperature: Sampling temperature (0.0 to 1.0)
            max_tokens: Maximum tokens to generate
            **kwargs: Additional provider-specific parameters
            
        Returns:
            Generated text response
        """
        pass

    async def generate_text_stream(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncIterator[str]:
        """
        Stream text completion token by token.

        Default implementation falls back to non-streaming generate_text
        and yields the whole response at once.
        """
        full = await self.generate_text(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        yield full
    
    @abstractmethod
    async def generate_embeddings(
        self,
        texts: List[str],
        **kwargs
    ) -> List[List[float]]:
        """
        Generate vector embeddings for texts.
        
        Args:
            texts: List of text strings to embed
            **kwargs: Additional provider-specific parameters
            
        Returns:
            List of embedding vectors
        """
        pass
    
    @abstractmethod
    def get_model_name(self) -> str:
        """
        Get the model name/identifier.
        
        Returns:
            Model name string
        """
        pass
    
    @abstractmethod
    def get_embedding_dimension(self) -> int:
        """
        Get the embedding vector dimension.
        
        Returns:
            Dimension size as integer
        """
        pass

################################################################################
# FILE: backend\providers\llm\openai_compat_provider.py
################################################################################

"""
OpenAI-compatible LLM provider implementation.
Supports OpenRouter, Groq, Cerebras, and other OpenAI-style APIs.
"""
from typing import List, Optional, Dict, Any, AsyncIterator
import logging
import httpx
import json

from backend.providers.llm.interface import LLMInterface

logger = logging.getLogger(__name__)


class OpenAICompatProvider(LLMInterface):
    """OpenAI-compatible LLM provider (text generation only)."""

    _FALLBACK_PROMPT_LIMIT_CHARS = 20000
    _FALLBACK_MIN_MAX_TOKENS = 1024

    def _build_headers(self) -> Dict[str, str]:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        headers.update(self.extra_headers)
        return headers

    def _build_payload(
        self,
        prompt: str,
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: Optional[int],
        response_format: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        payload: Dict[str, Any] = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
        }
        if max_tokens is not None:
            payload["max_tokens"] = max_tokens
        if isinstance(response_format, dict) and response_format:
            payload["response_format"] = response_format
        return payload

    def _shrink_request(
        self,
        prompt: str,
        max_tokens: Optional[int],
    ) -> tuple[str, Optional[int]]:
        reduced_prompt = prompt
        if len(prompt) > self._FALLBACK_PROMPT_LIMIT_CHARS:
            reduced_prompt = prompt[: self._FALLBACK_PROMPT_LIMIT_CHARS]

        reduced_max_tokens = max_tokens
        if max_tokens is not None:
            reduced_max_tokens = max(
                self._FALLBACK_MIN_MAX_TOKENS,
                max_tokens // 2,
            )

        logger.warning(
            "%s returned 413. Retrying with smaller payload (prompt chars: %s -> %s, max_tokens: %s -> %s)",
            self.provider_label,
            len(prompt),
            len(reduced_prompt),
            max_tokens,
            reduced_max_tokens,
        )
        return reduced_prompt, reduced_max_tokens

    def __init__(
        self,
        api_key: str,
        base_url: str,
        model_name: str,
        provider_label: str,
        extra_headers: Optional[Dict[str, str]] = None
    ) -> None:
        self.api_key = api_key
        self.base_url = base_url.rstrip("/")
        self.model_name = model_name
        self.provider_label = provider_label
        self.extra_headers = extra_headers or {}
        logger.info(
            "%s provider initialized with model: %s",
            self.provider_label,
            self.model_name
        )

    async def generate_text(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        headers = self._build_headers()
        endpoint = f"{self.base_url}/chat/completions"

        current_prompt = prompt
        current_max_tokens = max_tokens
        response_format = kwargs.get("response_format") if isinstance(kwargs.get("response_format"), dict) else None

        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                data: Optional[Dict[str, Any]] = None
                for attempt in range(2):
                    payload = self._build_payload(
                        prompt=current_prompt,
                        system_prompt=system_prompt,
                        temperature=temperature,
                        max_tokens=current_max_tokens,
                        response_format=response_format,
                    )

                    response = await client.post(endpoint, headers=headers, json=payload)

                    try:
                        response.raise_for_status()
                        data = response.json()
                        break
                    except httpx.HTTPStatusError:
                        should_retry = response.status_code == 413 and attempt == 0
                        if not should_retry:
                            raise

                        current_prompt, current_max_tokens = self._shrink_request(
                            prompt=current_prompt,
                            max_tokens=current_max_tokens,
                        )

                if data is None:
                    raise ValueError("No response returned from provider")

            choices = data.get("choices") or []
            if not choices:
                raise ValueError("No choices returned from provider")

            message = choices[0].get("message") or {}
            content = message.get("content")
            if not content:
                raise ValueError("Empty content returned from provider")

            return content
        except Exception as e:
            logger.error("Error generating text with %s: %s", self.provider_label, str(e))
            raise

    async def generate_text_stream(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncIterator[str]:
        """Stream text token-by-token via SSE from OpenAI-compat endpoint."""
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            payload: Dict[str, Any] = {
                "model": self.model_name,
                "messages": messages,
                "temperature": temperature,
                "stream": True,
            }
            if max_tokens is not None:
                payload["max_tokens"] = max_tokens

            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }
            headers.update(self.extra_headers)

            endpoint = f"{self.base_url}/chat/completions"
            async with httpx.AsyncClient(timeout=120.0) as client:
                async with client.stream(
                    "POST", endpoint, headers=headers, json=payload
                ) as response:
                    response.raise_for_status()
                    async for line in response.aiter_lines():
                        if not line.startswith("data: "):
                            continue
                        data_str = line[6:]
                        if data_str.strip() == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data_str)
                            delta = (
                                chunk.get("choices", [{}])[0]
                                .get("delta", {})
                                .get("content")
                            )
                            if delta:
                                yield delta
                        except json.JSONDecodeError:
                            continue
        except Exception as e:
            logger.error(
                "Error streaming text with %s: %s", self.provider_label, str(e)
            )
            raise

    async def generate_embeddings(
        self,
        texts: List[str],
        **kwargs
    ) -> List[List[float]]:
        raise NotImplementedError(
            "OpenAICompatProvider does not support embeddings in this project."
        )

    def get_model_name(self) -> str:
        return self.model_name

    def get_embedding_dimension(self) -> int:
        return 0

################################################################################
# FILE: backend\providers\llm\voyage_provider.py
################################################################################

"""
Voyage AI Embedding Provider Implementation.
Uses voyageai SDK for embeddings.
"""
from typing import List, Optional
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor

from backend.providers.llm.interface import LLMInterface
from backend.config import settings

logger = logging.getLogger(__name__)


class VoyageProvider(LLMInterface):
    """Voyage AI provider implementation (embeddings only)."""

    def __init__(self, api_key: str = None, embed_model: str = None, output_dimension: int = None):
        self.api_key = api_key or getattr(settings, "voyage_api_key", "")
        self.embed_model = embed_model or getattr(settings, "voyage_embed_model", "voyage-3-large")
        self.output_dimension = output_dimension or getattr(settings, "voyage_output_dimension", 1024)

        try:
            import voyageai
        except ImportError as exc:
            raise ImportError("voyageai is not installed. Install with: pip install voyageai") from exc

        self.client = voyageai.Client(api_key=self.api_key)
        self._executor = ThreadPoolExecutor(max_workers=4)
        logger.info(f"Voyage provider initialized with embedding model: {self.embed_model}")

    async def generate_text(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        raise NotImplementedError("VoyageProvider does not support text generation in this project.")

    async def generate_embeddings(
        self,
        texts: List[str],
        **kwargs
    ) -> List[List[float]]:
        try:
            if not texts:
                return []

            batch_size = int(kwargs.get("batch_size") or 10)
            max_batch_tokens = int(kwargs.get("max_batch_tokens") or 120000)
            safe_limit = int(max_batch_tokens * 0.9)

            def estimate_tokens(text: str) -> int:
                # Rough heuristic: ~4 chars per token for English-like text.
                return max(1, len(text) // 4)

            def truncate_to_limit(text: str) -> str:
                max_chars = max_batch_tokens * 4
                if len(text) <= max_chars:
                    return text
                logger.warning("Truncating text to fit Voyage batch token limit")
                return text[:max_chars]

            batches: List[List[str]] = []
            current_batch: List[str] = []
            current_tokens = 0

            for text in texts:
                safe_text = truncate_to_limit(text)
                token_est = estimate_tokens(safe_text)

                if current_batch and (
                    len(current_batch) >= batch_size or current_tokens + token_est > safe_limit
                ):
                    batches.append(current_batch)
                    current_batch = []
                    current_tokens = 0

                current_batch.append(safe_text)
                current_tokens += token_est

            if current_batch:
                batches.append(current_batch)

            loop = asyncio.get_event_loop()
            results: List[List[float]] = []
            for batch in batches:
                response = await loop.run_in_executor(
                    self._executor,
                    lambda b=batch: self.client.embed(
                        texts=b,
                        model=self.embed_model,
                        input_type="document",
                        output_dimension=self.output_dimension
                    )
                )
                results.extend(response.embeddings)

            return results
        except Exception as e:
            logger.error(f"Error generating embeddings with Voyage: {str(e)}")
            raise

    def get_model_name(self) -> str:
        return self.embed_model

    def get_embedding_dimension(self) -> int:
        model_name = (self.embed_model or "").lower()
        if self.output_dimension:
            return int(self.output_dimension)
        if "voyage-3" in model_name:
            return 1024
        return 1024

################################################################################
# FILE: backend\routes\__init__.py
################################################################################

"""Routes package initialization.

Keep this module import-light to avoid side effects and circular imports.
"""

__all__ = [
    "auth",
    "bot_config",
    "documents",
    "handoff",
    "health",
    "interview",
    "messages",
    "projects",
    "query",
    "srs",
    "stats",
    "app_config",
    "stt",
]

################################################################################
# FILE: backend\routes\app_config.py
################################################################################

"""
Application configuration routes.
Exposes provider availability and runtime selections.
"""
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field
from typing import Dict, List

from backend.config import settings
from backend.runtime_config import get_runtime_value, update_runtime_config
from backend.providers.llm.factory import LLMProviderFactory
from backend.database.models import User
from backend.routes.auth import get_current_user

router = APIRouter(prefix="/config", tags=["App Config"])


class ProviderUpdate(BaseModel):
    llm_provider: str


@router.get("/providers")
async def get_providers(_user: User = Depends(get_current_user)) -> Dict[str, object]:
    """Return available providers and current selections."""
    llm_available = LLMProviderFactory.get_available_providers()

    return {
        "available": {
            "llm": llm_available,
        },
        "llm_provider": get_runtime_value("llm_provider", settings.llm_provider),
    }


@router.post("/providers")
async def update_providers(payload: ProviderUpdate, _user: User = Depends(get_current_user)) -> Dict[str, object]:
    """Update runtime provider selections."""
    llm_available = set(LLMProviderFactory.get_available_providers())

    if payload.llm_provider not in llm_available:
        raise HTTPException(status_code=400, detail="Unsupported LLM provider")

    updates = {
        "llm_provider": payload.llm_provider,
    }

    config = update_runtime_config(updates)

    return {
        "llm_provider": config.get("llm_provider", settings.llm_provider),
    }


################################################################################
# FILE: backend\routes\auth.py
################################################################################

"""
Authentication routes: register, login, get current user.
"""
from __future__ import annotations

import bcrypt
import jwt
from datetime import datetime, timedelta, timezone

from fastapi import APIRouter, Depends, HTTPException, Header
from pydantic import BaseModel, Field
from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession

from backend.config import settings
from backend.database import get_db
from backend.database.models import User
from backend.errors import is_database_unavailable_error, db_unavailable_http_exception

router = APIRouter(prefix="/auth", tags=["Authentication"])


class RegisterRequest(BaseModel):
    name: str = Field(..., min_length=2, max_length=255)
    email: str = Field(..., min_length=5, max_length=255)
    password: str = Field(..., min_length=6, max_length=128)


class LoginRequest(BaseModel):
    email: str
    password: str


class AuthResponse(BaseModel):
    token: str
    user: dict


class UserResponse(BaseModel):
    id: int
    name: str
    email: str
    role: str


def _hash_password(password: str) -> str:
    return bcrypt.hashpw(password.encode("utf-8"), bcrypt.gensalt()).decode("utf-8")


def _verify_password(password: str, hashed: str) -> bool:
    try:
        return bcrypt.checkpw(password.encode("utf-8"), hashed.encode("utf-8"))
    except Exception:
        return False


def _create_token(user_id: int, email: str) -> str:
    payload = {
        "sub": str(user_id),
        "email": email,
        "exp": datetime.now(timezone.utc) + timedelta(hours=settings.jwt_expiry_hours),
    }
    return jwt.encode(payload, settings.jwt_secret, algorithm=settings.jwt_algorithm)


def _normalize_email(email: str) -> str:
    return str(email or "").strip().lower()


def _decode_token(token: str) -> dict:
    try:
        return jwt.decode(token, settings.jwt_secret, algorithms=[settings.jwt_algorithm])
    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=401, detail="Token expired")
    except jwt.InvalidTokenError:
        raise HTTPException(status_code=401, detail="Invalid token")


async def get_current_user(
    authorization: str = Header(default=""),
    db: AsyncSession = Depends(get_db),
) -> User:
    if not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Missing or invalid authorization header")
    token = authorization[7:]
    payload = _decode_token(token)
    raw_user_id = payload.get("sub")
    try:
        user_id = int(raw_user_id)
    except (TypeError, ValueError):
        raise HTTPException(status_code=401, detail="Invalid token subject")
    try:
        result = await db.execute(select(User).where(User.id == user_id))
    except Exception as e:
        if is_database_unavailable_error(e):
            raise db_unavailable_http_exception()
        raise
    user = result.scalar_one_or_none()
    if not user:
        raise HTTPException(status_code=401, detail="User not found")
    return user


@router.post("/register", response_model=AuthResponse)
async def register(data: RegisterRequest, db: AsyncSession = Depends(get_db)):
    normalized_email = _normalize_email(data.email)
    if not normalized_email:
        raise HTTPException(status_code=400, detail="Email is required")

    try:
        existing = await db.execute(
            select(User)
            .where(func.lower(User.email) == normalized_email)
            .order_by(User.id.desc())
        )
    except Exception as e:
        if is_database_unavailable_error(e):
            raise db_unavailable_http_exception()
        raise
    if existing.scalars().first() is not None:
        raise HTTPException(status_code=409, detail="Email already registered")

    user = User(
        name=data.name,
        email=normalized_email,
        password_hash=_hash_password(data.password),
    )
    try:
        db.add(user)
        await db.commit()
        await db.refresh(user)
    except Exception as e:
        if is_database_unavailable_error(e):
            raise db_unavailable_http_exception()
        raise

    token = _create_token(user.id, user.email)
    return {
        "token": token,
        "user": {"id": user.id, "name": user.name, "email": user.email, "role": user.role},
    }


@router.post("/login", response_model=AuthResponse)
async def login(data: LoginRequest, db: AsyncSession = Depends(get_db)):
    normalized_email = _normalize_email(data.email)
    if not normalized_email or not data.password:
        raise HTTPException(status_code=401, detail="Invalid credentials")

    try:
        result = await db.execute(
            select(User)
            .where(func.lower(User.email) == normalized_email)
            .order_by(User.id.desc())
        )
    except Exception as e:
        if is_database_unavailable_error(e):
            raise db_unavailable_http_exception()
        raise
    user = result.scalars().first()

    if not user or not user.password_hash or not _verify_password(data.password, user.password_hash):
        raise HTTPException(status_code=401, detail="Invalid credentials")

    token = _create_token(user.id, user.email)
    return {
        "token": token,
        "user": {"id": user.id, "name": user.name, "email": user.email, "role": user.role},
    }


@router.get("/me", response_model=UserResponse)
async def me(user: User = Depends(get_current_user)):
    return {"id": user.id, "name": user.name, "email": user.email, "role": user.role}

################################################################################
# FILE: backend\routes\bot_config.py
################################################################################

"""
Bot Configuration Routes.
API endpoints for configuring the Telegram bot.
"""
from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Depends
from pydantic import BaseModel
from typing import Optional
from pathlib import Path
import httpx
from telegram_bot.config import bot_settings
from backend.database.models import User
from backend.routes.auth import get_current_user
from backend.runtime_config import get_runtime_value, update_runtime_config

router = APIRouter(prefix="/bot", tags=["Bot Config"])

# Path to .env file
_ENV_PATH = Path(__file__).resolve().parent.parent.parent / ".env"

# Keys we manage for Telegram config
_TG_ENV_KEYS = {
    "TELEGRAM_BOT_TOKEN",
    "TELEGRAM_ADMIN_ID",
    "BOT_API_EMAIL",
    "BOT_API_PASSWORD",
    "API_BASE_URL",
}


class BotConfig(BaseModel):
    active_project_id: Optional[int] = None


class TelegramConfigUpdate(BaseModel):
    telegram_bot_token: Optional[str] = None
    telegram_admin_id: Optional[str] = None
    bot_api_email: Optional[str] = None
    bot_api_password: Optional[str] = None
    api_base_url: Optional[str] = None


def _read_env_values() -> dict[str, str]:
    """Read relevant Telegram keys from .env file."""
    values: dict[str, str] = {}
    if not _ENV_PATH.exists():
        return values
    for line in _ENV_PATH.read_text(encoding="utf-8").splitlines():
        stripped = line.strip()
        if not stripped or stripped.startswith("#"):
            continue
        if "=" not in stripped:
            continue
        key, _, val = stripped.partition("=")
        key = key.strip()
        if key in _TG_ENV_KEYS:
            values[key] = val.strip()
    return values


def _write_env_values(updates: dict[str, str]) -> None:
    """Update specific keys in .env file, preserving all other content."""
    if not _ENV_PATH.exists():
        lines_out = []
    else:
        lines_out = _ENV_PATH.read_text(encoding="utf-8").splitlines()

    keys_written: set[str] = set()

    for i, line in enumerate(lines_out):
        stripped = line.strip()
        if not stripped or stripped.startswith("#") or "=" not in stripped:
            continue
        key = stripped.partition("=")[0].strip()
        if key in updates:
            lines_out[i] = f"{key}={updates[key]}"
            keys_written.add(key)

    # Append any keys that weren't already in the file
    for key, val in updates.items():
        if key not in keys_written:
            lines_out.append(f"{key}={val}")

    _ENV_PATH.write_text("\n".join(lines_out) + "\n", encoding="utf-8")


def _mask_token(token: str) -> str:
    """Mask bot token, showing only first 8 and last 4 chars."""
    if not token or len(token) < 16:
        return "***"
    return token[:8] + ":" + "*" * 30 + token[-4:]


@router.get("/telegram-config")
async def get_telegram_config(_user: User = Depends(get_current_user)):
    """Return current Telegram bot configuration (token is masked)."""
    env = _read_env_values()
    token = env.get("TELEGRAM_BOT_TOKEN", "")
    return {
        "telegram_bot_token": _mask_token(token) if token else "",
        "telegram_admin_id": env.get("TELEGRAM_ADMIN_ID", ""),
        "bot_api_email": env.get("BOT_API_EMAIL", "admin@tawasul.com"),
        "bot_api_password": "",  # Never return password
        "api_base_url": env.get("API_BASE_URL", "http://localhost:8500"),
        "has_token": bool(token),
    }


@router.post("/telegram-config")
async def update_telegram_config(
    payload: TelegramConfigUpdate,
    _user: User = Depends(get_current_user),
):
    """Save Telegram bot configuration to .env file."""
    updates: dict[str, str] = {}

    if payload.telegram_bot_token is not None and payload.telegram_bot_token.strip():
        updates["TELEGRAM_BOT_TOKEN"] = payload.telegram_bot_token.strip()
    if payload.telegram_admin_id is not None:
        updates["TELEGRAM_ADMIN_ID"] = payload.telegram_admin_id.strip()
    if payload.bot_api_email is not None and payload.bot_api_email.strip():
        updates["BOT_API_EMAIL"] = payload.bot_api_email.strip()
    if payload.bot_api_password is not None and payload.bot_api_password.strip():
        updates["BOT_API_PASSWORD"] = payload.bot_api_password.strip()
    if payload.api_base_url is not None and payload.api_base_url.strip():
        updates["API_BASE_URL"] = payload.api_base_url.strip()

    if not updates:
        raise HTTPException(status_code=400, detail="No valid fields to update")

    try:
        _write_env_values(updates)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save config: {e}")

    return {"status": "success", "updated_keys": list(updates.keys())}


@router.get("/config")
async def get_bot_config(_user: User = Depends(get_current_user)):
    """Get current bot configuration."""
    return {
        "active_project_id": get_runtime_value("bot_active_project_id", None),
    }

@router.post("/config")
async def update_bot_config(config: BotConfig, _user: User = Depends(get_current_user)):
    """Update bot configuration (active project)."""
    current_config = {
        "active_project_id": get_runtime_value("bot_active_project_id", None),
    }
    if config.active_project_id is not None:
        current_config["active_project_id"] = config.active_project_id
        update_runtime_config({"bot_active_project_id": config.active_project_id})
    return current_config

@router.post("/profile")
async def update_bot_profile(
    name: str = Form(...),
    _user: User = Depends(get_current_user),
):
    """
    Update Telegram Bot Profile (Name).
    Requires 'setMyName' permission.
    """
    try:
        async with httpx.AsyncClient() as client:
            # Update Name
            url = f"https://api.telegram.org/bot{bot_settings.telegram_bot_token}/setMyName"
            response = await client.post(url, json={"name": name})
            response.raise_for_status()
            
            return {"status": "success", "message": "Bot profile updated"}
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


################################################################################
# FILE: backend\routes\documents.py
################################################################################

"""
Document Routes.
API endpoints for document management.
"""
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, BackgroundTasks
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from datetime import datetime
from backend.database import get_db
from backend.database.connection import async_session_maker
from backend.database.models import Asset, Project, User
from backend.routes.auth import get_current_user
from backend.config import settings
from backend.services.file_service import FileService

router = APIRouter(tags=["Documents"])
_file_service = FileService()


async def _verify_project_access(db: AsyncSession, project_id: int, user: User):
    """Verify the user owns this project."""
    stmt = select(Project).where(Project.id == project_id, Project.user_id == user.id)
    result = await db.execute(stmt)
    if not result.scalar_one_or_none():
        raise HTTPException(status_code=404, detail="Project not found")


# Response Models
class AssetResponse(BaseModel):
    id: int
    project_id: int
    filename: str
    original_filename: str
    file_size: int
    file_type: str
    status: str
    error_message: Optional[str]
    created_at: datetime
    processed_at: Optional[datetime]
    extra_metadata: Dict[str, Any]

    class Config:
        from_attributes = True


class PresignUploadRequest(BaseModel):
    filename: str
    file_size: int
    content_type: Optional[str] = None


class PresignUploadResponse(BaseModel):
    upload_url: str
    file_key: str
    unique_filename: str
    content_type: str
    expires_in: int


class CompleteUploadRequest(BaseModel):
    unique_filename: str
    original_filename: str
    file_key: str
    file_size: int
    file_type: str


# Routes
@router.post("/projects/{project_id}/documents/presign", response_model=PresignUploadResponse)
async def presign_document_upload(
    project_id: int,
    payload: PresignUploadRequest,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    await _verify_project_access(db, project_id, user)
    if payload.file_size > _file_service.max_size_bytes:
        raise HTTPException(status_code=400, detail=f"File too large. Maximum size is {settings.max_file_size_mb}MB")

    ext = (payload.filename.rsplit('.', 1)[-1].lower() if '.' in payload.filename else "")
    if f".{ext}" not in _file_service.get_supported_extensions():
        raise HTTPException(status_code=400, detail="Unsupported file type. Supported: PDF, TXT, DOCX")

    try:
        response = await _file_service.generate_presigned_upload(
            project_id=project_id,
            filename=payload.filename,
            content_type=payload.content_type or "application/octet-stream",
        )
        return PresignUploadResponse(**response)
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc))
    except Exception as exc:  # noqa: BLE001
        raise HTTPException(status_code=500, detail=str(exc))


@router.post("/projects/{project_id}/documents/complete", response_model=AssetResponse, status_code=201)
async def complete_document_upload(
    project_id: int,
    payload: CompleteUploadRequest,
    background_tasks: BackgroundTasks,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    await _verify_project_access(db, project_id, user)
    if payload.file_size > _file_service.max_size_bytes:
        raise HTTPException(status_code=400, detail=f"File too large. Maximum size is {settings.max_file_size_mb}MB")

    file_type = str(payload.file_type or "").lower()
    if f".{file_type}" not in _file_service.get_supported_extensions():
        raise HTTPException(status_code=400, detail="Unsupported file type. Supported: PDF, TXT, DOCX")

    if str(settings.object_storage_provider or "local").strip().lower() in {"aws_s3", "s3"}:
        file_path = f"s3://{settings.aws_s3_bucket}/{payload.file_key}"
    else:
        file_path = payload.file_key

    asset = Asset(
        project_id=project_id,
        filename=payload.unique_filename,
        original_filename=payload.original_filename,
        file_path=file_path,
        file_size=payload.file_size,
        file_type=file_type,
        status="uploaded",
    )
    db.add(asset)
    await db.commit()
    await db.refresh(asset)

    background_tasks.add_task(
        _process_document_asset,
        asset_id=asset.id,
    )
    return asset


@router.post("/projects/{project_id}/documents", response_model=AssetResponse, status_code=201)
async def upload_document(
    project_id: int,
    file: UploadFile = File(...),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Upload document to project.
    Document will be processed in background.
    """
    await _verify_project_access(db, project_id, user)
    try:
        # Read file
        file_content = await file.read()
        file_size = len(file_content)

        is_valid, error_msg = _file_service.validate_file(file.filename, file_size)
        if not is_valid:
            raise HTTPException(status_code=400, detail=error_msg or "Invalid file")

        unique_filename, file_path = await _file_service.save_upload_file(
            file_content=file_content,
            filename=file.filename,
            project_id=project_id,
        )
        file_type = file.filename.rsplit('.', 1)[-1].lower() if '.' in file.filename else ""
        asset = Asset(
            project_id=project_id,
            filename=unique_filename,
            original_filename=file.filename,
            file_path=file_path,
            file_size=file_size,
            file_type=file_type,
            status="uploaded",
        )
        db.add(asset)
        await db.commit()
        await db.refresh(asset)

        # Process in background
        background_tasks.add_task(
            _process_document_asset,
            asset_id=asset.id
        )

        return asset

    except HTTPException:
        raise
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/projects/{project_id}/documents", response_model=List[AssetResponse])
async def list_project_documents(
    project_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """List all documents in project."""
    await _verify_project_access(db, project_id, user)
    try:
        stmt = select(Asset).where(Asset.project_id == project_id).order_by(Asset.created_at.desc())
        result = await db.execute(stmt)
        return list(result.scalars().all())
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/documents/{asset_id}", response_model=AssetResponse)
async def get_document(
    asset_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Get document by ID."""
    try:
        result = await db.execute(select(Asset).where(Asset.id == asset_id))
        document = result.scalar_one_or_none()
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")
        return document
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post("/documents/{asset_id}/process", response_model=AssetResponse)
async def process_document(
    asset_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Manually trigger document processing."""
    try:
        await _process_document_asset(asset_id=asset_id)
        result = await db.execute(select(Asset).where(Asset.id == asset_id))
        document = result.scalar_one_or_none()
        if not document:
            raise HTTPException(status_code=404, detail="Document not found")
        return document
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/documents/{asset_id}", status_code=204)
async def delete_document(
    asset_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Delete document."""
    try:
        result = await db.execute(select(Asset).where(Asset.id == asset_id))
        asset = result.scalar_one_or_none()
        if not asset:
            raise HTTPException(status_code=404, detail="Document not found")

        await _file_service.delete_file(asset.file_path)
        await db.delete(asset)
        await db.commit()
        return None
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


async def _process_document_asset(asset_id: int) -> bool:
    async with async_session_maker() as db:
        result = await db.execute(select(Asset).where(Asset.id == asset_id))
        asset = result.scalar_one_or_none()
        if not asset:
            raise ValueError(f"Asset not found: {asset_id}")

        asset.status = "processing"
        await db.commit()

        try:
            text = await _file_service.extract_text(asset.file_path)
            asset.extracted_text = text
            asset.status = "completed"
            asset.processed_at = datetime.utcnow()
            metadata = dict(asset.extra_metadata or {})
            metadata.update({"stage": "completed", "progress": 100, "processed_chunks": 1, "total_chunks": 1})
            asset.extra_metadata = metadata
            await db.commit()
            return True
        except Exception as exc:  # noqa: BLE001
            asset.status = "failed"
            asset.error_message = str(exc)
            await db.commit()
            raise

################################################################################
# FILE: backend\routes\handoff.py
################################################################################

"""
Project handoff routes: send SRS + summary to engineering team.
"""
from __future__ import annotations

import logging

from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel, Field
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from backend.database import get_db
from backend.database.models import SRSDraft, Project, ChatMessage, User
from backend.routes.auth import get_current_user

logger = logging.getLogger(__name__)

router = APIRouter(tags=["Handoff"])


class HandoffRequest(BaseModel):
    client_name: str = Field(default="")
    client_email: str = Field(default="")
    notes: str = Field(default="")


class HandoffResponse(BaseModel):
    success: bool
    message: str
    package: dict


@router.post("/projects/{project_id}/handoff", response_model=HandoffResponse)
async def create_handoff(
    project_id: int,
    data: HandoffRequest,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    # Get project (scoped to user)
    result = await db.execute(
        select(Project).where(Project.id == project_id, Project.user_id == user.id)
    )
    project = result.scalar_one_or_none()
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    # Get latest SRS
    srs_result = await db.execute(
        select(SRSDraft)
        .where(SRSDraft.project_id == project_id)
        .order_by(SRSDraft.version.desc())
        .limit(1)
    )
    srs_draft = srs_result.scalar_one_or_none()

    # Get conversation summary
    msgs_result = await db.execute(
        select(ChatMessage)
        .where(ChatMessage.project_id == project_id)
        .order_by(ChatMessage.created_at.asc())
    )
    messages = list(msgs_result.scalars().all())

    # Build the handoff package
    conversation_summary = []
    for msg in messages[-20:]:  # Last 20 messages
        conversation_summary.append({
            "role": msg.role,
            "content": msg.content[:500],  # Truncate long messages
        })

    srs_content = srs_draft.content if srs_draft else None

    package = {
        "project": {
            "id": project.id,
            "name": project.name,
            "description": project.description or "",
        },
        "srs": srs_content,
        "conversation_summary": conversation_summary,
        "client": {
            "name": data.client_name,
            "email": data.client_email,
            "notes": data.notes,
        },
        "message_count": len(messages),
    }

    logger.info(f"Handoff package created for project {project_id}")

    return {
        "success": True,
        "message": "Handoff package created successfully",
        "package": package,
    }

################################################################################
# FILE: backend\routes\health.py
################################################################################

"""
Health Check Routes.
API endpoints for system health monitoring.
"""
from datetime import datetime, timezone
import logging

from fastapi import APIRouter, Depends
from pydantic import BaseModel
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

from backend.config import settings
from backend.database import get_db

logger = logging.getLogger(__name__)

router = APIRouter(tags=["Health"])


class HealthResponse(BaseModel):
    status: str
    database: str
    llm_provider: str


class ReadinessResponse(BaseModel):
    status: str
    timestamp: str
    environment: str
    checks: dict


async def _database_readiness(db: AsyncSession) -> dict:
    try:
        await db.execute(text("SELECT 1"))
        return {"status": "ready", "details": "Connection is healthy"}
    except Exception as exc:  # noqa: BLE001
        return {"status": "not_ready", "details": f"DB check failed: {exc}"}


def _llm_config_readiness() -> dict:
    warnings_list, errors_list = settings.startup_issues()
    if errors_list:
        return {"status": "not_ready", "details": " | ".join(errors_list)}
    if warnings_list:
        return {"status": "degraded", "details": " | ".join(warnings_list)}
    return {"status": "ready", "details": "Provider config is valid"}


@router.get("/health", response_model=HealthResponse)
async def health_check(db: AsyncSession = Depends(get_db)):
    """Check system health status including database and LLM provider."""
    # --- Database ---
    try:
        await db.execute(text("SELECT 1"))
        db_status = "connected"
    except Exception:
        db_status = "disconnected"

    # --- Overall status ---
    if db_status == "disconnected":
        overall = "unhealthy"
    else:
        overall = "healthy"

    return {
        "status": overall,
        "database": db_status,
        "llm_provider": settings.llm_provider,
    }


@router.get("/health/ready", response_model=ReadinessResponse)
async def readiness_check(db: AsyncSession = Depends(get_db)):
    """Production-style readiness probe with component status details."""
    checks = {
        "database": await _database_readiness(db),
        "llm_config": _llm_config_readiness(),
    }

    overall = "ready"
    if any(item.get("status") == "not_ready" for item in checks.values()):
        overall = "not_ready"
    elif any(item.get("status") == "degraded" for item in checks.values()):
        overall = "degraded"

    return {
        "status": overall,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "environment": settings.environment,
        "checks": checks,
    }


@router.get("/")
async def root():
    """Root endpoint."""
    return {
        "name": settings.api_title,
        "version": settings.api_version,
        "docs": "/docs",
        "health": "/health"
    }

################################################################################
# FILE: backend\routes\interview.py
################################################################################

"""
Interview routes for guided requirements questions.
"""
from __future__ import annotations

import logging
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from backend.database import get_db, async_session_maker
from backend.database.models import Project, User, SRSDraft
from backend.routes.auth import get_current_user
from backend.services.interview_service import InterviewService
from backend.services.telemetry_service import TelemetryService

logger = logging.getLogger(__name__)

router = APIRouter(tags=["Interview"])
_interview_service = None


def get_interview_service() -> InterviewService:
    global _interview_service
    if _interview_service is None:
        _interview_service = InterviewService()
    return _interview_service


class InterviewRequest(BaseModel):
    language: str = Field(default="ar", pattern="^(ar|en)$")
    last_summary: Optional[Dict[str, Any]] = None
    last_coverage: Optional[Dict[str, Any]] = None


def _normalize_last_summary(value: Optional[Dict[str, Any]]) -> Optional[Dict[str, List[str]]]:
    if not isinstance(value, dict):
        return None

    normalized: Dict[str, List[str]] = {}
    for key, raw_items in value.items():
        area = str(key).strip()
        if not area:
            continue
        if not isinstance(raw_items, list):
            continue
        items = [str(item).strip() for item in raw_items if str(item).strip()]
        if items:
            normalized[area] = items

    return normalized or None


def _to_float(value: Any) -> Optional[float]:
    if value is None:
        return None
    if isinstance(value, str):
        cleaned = value.strip().replace("%", "")
        if not cleaned:
            return None
        try:
            return float(cleaned)
        except ValueError:
            return None
    try:
        return float(value)
    except (TypeError, ValueError):
        return None


def _normalize_last_coverage(value: Optional[Dict[str, Any]]) -> Optional[Dict[str, float]]:
    if not isinstance(value, dict):
        return None

    normalized: Dict[str, float] = {}
    for key, raw_value in value.items():
        area = str(key).strip()
        if not area:
            continue
        parsed = _to_float(raw_value)
        if parsed is None:
            continue
        normalized[area] = max(0.0, min(parsed, 100.0))

    return normalized or None


class InterviewResponse(BaseModel):
    question: str
    stage: str
    done: bool
    suggested_answers: List[str] | None = None
    summary: Any = None
    coverage: dict | None = None
    signals: dict | None = None
    live_patch: dict | None = None
    cycle_trace: dict | None = None
    topic_navigation: dict | None = None


class InterviewDraftPayload(BaseModel):
    summary: Optional[Dict[str, Any]] = None
    coverage: Optional[Dict[str, Any]] = None
    signals: Optional[Dict[str, Any]] = None
    livePatch: Optional[Dict[str, Any]] = None
    cycleTrace: Optional[Dict[str, Any]] = None
    topicNavigation: Optional[Dict[str, Any]] = None
    stage: str = "discovery"
    mode: bool = False
    lastAssistantQuestion: str = ""
    savedAt: Optional[str] = None
    lang: str = "ar"

    def __init__(self, **data: Any) -> None:  # noqa: ANN401
        if "lang" in data and data["lang"] not in ("ar", "en"):
            data["lang"] = "ar"
        super().__init__(**data)


def _get_project_interview_draft(project: Project) -> Optional[Dict[str, Any]]:
    metadata = project.extra_metadata if isinstance(project.extra_metadata, dict) else {}
    draft = metadata.get("interview_draft")
    return draft if isinstance(draft, dict) else None


async def _get_user_project(db: AsyncSession, project_id: int, user: User) -> Project:
    """Fetch project scoped to the authenticated user. Raises 404 if not found."""
    stmt = select(Project).where(Project.id == project_id, Project.user_id == user.id)
    result = await db.execute(stmt)
    project = result.scalar_one_or_none()
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    return project


@router.post("/projects/{project_id}/interview/next", response_model=InterviewResponse)
async def next_question(
    project_id: int, 
    payload: InterviewRequest, 
    background_tasks: BackgroundTasks,
    user: User = Depends(get_current_user), 
    db: AsyncSession = Depends(get_db)
):
    await _get_user_project(db, project_id, user)
    
    # We do not lock the project here anymore to prevent blocking the fast chatter.
    # The background task will lock the project when updating SRSDraft.
    try:
        service = get_interview_service()
        stmt_draft = select(SRSDraft).where(SRSDraft.project_id == project_id).order_by(SRSDraft.version.desc()).limit(1)
        latest_draft = await db.scalar(stmt_draft)
        
        draft_content = latest_draft.content if latest_draft and isinstance(latest_draft.content, dict) else {}
        current_summary = draft_content.get("summary", {}) if draft_content else {}
        current_coverage = draft_content.get("coverage", {}) if draft_content else {}

        if not current_summary and payload.last_summary:
            current_summary = _normalize_last_summary(payload.last_summary) or {}
        if not current_coverage and payload.last_coverage:
            current_coverage = _normalize_last_coverage(payload.last_coverage) or {}

        # 1. Run the super fast Chatter Agent
        result = await service.get_chat_response(
            db=db, project_id=project_id,
            language=payload.language,
            last_summary=current_summary,
            last_coverage=current_coverage
        )

        # 2. Fire the asynchronous Extractor Agent in the background
        # Pass async_session_maker to create a new session isolated from the HTTP request context
        background_tasks.add_task(
            service.extract_background_patches,
            project_id=project_id,
            language=payload.language,
            session_factory=async_session_maker
        )

        # 3. Return immediately!
        return {
            "question": result["question"],
            "stage": result["stage"],
            "done": result["done"],
            "suggested_answers": result.get("suggested_answers"),
            "summary": current_summary, # we just return the current one, background task updates it
            "coverage": current_coverage,
            "signals": result.get("signals"),
            "live_patch": result.get("live_patch"),
            "cycle_trace": result.get("cycle_trace"),
            "topic_navigation": result.get("topic_navigation"),
        }

    except Exception as e:
        await db.rollback()
        logger.error(f"Interview error: {e}")
        raise HTTPException(status_code=500, detail="Processing error")


@router.get("/projects/{project_id}/interview/telemetry")
async def get_interview_telemetry(
    project_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    await _get_user_project(db, project_id, user)
    report = await TelemetryService.get_report(db=db, project_id=project_id)
    return report


@router.get("/projects/{project_id}/interview/draft")
async def get_interview_draft(
    project_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    project = await _get_user_project(db, project_id, user)
    return {"draft": _get_project_interview_draft(project)}


@router.post("/projects/{project_id}/interview/draft")
async def save_interview_draft(
    project_id: int,
    payload: InterviewDraftPayload,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    project = await _get_user_project(db, project_id, user)
    draft_data = payload.model_dump()
    draft_data["summary"] = _normalize_last_summary(payload.summary)
    draft_data["coverage"] = _normalize_last_coverage(payload.coverage)

    metadata = dict(project.extra_metadata or {})
    metadata["interview_draft"] = draft_data
    project.extra_metadata = metadata
    await db.commit()
    return {"success": True, "draft": draft_data}


@router.delete("/projects/{project_id}/interview/draft")
async def clear_interview_draft(
    project_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    project = await _get_user_project(db, project_id, user)
    metadata = dict(project.extra_metadata or {})
    metadata.pop("interview_draft", None)
    project.extra_metadata = metadata
    await db.commit()
    return {"success": True}

################################################################################
# FILE: backend\routes\judge.py
################################################################################

from __future__ import annotations

from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel, Field
from sqlalchemy.ext.asyncio import AsyncSession

from backend.database import get_db
from backend.services.judging_service import JudgingService

router = APIRouter(tags=["Judging"])
_judging_service: JudgingService | None = None


def get_judging_service() -> JudgingService:
    """
    Judging Service Routes for FastAPI
    Handles endpoints for judging and refining SRS documents.
    Author: Adel Sobhy
    Date: 2026-02-15
    """
    global _judging_service
    if _judging_service is None:
        _judging_service = JudgingService()
    return _judging_service


class JudgingRequest(BaseModel):
    project_id: int
    srs_content: dict
    analysis_content: str = ""
    language: str = Field(default="ar", pattern="^(ar|en)$")
    store_refined: bool = True


class JudgingResponse(BaseModel):
    technical_critique: dict
    business_critique: dict
    refined_srs: dict
    refined_analysis: str
    summary: dict
    timestamp: str


@router.post("/projects/{project_id}/judge", response_model=JudgingResponse)
async def judge_project(
    project_id: int,
    payload: JudgingRequest,
    db: AsyncSession = Depends(get_db),
):
    service = get_judging_service()

    try:
        result = await service.judge_and_refine(
            srs_content=payload.srs_content,
            analysis_content=payload.analysis_content,
            language=payload.language,
            store_refined=payload.store_refined,
            db=db,
            project_id=project_id,
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Judging failed: {str(e)}")

################################################################################
# FILE: backend\routes\messages.py
################################################################################

"""
Project chat message routes.
"""
from __future__ import annotations

from typing import List, Dict, Any, Optional

from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel, Field
from sqlalchemy import select, delete
from sqlalchemy.ext.asyncio import AsyncSession

from backend.database import get_db
from backend.database.models import ChatMessage, Project, User
from backend.routes.auth import get_current_user
from backend.services.live_patch_service import LivePatchService
from backend.services.telemetry_service import TelemetryService

router = APIRouter(tags=["Messages"])


class MessagePayload(BaseModel):
    role: str = Field(..., pattern="^(user|assistant|system)$")
    content: str = Field(..., min_length=1)
    metadata: Dict[str, Any] | None = None


class MessagesRequest(BaseModel):
    messages: List[MessagePayload]


class MessageOut(BaseModel):
    id: int
    role: str
    content: str
    metadata: Optional[Dict[str, Any]] = None
    created_at: str


class LivePatchRequest(BaseModel):
    language: str = Field(default="ar", pattern="^(ar|en)$")
    last_summary: Dict[str, Any] | None = None
    last_coverage: Dict[str, float] | None = None


async def _get_user_project(db: AsyncSession, project_id: int, user: User) -> Project:
    """Fetch project scoped to the authenticated user. Raises 404 if not found."""
    stmt = select(Project).where(Project.id == project_id, Project.user_id == user.id)
    result = await db.execute(stmt)
    project = result.scalar_one_or_none()
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    return project


@router.get("/projects/{project_id}/messages", response_model=List[MessageOut])
async def get_messages(
    project_id: int,
    limit: int = 120,
    before_id: int | None = None,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """Retrieve all chat messages for a project, ordered by creation time."""
    await _get_user_project(db, project_id, user)
    safe_limit = max(1, min(300, int(limit)))
    stmt = (
        select(ChatMessage)
        .where(ChatMessage.project_id == project_id)
        .order_by(ChatMessage.id.desc())
        .limit(safe_limit)
    )
    if before_id is not None:
        stmt = stmt.where(ChatMessage.id < int(before_id))

    result = await db.execute(stmt)
    rows = list(result.scalars().all())
    rows.reverse()
    return [
        MessageOut(
            id=r.id,
            role=r.role,
            content=r.content,
            metadata=r.extra_metadata,
            created_at=r.created_at.isoformat() if r.created_at else "",
        )
        for r in rows
    ]


@router.post("/projects/{project_id}/messages")
async def add_messages(
    project_id: int,
    payload: MessagesRequest,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    await _get_user_project(db, project_id, user)
    if not payload.messages:
        raise HTTPException(status_code=400, detail="No messages provided")

    for msg in payload.messages:
        metadata = msg.metadata if isinstance(msg.metadata, dict) else {}
        is_stt_user_message = msg.role == "user" and str(metadata.get("source", "")).lower() == "stt"
        if is_stt_user_message and not bool(metadata.get("transcript_confirmed")):
            raise HTTPException(
                status_code=400,
                detail="STT transcript requires explicit human confirmation before submission",
            )

    for msg in payload.messages:
        record = ChatMessage(
            project_id=project_id,
            role=msg.role,
            content=msg.content,
            extra_metadata=msg.metadata or {},
        )
        db.add(record)
        await TelemetryService.record_message_event(
            db=db,
            project_id=project_id,
            metadata_payload=msg.metadata,
        )

    await db.commit()
    return {"success": True, "count": len(payload.messages)}


@router.delete("/projects/{project_id}/messages")
async def clear_messages(
    project_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """Delete all chat messages for a project."""
    await _get_user_project(db, project_id, user)
    stmt = delete(ChatMessage).where(ChatMessage.project_id == project_id)
    result = await db.execute(stmt)
    deleted = int(result.rowcount or 0)
    await db.commit()
    return {"success": True, "deleted": deleted}


@router.post("/projects/{project_id}/messages/live-patch")
async def refresh_live_patch(
    project_id: int,
    payload: LivePatchRequest,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """Build and return live SRS patch from full chat history (not interview-only)."""
    project = await _get_user_project(db, project_id, user)

    messages_stmt = (
        select(ChatMessage)
        .where(ChatMessage.project_id == project_id)
        .order_by(ChatMessage.id.desc())
        .limit(120)
    )
    messages_result = await db.execute(messages_stmt)
    messages = list(messages_result.scalars().all())
    messages.reverse()

    metadata = project.extra_metadata if isinstance(project.extra_metadata, dict) else {}
    stored_state = metadata.get("live_patch_state") if isinstance(metadata.get("live_patch_state"), dict) else {}

    last_summary = payload.last_summary
    if not isinstance(last_summary, dict):
        last_summary = stored_state.get("summary") if isinstance(stored_state.get("summary"), dict) else {}

    last_coverage = payload.last_coverage
    if not isinstance(last_coverage, dict):
        last_coverage = stored_state.get("coverage") if isinstance(stored_state.get("coverage"), dict) else {}

    result = await LivePatchService.build_from_messages(
        language=payload.language,
        messages=messages,
        last_summary=last_summary,
        last_coverage=last_coverage,
    )

    next_metadata = dict(metadata)
    next_metadata["live_patch_state"] = {
        "summary": result.get("summary") or {},
        "coverage": result.get("coverage") or {},
    }
    project.extra_metadata = next_metadata
    await db.commit()

    return result


@router.get("/projects/{project_id}/messages/telemetry")
async def get_messages_telemetry(
    project_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    await _get_user_project(db, project_id, user)
    report = await TelemetryService.get_report(db=db, project_id=project_id)
    return report

################################################################################
# FILE: backend\routes\projects.py
################################################################################

"""
Project Routes.
API endpoints for project management.
"""
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
from datetime import datetime
from backend.database import get_db
from backend.database.models import User
from backend.controllers.project_controller import ProjectController
from backend.routes.auth import get_current_user
from backend.errors import is_database_unavailable_error, db_unavailable_http_exception

router = APIRouter(prefix="/projects", tags=["Projects"])
_project_controller = None
PROJECT_NOT_FOUND = "Project not found"


def get_project_controller() -> ProjectController:
    global _project_controller
    if _project_controller is None:
        _project_controller = ProjectController()
    return _project_controller


# Request/Response Models
class ProjectCreate(BaseModel):
    name: str = Field(..., min_length=1, max_length=255)
    description: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


class ProjectUpdate(BaseModel):
    name: Optional[str] = Field(None, min_length=1, max_length=255)
    description: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


class ProjectResponse(BaseModel):
    id: int
    name: str
    description: Optional[str]
    extra_metadata: Dict[str, Any]
    created_at: datetime
    updated_at: Optional[datetime]

    class Config:
        from_attributes = True


class ProjectListResponse(BaseModel):
    id: int
    name: str
    description: Optional[str]
    created_at: datetime
    updated_at: Optional[datetime]

    class Config:
        from_attributes = True


class ProjectStatsResponse(BaseModel):
    project: ProjectResponse
    stats: Dict[str, Any]


# Routes
@router.post("/", response_model=ProjectResponse, status_code=201)
async def create_project(
    project_data: ProjectCreate,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Create a new project."""
    try:
        project_controller = get_project_controller()
        project = await project_controller.create_project(
            db=db,
            name=project_data.name,
            description=project_data.description,
            metadata=project_data.metadata,
            user_id=user.id
        )
        return project
    except Exception as e:
        if is_database_unavailable_error(e):
            raise db_unavailable_http_exception()
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/", response_model=List[ProjectListResponse])
async def list_projects(
    skip: int = 0,
    limit: int = 100,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """List projects owned by the current user."""
    try:
        project_controller = get_project_controller()
        projects = await project_controller.list_projects(db=db, skip=skip, limit=limit, user_id=user.id)
        return projects
    except Exception as e:
        if is_database_unavailable_error(e):
            raise db_unavailable_http_exception()
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/{project_id}", response_model=ProjectResponse)
async def get_project(
    project_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Get project by ID."""
    try:
        project_controller = get_project_controller()
        project = await project_controller.get_project(db=db, project_id=project_id, user_id=user.id)
        if not project:
            raise HTTPException(status_code=404, detail=PROJECT_NOT_FOUND)
        return project
    except HTTPException:
        raise
    except Exception as e:
        if is_database_unavailable_error(e):
            raise db_unavailable_http_exception()
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/{project_id}/stats", response_model=ProjectStatsResponse)
async def get_project_stats(
    project_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Get project statistics."""
    try:
        project_controller = get_project_controller()
        project = await project_controller.get_project(db=db, project_id=project_id, user_id=user.id)
        if not project:
            raise HTTPException(status_code=404, detail=PROJECT_NOT_FOUND)

        stats = await project_controller.get_project_stats(db=db, project_id=project_id)

        return {
            "project": project,
            "stats": stats
        }
    except HTTPException:
        raise
    except Exception as e:
        if is_database_unavailable_error(e):
            raise db_unavailable_http_exception()
        raise HTTPException(status_code=400, detail=str(e))


@router.put("/{project_id}", response_model=ProjectResponse)
async def update_project(
    project_id: int,
    project_data: ProjectUpdate,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Update project."""
    try:
        project_controller = get_project_controller()
        # Verify ownership first
        existing = await project_controller.get_project(db=db, project_id=project_id, user_id=user.id)
        if not existing:
            raise HTTPException(status_code=404, detail=PROJECT_NOT_FOUND)

        project = await project_controller.update_project(
            db=db,
            project_id=project_id,
            name=project_data.name,
            description=project_data.description,
            metadata=project_data.metadata
        )
        return project
    except HTTPException:
        raise
    except Exception as e:
        if is_database_unavailable_error(e):
            raise db_unavailable_http_exception()
        raise HTTPException(status_code=400, detail=str(e))


@router.delete("/{project_id}", status_code=204)
async def delete_project(
    project_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Delete project and all associated data."""
    try:
        project_controller = get_project_controller()
        deleted = await project_controller.delete_project(db=db, project_id=project_id, user_id=user.id)
        if not deleted:
            raise HTTPException(status_code=404, detail=PROJECT_NOT_FOUND)
        return None
    except HTTPException:
        raise
    except Exception as e:
        if is_database_unavailable_error(e):
            raise db_unavailable_http_exception()
        raise HTTPException(status_code=400, detail=str(e))

################################################################################
# FILE: backend\routes\query.py
################################################################################

"""
Query Routes.
API endpoints for querying project transcripts.
"""
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
from backend.database import get_db
from backend.database.models import Project, User
from backend.routes.auth import get_current_user
from backend.controllers.query_controller import QueryController

router = APIRouter(tags=["Query"])
_query_controller = None


def get_query_controller() -> QueryController:
    global _query_controller
    if _query_controller is None:
        _query_controller = QueryController()
    return _query_controller


async def _verify_project_access(db: AsyncSession, project_id: int, user: User):
    """Verify the user owns this project."""
    stmt = select(Project).where(Project.id == project_id, Project.user_id == user.id)
    result = await db.execute(stmt)
    if not result.scalar_one_or_none():
        raise HTTPException(status_code=404, detail="Project not found")


# Request/Response Models
class QueryRequest(BaseModel):
    query: str = Field(..., min_length=1)
    top_k: Optional[int] = Field(default=None, ge=1, le=20)
    language: str = Field(default="ar", pattern="^(ar|en)$")
    asset_id: Optional[int] = None


class SourceInfo(BaseModel):
    document_name: str
    chunk_index: int
    similarity: float
    asset_id: Optional[int] = None


class QueryResponse(BaseModel):
    answer: str
    sources: List[SourceInfo]
    context_used: int


# Routes
@router.post("/projects/{project_id}/query", response_model=QueryResponse)
async def query_project(
    project_id: int,
    query_data: QueryRequest,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Ask a question about project transcript content.
    Returns AI-generated answer with source references.
    """
    await _verify_project_access(db, project_id, user)
    try:
        query_controller = get_query_controller()
        result = await query_controller.answer_query(
            db=db,
            project_id=project_id,
            query=query_data.query,
            top_k=int(query_data.top_k or 5),
            language=query_data.language,
            asset_id=query_data.asset_id
        )

        return result

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/projects/{project_id}/query/stream")
async def query_project_stream(
    project_id: int,
    query_data: QueryRequest,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    Stream an AI-generated answer via Server-Sent Events.
    Emits: source references event, then token events, then [DONE].
    """
    await _verify_project_access(db, project_id, user)
    query_controller = get_query_controller()
    top_k = int(query_data.top_k or 5)

    return StreamingResponse(
        query_controller.answer_query_stream(
            db=db,
            project_id=project_id,
            query=query_data.query,
            top_k=top_k,
            language=query_data.language,
            asset_id=query_data.asset_id,
        ),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
        },
    )

################################################################################
# FILE: backend\routes\srs.py
################################################################################

"""
SRS draft routes.
"""
from __future__ import annotations

import logging

from fastapi import APIRouter, Depends, HTTPException, Response
from pydantic import BaseModel, Field
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

logger = logging.getLogger(__name__)

from backend.database import get_db
from backend.database.models import Project, User
from backend.routes.auth import get_current_user
from backend.services.srs_service import SRSService

router = APIRouter(tags=["SRS"])
_srs_service = None


def get_srs_service() -> SRSService:
    global _srs_service
    if _srs_service is None:
        _srs_service = SRSService()
    return _srs_service


class SRSRefreshRequest(BaseModel):
    language: str = Field(default="ar", pattern="^(ar|en)$")


class SRSDraftResponse(BaseModel):
    project_id: int
    version: int
    status: str
    language: str
    content: dict
    created_at: str | None = None


async def _verify_project_access(db: AsyncSession, project_id: int, user: User):
    """Verify the user owns this project."""
    stmt = select(Project).where(Project.id == project_id, Project.user_id == user.id)
    result = await db.execute(stmt)
    if not result.scalar_one_or_none():
        raise HTTPException(status_code=404, detail="Project not found")


@router.get("/projects/{project_id}/srs", response_model=SRSDraftResponse)
async def get_latest_srs(
    project_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    await _verify_project_access(db, project_id, user)
    service = get_srs_service()
    draft = await service.get_latest_draft(db, project_id)
    if not draft:
        raise HTTPException(status_code=404, detail="SRS draft not found")

    return {
        "project_id": draft.project_id,
        "version": draft.version,
        "status": draft.status,
        "language": draft.language,
        "content": draft.content,
        "created_at": draft.created_at.isoformat() if draft.created_at else None,
    }


@router.post("/projects/{project_id}/srs/refresh", response_model=SRSDraftResponse)
async def refresh_srs(
    project_id: int,
    payload: SRSRefreshRequest,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    await _verify_project_access(db, project_id, user)
    service = get_srs_service()
    try:
        draft = await service.generate_draft(db, project_id, payload.language)
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc
    except Exception as exc:  # noqa: BLE001
        logger.error("SRS generation failed for project %s: %s", project_id, exc)
        raise HTTPException(
            status_code=503,
            detail="SRS generation temporarily unavailable. Please try again or switch the AI provider in settings.",
        ) from exc

    await db.commit()

    return {
        "project_id": draft.project_id,
        "version": draft.version,
        "status": draft.status,
        "language": draft.language,
        "content": draft.content,
        "created_at": draft.created_at.isoformat() if draft.created_at else None,
    }


@router.get("/projects/{project_id}/srs/export")
async def export_srs(
    project_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    await _verify_project_access(db, project_id, user)
    service = get_srs_service()
    draft = await service.get_latest_draft(db, project_id)
    if not draft:
        raise HTTPException(status_code=404, detail="SRS draft not found")

    pdf_bytes = service.export_pdf(draft)
    filename = f"srs_project_{project_id}_v{draft.version}.pdf"
    return Response(
        content=pdf_bytes,
        media_type="application/pdf",
        headers={"Content-Disposition": f"attachment; filename={filename}"},
    )

################################################################################
# FILE: backend\routes\stats.py
################################################################################

"""
Stats Routes.
API endpoints for global statistics.
"""
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import func, select
from backend.database import get_db
from backend.database.models import Project, Asset, User
from backend.routes.auth import get_current_user
from backend.errors import is_database_unavailable_error, db_unavailable_http_exception

router = APIRouter(prefix="/stats", tags=["Stats"])


@router.get("/")
async def get_global_stats(
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """Get statistics scoped to the current user's projects."""
    try:
        user_projects = select(Project.id).where(Project.user_id == user.id)

        project_count = (await db.execute(
            select(func.count()).select_from(Project).where(Project.user_id == user.id)
        )).scalar() or 0

        doc_count = (await db.execute(
            select(func.count()).select_from(Asset).where(Asset.project_id.in_(user_projects))
        )).scalar() or 0

        transcript_count = (await db.execute(
            select(func.count()).select_from(Asset).where(
                Asset.project_id.in_(user_projects),
                Asset.extracted_text.isnot(None),
                Asset.extracted_text != ""
            )
        )).scalar() or 0

        return {"projects": project_count, "documents": doc_count, "transcripts": transcript_count}
    except Exception as e:
        if is_database_unavailable_error(e):
            raise db_unavailable_http_exception()
        raise HTTPException(status_code=500, detail=str(e))

################################################################################
# FILE: backend\routes\stt.py
################################################################################

"""
Speech-to-Text routes with provider failover and circuit-breaker protection.
"""
from __future__ import annotations

import asyncio
import logging
import os
import uuid
from typing import Dict

import aiofiles
from fastapi import APIRouter, File, Form, HTTPException, UploadFile
from pydantic import BaseModel

from backend.config import settings
from backend.providers.llm.factory import LLMProviderFactory
from backend.services.stt_service import (
    GroqWhisperProvider,
    OpenAIWhisperProvider,
    SUPPORTED_LANGUAGES,
    is_allowed_file,
)
from backend.services.resilience_service import run_with_failover, circuit_breakers

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/stt", tags=["Speech-to-Text"])
UPLOAD_CHUNK_SIZE_BYTES = 1024 * 1024


class TranscribeResponse(BaseModel):
    success: bool
    text: str
    language: str
    provider: str
    confidence: float | None = None
    requires_confirmation: bool = True
    quality_warnings: list[str] = []


def _validate_transcribe_request(file: UploadFile, language: str) -> None:
    if not file.filename:
        raise HTTPException(status_code=400, detail="No file selected")

    if not is_allowed_file(file.filename):
        raise HTTPException(status_code=400, detail="Invalid file format")

    if language != "auto" and language not in SUPPORTED_LANGUAGES:
        raise HTTPException(status_code=400, detail="Unsupported language")


def _build_provider_calls(file_path: str, language: str):
    provider_calls = []

    if settings.groq_api_key:
        groq_provider = GroqWhisperProvider(
            api_key=settings.groq_api_key,
            base_url=settings.groq_base_url,
        )
        provider_calls.append(("groq", lambda: _transcribe_async(groq_provider, file_path, language)))

    if settings.openai_api_key:
        openai_provider = OpenAIWhisperProvider(
            api_key=settings.openai_api_key,
            base_url=settings.openai_base_url,
            model_name=settings.openai_stt_model,
        )
        provider_calls.append(("openai", lambda: _transcribe_async(openai_provider, file_path, language)))

    return provider_calls


async def post_process_stt_text(text: str) -> str:
    """Uses LLM to fix messy STT outputs, especially for Egyptian Arabic."""
    if not text or not text.strip():
        return text
    
    system_prompt = """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØµØ­ÙŠØ­ Ø§Ù„Ù†ØµÙˆØµ.
Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙØ¯Ø®Ù„ Ù‡Ùˆ Ù…Ø®Ø±Ø¬Ø§Øª Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ (Speech-to-Text) Ù„Ø¹Ù…ÙŠÙ„ ÙŠØªØ­Ø¯Ø« Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ©. 
Ù‚Ø¯ ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ù†Øµ Ø¹Ù„Ù‰ Ø£Ø®Ø·Ø§Ø¡ Ø§Ø³ØªÙ…Ø§Ø¹ØŒ ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ù…ÙÙ‡ÙˆÙ…Ø©ØŒ ØªØ¯Ø§Ø®Ù„ ÙÙŠ Ø§Ù„Ø­Ø±ÙˆÙØŒ Ø£Ùˆ Ù…ØµØ·Ù„Ø­Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù†ÙØ·Ù‚Øª Ø¨Ø¹Ø§Ù…ÙŠØ© (Ø¹Ùƒ).

Ù…Ù‡Ù…ØªÙƒ:
1. ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ© ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø´ÙˆÙ‡Ø© Ù„ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ.
2. Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø±ÙˆØ­ Ø§Ù„ÙƒÙ„Ø§Ù… ÙˆÙ…Ù‚ØµØ¯Ù‡ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„ÙˆØ§Ø¶Ø­Ø© Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙØµØ­Ù‰ Ù…Ø¨Ø³Ø·Ø©.
3. ÙŠÙÙ…Ù†Ø¹ Ù…Ù†Ø¹Ø§Ù‹ Ø¨Ø§ØªØ§Ù‹ ØªØºÙŠÙŠØ± Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£ØµÙ„ÙŠØŒ Ø£Ùˆ Ø¥Ø¶Ø§ÙØ© Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª/Ù…ØªØ·Ù„Ø¨Ø§Øª Ù…Ù† Ø¹Ù†Ø¯Ùƒ.
4. Ø£Ø¹Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØµØ­Ø­ ÙÙ‚Ø· ÙˆØ¨Ø¯ÙˆÙ† Ø£ÙŠ Ù…Ù‚Ø¯Ù…Ø§ØªØŒ Ø£Ùˆ Ø´Ø±ÙˆØ­Ø§ØªØŒ Ø£Ùˆ Ø¹Ù„Ø§Ù…Ø§Øª ØªÙ†ØµÙŠØµ."""

    try:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù€ LLM Provider Ø§Ù„Ù…ØªØ§Ø­ Ø­Ø§Ù„ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…
        llm = LLMProviderFactory.create_provider()
        corrected_text = await llm.generate_text(
            prompt=text,
            system_prompt=system_prompt,
            temperature=0.1,  # Ø­Ø±Ø§Ø±Ø© Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ø§Ù‹ Ù„Ù…Ù†Ø¹ Ø§Ù„Ù‡Ù„ÙˆØ³Ø© ÙˆØ§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØµØ­ÙŠØ­ ÙÙ‚Ø·
            max_tokens=1500
        )
        return corrected_text.strip()
    except Exception as e:
        logger.error(f"Error in STT post-processing: {e}")
        # ÙÙŠ Ø­Ø§Ù„Ø© ÙØ´Ù„ Ø§Ù„Ù€ LLM Ù„Ø£ÙŠ Ø³Ø¨Ø¨ØŒ Ù†Ø±Ø¬Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ Ø­ØªÙ‰ Ù„Ø§ ÙŠØªØ¹Ø·Ù„ Ø§Ù„Ù†Ø¸Ø§Ù…
        return text


def _format_transcribe_response(result: Dict[str, object], used_provider: str, language: str) -> Dict[str, object]:
    detected_lang = result.get("language", language)
    lang_label = SUPPORTED_LANGUAGES.get(detected_lang, detected_lang)
    quality = result.get("quality") if isinstance(result.get("quality"), dict) else {}

    return {
        "success": True,
        "text": result.get("text", ""),
        "language": lang_label,
        "provider": used_provider,
        "confidence": quality.get("confidence"),
        "requires_confirmation": bool(quality.get("requires_confirmation", True)),
        "quality_warnings": quality.get("warnings") if isinstance(quality.get("warnings"), list) else [],
    }


@router.get("/providers")
async def list_providers() -> Dict[str, object]:
    groq_configured = bool(settings.groq_api_key)
    openai_configured = bool(settings.openai_api_key)
    groq_circuit_open = await circuit_breakers.is_open("stt:groq")
    openai_circuit_open = await circuit_breakers.is_open("stt:openai")
    providers = [
        {
            "name": "groq",
            "display_name": "Groq Whisper",
            "configured": groq_configured,
            "circuit_open": groq_circuit_open,
        },
        {
            "name": "openai",
            "display_name": "OpenAI Whisper",
            "configured": openai_configured,
            "circuit_open": openai_circuit_open,
        },
    ]
    return {
        "providers": providers,
        "default": "groq",
    }


@router.post("/transcribe", response_model=TranscribeResponse)
async def transcribe_audio(
    file: UploadFile = File(...),
    language: str = Form("auto"),
):
    _validate_transcribe_request(file=file, language=language)

    max_size = settings.stt_max_file_size_mb * 1024 * 1024

    ext = os.path.splitext(file.filename)[1].lower()
    safe_name = f"{uuid.uuid4().hex}{ext}"
    upload_dir = os.path.join(settings.upload_dir, "stt")
    os.makedirs(upload_dir, exist_ok=True)
    file_path = os.path.join(upload_dir, safe_name)

    try:
        await _stream_upload_to_disk(
            file=file,
            destination_path=file_path,
            max_size_bytes=max_size,
            chunk_size_bytes=UPLOAD_CHUNK_SIZE_BYTES,
        )

        provider_calls = _build_provider_calls(file_path=file_path, language=language)

        if not provider_calls:
            raise HTTPException(status_code=503, detail="No STT providers configured")

        result, used_provider = await run_with_failover(
            provider_calls,
            breaker_prefix="stt",
            failure_threshold=2,
            cooldown_seconds=60,
        )

        # ====== Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ØªØ¹Ø¯ÙŠÙ„: ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM ======
        original_text = result.get("text", "")
        if original_text:
            corrected_text = await post_process_stt_text(original_text)
            result["text"] = corrected_text
            
            # ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ù„Ù„Ù€ Console Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„ÙØ±Ù‚
            logger.info(f"Original STT: {original_text}")
            logger.info(f"Corrected STT: {corrected_text}")
        # ====================================================

        return _format_transcribe_response(result=result, used_provider=used_provider, language=language)
    except RuntimeError as exc:
        detail = str(exc)
        if "No available providers" in detail or "All providers failed" in detail:
            raise HTTPException(
                status_code=503,
                detail="Speech transcription is temporarily unavailable. Please type your message manually.",
            ) from exc
        raise HTTPException(status_code=500, detail=detail) from exc
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=500, detail=str(exc)) from exc
    finally:
        await file.close()
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
            except OSError:
                pass


async def _transcribe_async(provider, file_path: str, language: str) -> Dict[str, object]:
    return await asyncio.to_thread(provider.transcribe_auto, file_path, language)


async def _stream_upload_to_disk(
    file: UploadFile,
    destination_path: str,
    max_size_bytes: int,
    chunk_size_bytes: int,
) -> int:
    total_bytes = 0
    async with aiofiles.open(destination_path, "wb") as handle:
        while True:
            chunk = await file.read(chunk_size_bytes)
            if not chunk:
                break

            total_bytes += len(chunk)
            if total_bytes > max_size_bytes:
                raise HTTPException(
                    status_code=413,
                    detail=f"File too large. Maximum size is {settings.stt_max_file_size_mb} MB.",
                )

            await handle.write(chunk)

    return total_bytes

################################################################################
# FILE: backend\services\__init__.py
################################################################################

"""Services package initialization.

Keep this module import-light to avoid side effects and circular imports.
"""

__all__ = []

################################################################################
# FILE: backend\services\agent_telemetry.py
################################################################################

"""
Agent telemetry helpers backed by process-local counters.
"""
from __future__ import annotations

from typing import Any, Dict


class AgentTelemetryService:
    _project_state: dict[int, Dict[str, int | bool]] = {}
    _global_suggestions_accepted: int = 0

    @classmethod
    async def record_turn(
        cls,
        project_id: int,
        signals: Dict[str, Any],
        suggested_answers_count: int,
    ) -> None:
        state = dict(cls._project_state.get(int(project_id), {}))
        state.setdefault("total_turns", 0)
        state.setdefault("ambiguity_detected", 0)
        state.setdefault("ambiguity_resolved", 0)
        state.setdefault("contradiction_risk", 0)
        state.setdefault("contradiction_caught", 0)
        state.setdefault("suggestions_shown", 0)
        state.setdefault("last_ambiguity", False)

        state["total_turns"] = int(state["total_turns"]) + 1

        if bool(signals.get("scope_budget_risk")):
            state["contradiction_risk"] = int(state["contradiction_risk"]) + 1
        if bool(signals.get("contradiction_detected")):
            state["contradiction_caught"] = int(state["contradiction_caught"]) + 1

        ambiguity_now = bool(signals.get("ambiguity_detected"))
        if ambiguity_now:
            state["ambiguity_detected"] = int(state["ambiguity_detected"]) + 1
        if bool(state.get("last_ambiguity")) and not ambiguity_now:
            state["ambiguity_resolved"] = int(state["ambiguity_resolved"]) + 1
        state["last_ambiguity"] = ambiguity_now

        shown = max(0, int(suggested_answers_count))
        if shown > 0:
            state["suggestions_shown"] = int(state["suggestions_shown"]) + shown

        cls._project_state[int(project_id)] = state

    @classmethod
    async def record_suggestion_accepted(cls) -> None:
        cls._global_suggestions_accepted = int(cls._global_suggestions_accepted) + 1

    @classmethod
    async def snapshot(cls, project_id: int) -> Dict[str, Any]:
        state = dict(cls._project_state.get(int(project_id), {}))

        turns = int(state.get("total_turns", 0))
        ambiguity_detected = int(state.get("ambiguity_detected", 0))
        ambiguity_resolved = int(state.get("ambiguity_resolved", 0))
        contradiction_risk = int(state.get("contradiction_risk", 0))
        contradiction_caught = int(state.get("contradiction_caught", 0))
        suggestions_shown = int(state.get("suggestions_shown", 0))
        suggestions_accepted = int(cls._global_suggestions_accepted)

        ambiguity_resolution_rate = (
            ambiguity_resolved / ambiguity_detected if ambiguity_detected > 0 else 0.0
        )
        contradiction_catch_rate = (
            contradiction_caught / contradiction_risk if contradiction_risk > 0 else 0.0
        )
        suggestion_acceptance_rate = (
            suggestions_accepted / suggestions_shown if suggestions_shown > 0 else 0.0
        )

        return {
            "turns": turns,
            "ambiguity_resolution_rate": round(ambiguity_resolution_rate, 3),
            "contradiction_catch_rate": round(contradiction_catch_rate, 3),
            "suggestion_acceptance_rate": round(suggestion_acceptance_rate, 3),
            "raw": {
                "ambiguity_detected": ambiguity_detected,
                "ambiguity_resolved": ambiguity_resolved,
                "contradiction_risk": contradiction_risk,
                "contradiction_caught": contradiction_caught,
                "suggestions_shown": suggestions_shown,
                "suggestions_accepted": suggestions_accepted,
            },
        }

################################################################################
# FILE: backend\services\answer_service.py
################################################################################

"""
Answer Generation Service.
Handles generating AI-powered answers using LLM.
"""
from typing import List, Dict, Any, Optional, AsyncIterator
from backend.providers.llm.factory import LLMProviderFactory
from backend.services.resilience_service import run_with_failover, circuit_breakers
from backend.runtime_config import get_runtime_value
from backend.config import settings
import json
import logging

logger = logging.getLogger(__name__)

_ANSWER_SYSTEM_PROMPT = """\
Ø£Ù†Øª Ù…Ù‡Ù†Ø¯Ø³ Ø¨Ø±Ù…Ø¬ÙŠØ§Øª Ø§Ø³ØªØ´Ø§Ø±ÙŠ (Consulting Software Engineer).
Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø­ÙˆÙ„ ØªÙØ§ØµÙŠÙ„ Ù…Ø´Ø±ÙˆØ¹Ù‡ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ§Ø­ØªØ±Ø§ÙÙŠØ©.

## Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ (SRS Snapshot):
{srs_snapshot}

## Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ§Ø±Ù…Ø© (Zero Hallucination Policy):
1. Ø§Ù„ØªÙ‚ÙŠØ¯ Ø§Ù„ØªØ§Ù… Ø¨Ø§Ù„Ø°Ø§ÙƒØ±Ø©: Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ ÙŠØ¬Ø¨ Ø£Ù† ØªÙØ³ØªÙ…Ø¯ Ø­ØµØ±ÙŠØ§Ù‹ Ù…Ù† (SRS Snapshot) Ø§Ù„Ù…Ø±ÙÙ‚ Ø£Ø¹Ù„Ø§Ù‡.
2. Ù…Ù†Ø¹ Ø§Ù„ØªØ£Ù„ÙŠÙ: ÙŠÙÙ…Ù†Ø¹ Ù…Ù†Ø¹Ø§Ù‹ Ø¨Ø§ØªØ§Ù‹ Ø§Ø®ØªØ±Ø§Ø¹ØŒ Ø§ÙØªØ±Ø§Ø¶ØŒ Ø£Ùˆ Ø§Ù‚ØªØ±Ø§Ø­ Ù…ÙŠØ²Ø§Øª Ø£Ùˆ Ø£Ø±Ù‚Ø§Ù… Ø£Ùˆ Ù‚ÙŠÙˆØ¯ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ØµØ±Ø§Ø­Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.
3. Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø¬Ù‡ÙˆÙ„: Ø¥Ø°Ø§ Ø³Ø£Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¹Ù† ØªÙØµÙŠÙ„Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù€ SnapshotØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ¬ÙŠØ¨Ù‡ Ø¨ÙˆØ¶ÙˆØ­: "Ù‡Ø°Ø§ Ø§Ù„Ù…ØªØ·Ù„Ø¨ Ù„Ù… Ù†Ù‚Ù… Ø¨ØªØ­Ø¯ÙŠØ¯Ù‡ Ø£Ùˆ Ù…Ù†Ø§Ù‚Ø´ØªÙ‡ Ø­ØªÙ‰ Ø§Ù„Ø¢Ù† Ø¶Ù…Ù† Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹. Ù‡Ù„ ØªÙˆØ¯ Ø£Ù† Ù†Ø¶ÙŠÙÙ‡ Ø§Ù„Ø¢Ù†ØŸ".
4. Ø§Ù„Ø¥ÙŠØ¬Ø§Ø² ÙˆØ§Ù„Ù…Ù‡Ù†ÙŠØ©: ÙƒÙ† Ù…Ø¨Ø§Ø´Ø±Ø§Ù‹ ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ù„ÙˆØ¨Ø§Ù‹ Ø§Ø­ØªØ±Ø§ÙÙŠØ§Ù‹ ÙŠØ¹ÙƒØ³ Ø®Ø¨Ø±ØªÙƒ Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ©.
5. Ù„ØºØ© Ø§Ù„Ø¹Ù…ÙŠÙ„: Ø£Ø¬Ø¨ Ø¨Ù†ÙØ³ Ø§Ù„Ù„ØºØ© Ø§Ù„ØªÙŠ Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ Ø³Ø¤Ø§Ù„Ù‡.
"""

_ANSWER_SYSTEM_PROMPT_EN = """\
You are a consulting software engineer.
Your task is to answer the client's questions about the current project clearly and professionally.

## Current project memory (SRS Snapshot):
{srs_snapshot}

## Strict answer rules (Zero Hallucination Policy):
1. Memory-only answers: your response must be derived exclusively from the SRS Snapshot above.
2. No fabrication: do not invent assumptions, features, numbers, or constraints that are not explicitly in memory.
3. Unknown handling: if the user asks about a detail not found in memory, respond clearly with: "This requirement has not been defined or discussed yet within the current project scope. Would you like us to add it now?"
4. Concise professionalism: keep answers direct and technically professional.
5. User language: answer in the same language as the user question.
"""


class AnswerService:
    """Service for generating answers from transcript context."""
    
    def __init__(self):
        """Initialize answer service."""
        self.llm_provider = LLMProviderFactory.create_provider()
        logger.info("Answer service initialized")

    @staticmethod
    def _candidate_llm_providers() -> List[str]:
        preferred = [
            "openrouter-gemini-2.0-flash",
            "groq-llama-3.3-70b-versatile",
            "cerebras-llama-3.3-70b",
            "cerebras-llama-3.1-8b",
            "openrouter-free",
            "gemini",
            "gemini-2.5-flash",
            "gemini-2.5-lite-flash",
        ]
        available = set(LLMProviderFactory.get_available_providers())
        if not available:
            return []

        selected = str(get_runtime_value("llm_provider", settings.llm_provider) or "").strip().lower()
        ordered: List[str] = []

        if selected in available:
            ordered.append(selected)

        for name in preferred:
            if name in available and name not in ordered:
                ordered.append(name)

        for name in sorted(available):
            if name not in ordered:
                ordered.append(name)

        return ordered

    async def _generate_text_resilient(
        self,
        *,
        prompt: str,
        temperature: float,
        max_tokens: int,
    ) -> str:
        providers = self._candidate_llm_providers()
        if not providers:
            return await self.llm_provider.generate_text(
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens,
            )

        provider_calls = []
        for provider_name in providers:
            provider_calls.append((
                provider_name,
                lambda pn=provider_name: self._provider_generate_text(
                    provider_name=pn,
                    prompt=prompt,
                    temperature=temperature,
                    max_tokens=max_tokens,
                ),
            ))

        result, _ = await run_with_failover(
            provider_calls,
            breaker_prefix="answer_text",
            failure_threshold=2,
            cooldown_seconds=45,
        )
        return str(result)

    @staticmethod
    async def _provider_generate_text(
        *,
        provider_name: str,
        prompt: str,
        temperature: float,
        max_tokens: int,
    ) -> str:
        provider = LLMProviderFactory.create_provider(provider_name)
        return await provider.generate_text(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
        )

    async def _generate_stream_resilient(
        self,
        *,
        prompt: str,
        temperature: float,
        max_tokens: int,
    ) -> AsyncIterator[str]:
        providers = self._candidate_llm_providers()
        if not providers:
            async for token in self.llm_provider.generate_text_stream(
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens,
            ):
                yield token
            return

        last_error: Exception | None = None
        for provider_name in providers:
            key = f"answer_stream:{provider_name}"
            if await circuit_breakers.is_open(key):
                continue

            provider = LLMProviderFactory.create_provider(provider_name)
            started = False
            try:
                async for token in provider.generate_text_stream(
                    prompt=prompt,
                    temperature=temperature,
                    max_tokens=max_tokens,
                ):
                    started = True
                    yield token
                await circuit_breakers.record_success(key)
                return
            except Exception as exc:  # noqa: BLE001
                last_error = exc
                await circuit_breakers.record_failure(key, threshold=2, cooldown_seconds=45)
                if started:
                    raise
                continue

        if last_error is not None:
            raise last_error
        raise RuntimeError("No available providers for answer streaming")
    
    async def generate_answer(
        self,
        query: str,
        context_chunks: List[Dict[str, Any]],
        language: str = "ar",  # Default to Arabic
        include_sources: bool = True,
        project_context: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Generate answer from query and context.
        
        Args:
            query: User question
            context_chunks: List of relevant transcript context blocks
            language: Response language ('ar' or 'en')
            include_sources: Whether to include source references
            
        Returns:
            Dict with 'answer' and optional 'sources'
        """
        try:
            # Build context from transcript blocks
            context = self._build_context(context_chunks, language)

            # Build prompt
            prompt = self._build_prompt(query, context, language, project_context)
            
            # Generate answer
            answer = await self._generate_text_resilient(
                prompt=prompt,
                temperature=0.7,
                max_tokens=2000,
            )
            
            # Format response
            response = {
                'answer': answer.strip(),
                'context_used': len(context_chunks)
            }
            
            if include_sources:
                response['sources'] = self._extract_sources(context_chunks)
            
            logger.info(f"Generated answer (length={len(answer)})")
            return response
            
        except Exception as e:
            logger.error(f"Error generating answer: {str(e)}")
            raise

    async def generate_answer_stream(
        self,
        query: str,
        context_chunks: List[Dict[str, Any]],
        language: str = "ar",
        project_context: Optional[Dict[str, Any]] = None,
    ) -> AsyncIterator[str]:
        """
        Stream answer tokens from LLM.

        Yields raw text tokens as they arrive from the provider.
        """
        try:
            context = self._build_context(context_chunks, language)
            prompt = self._build_prompt(query, context, language, project_context)

            async for token in self._generate_stream_resilient(
                prompt=prompt,
                temperature=0.7,
                max_tokens=2000,
            ):
                yield token

        except Exception as e:
            logger.error(f"Error streaming answer: {str(e)}")
            raise
    
    def _build_context(self, chunks: List[Dict[str, Any]], language: str = "ar") -> str:
        """
        Build context string from transcript blocks.

        Args:
            chunks: List of context dictionaries
            language: 'ar' or 'en' â€” controls source label language

        Returns:
            Formatted context string
        """
        context_parts = []

        for chunk in chunks:
            metadata = chunk.get('metadata', {})
            doc_name = metadata.get('document_name', 'Unknown')
            content = chunk.get('content', '')

            source_index = len(context_parts) + 1
            label = f"Ù…ØµØ¯Ø± {source_index}" if language == "ar" else f"Source {source_index}"
            context_parts.append(f"[{label} - {doc_name}]\n{content}")
        
        return "\n\n".join(context_parts)
    
    def _build_prompt(
        self,
        query: str,
        context: str,
        language: str,
        project_context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Build prompt for LLM.
        
        Args:
            query: User question
            context: Context text
            language: Response language
            
        Returns:
            Formatted prompt
        """
        response_style = self._response_style_from_query(query=query, language=language)
        project_profile = self._build_project_profile(project_context)

        if language == "ar":
            system_prompt = """Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± ØªÙ‚Ù†ÙŠ/Ø£Ø¹Ù…Ø§Ù„ Ø§Ø­ØªØ±Ø§ÙÙŠ Ø¨Ù…Ø³ØªÙˆÙ‰ Ù…Ø¤Ø³Ø³ÙŠ (Industry-grade) Ù„Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©.
Ø§Ù„Ù‡Ø¯Ù: ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù…Ù„ÙŠØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙ†ÙÙŠØ° ÙˆØªØ®Ø¯Ù… Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø­Ø§Ù„ÙŠ.

Ù‚ÙˆØ§Ø¹Ø¯ Ø¥Ù„Ø²Ø§Ù…ÙŠØ©:
1) Ø§Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø£ÙˆÙ„Ù‹Ø§ØŒ Ø«Ù… Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ù…Ø´Ø±ÙˆØ¹/SRS Ø¥Ù† ØªÙˆÙØ±.
2) Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø¹Ù„ÙˆÙ…Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ ØµØ±Ù‘Ø­ Ø¨Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­ ÙˆÙ„Ø§ ØªØ®ØªÙ„Ù‚.
3) Ø§Ø±Ø¨Ø· ÙƒÙ„ Ù†Ù‚Ø·Ø© Ù…Ù‡Ù…Ø© Ø¨Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ Ù…Ø«Ù„ (Ù…ØµØ¯Ø± 1).
4) Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù‡Ù†ÙŠØ© ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙ†ÙÙŠØ°: Ù‚Ø±Ø§Ø±Ø§ØªØŒ Ø®Ø·ÙˆØ§ØªØŒ Ø£Ùˆ Ù…Ø¹Ø§ÙŠÙŠØ± ÙˆØ§Ø¶Ø­Ø©.
5) Ù„Ø§ ØªÙƒØ±Ù‘Ø± Ù†ØµÙˆØµ Ø¹Ø§Ù…Ø©Ø› Ø®ØµÙ‘Øµ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨.

ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬:
- Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø£ÙˆÙ„Ù‹Ø§ (2-4 Ø³Ø·ÙˆØ±).
- Ø«Ù… Ù†Ù‚Ø§Ø· ØªÙ†ÙÙŠØ°ÙŠØ© Ù‚ØµÙŠØ±Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø© Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª.
- Ø«Ù… Ø¨Ù†Ø¯ "Ù…Ø®Ø§Ø·Ø±/Ø§ÙØªØ±Ø§Ø¶Ø§Øª" Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ø§Ù‚ØµØ©."""

            prompt = f"""{system_prompt}

Ø¨Ø±ÙˆÙØ§ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹:
{project_profile}

Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
{response_style}

Ø§Ù„Ø³ÙŠØ§Ù‚:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„:
{query}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""
        else:
            system_prompt = """You are an enterprise solution assistant (industry-grade) for software projects.
Goal: deliver project-serving, execution-ready answers.

Mandatory rules:
1) Prioritize provided interview transcript context, then use project/SRS profile if available.
2) If evidence is missing, say it clearly and avoid fabrication.
3) Cite key claims inline as (Source 1), (Source 2), etc.
4) Keep outputs implementation-oriented: decisions, steps, and criteria.
5) Avoid generic boilerplate; tailor to this project.

Output format:
- Direct answer first (2-4 lines).
- Then concise action bullets when useful, with citations.
- Add "Risks/Assumptions" when data is incomplete."""

            prompt = f"""{system_prompt}

Project profile:
{project_profile}

Preferred response style:
{response_style}

Context:
{context}

Question:
{query}

Answer:"""
        
        return prompt
    
    def _extract_sources(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Extract source information from chunks.

        Args:
            chunks: List of chunk dictionaries

        Returns:
            List of source information
        """
        sources = []
        for chunk in chunks:
            metadata = chunk.get('metadata', {})
            sources.append({
                'document_name': metadata.get('document_name', 'Unknown'),
                'chunk_index': metadata.get('chunk_index', 0),
                'similarity': chunk.get('similarity', 0.0),
                'asset_id': chunk.get('asset_id')
            })

        return sources

    # ------------------------------------------------------------------
    # No-context fallback (LLM-only mode when no documents are uploaded)
    # ------------------------------------------------------------------

    async def generate_answer_no_context(
        self,
        query: str,
        language: str = "ar",
        project_context: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Generate answer using LLM general knowledge when no documents exist."""
        try:
            prompt = self._build_no_context_prompt(query, language, project_context)
            answer = await self._generate_text_resilient(
                prompt=prompt,
                temperature=0.7,
                max_tokens=2000,
            )
            return {
                'answer': answer.strip(),
                'sources': [],
                'context_used': 0,
            }
        except Exception as e:
            logger.error(f"Error generating no-context answer: {str(e)}")
            raise

    async def generate_answer_no_context_stream(
        self,
        query: str,
        language: str = "ar",
        project_context: Optional[Dict[str, Any]] = None,
    ) -> AsyncIterator[str]:
        """Stream answer tokens using LLM general knowledge when no documents exist."""
        try:
            prompt = self._build_no_context_prompt(query, language, project_context)
            async for token in self._generate_stream_resilient(
                prompt=prompt,
                temperature=0.7,
                max_tokens=2000,
            ):
                yield token
        except Exception as e:
            logger.error(f"Error streaming no-context answer: {str(e)}")
            raise

    @staticmethod
    def _build_no_context_prompt(
        query: str,
        language: str,
        project_context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Build strict memory-only prompt for no-RAG answer mode."""
        snapshot = "{}"
        if isinstance(project_context, dict):
            srs = project_context.get("srs")
            if isinstance(srs, dict):
                content = srs.get("content")
                if isinstance(content, dict) and content:
                    snapshot = json.dumps(content, ensure_ascii=False)

        if language == "ar":
            base = _ANSWER_SYSTEM_PROMPT.format(srs_snapshot=snapshot)
            return f"""{base}

Ø§Ù„Ø³Ø¤Ø§Ù„:
{query}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""

        base = _ANSWER_SYSTEM_PROMPT_EN.format(srs_snapshot=snapshot)
        return f"""{base}

Question:
{query}

Answer:"""

    @staticmethod
    def _build_project_profile(project_context: Optional[Dict[str, Any]]) -> str:
        if not project_context:
            return "N/A"

        profile: Dict[str, Any] = {
            "project_name": project_context.get("project_name") or "",
            "project_description": project_context.get("project_description") or "",
        }

        srs = project_context.get("srs")
        if isinstance(srs, dict):
            profile["srs_version"] = srs.get("version")
            profile["srs_status"] = srs.get("status")
            content = srs.get("content") if isinstance(srs.get("content"), dict) else {}
            if content:
                profile["srs_snapshot"] = content

        serialized = json.dumps(profile, ensure_ascii=False)
        return serialized[:4000] + ("â€¦" if len(serialized) > 4000 else "")

    @staticmethod
    def _response_style_from_query(query: str, language: str) -> str:
        q = (query or "").lower()
        is_ar = language == "ar"

        if any(k in q for k in ["Ù‚Ø§Ø±Ù†", "ÙØ±Ù‚", "Ù…Ù‚Ø§Ø±Ù†Ø©", "compare", "difference", "vs"]):
            return (
                "Ù‚Ø¯Ù‘Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø®ØªØµØ±Ø© Ø¨Ø¬Ø¯ÙˆÙ„/Ù†Ù‚Ø§Ø·: Ø®ÙŠØ§Ø±Ø§ØªØŒ Ù…Ø²Ø§ÙŠØ§ØŒ Ù…Ø®Ø§Ø·Ø±ØŒ ÙˆØªÙˆØµÙŠØ© Ù†Ù‡Ø§Ø¦ÙŠØ©."
                if is_ar else
                "Provide a compact comparison: options, pros, risks, and a final recommendation."
            )
        if any(k in q for k in ["Ø®Ø·ÙˆØ©", "Ø®Ø·ÙˆØ§Øª", "plan", "roadmap", "implement", "ØªÙ†ÙÙŠØ°"]):
            return (
                "Ø£Ø¹Ø· Ø®Ø·Ø© ØªÙ†ÙÙŠØ° Ù…Ø±ØªØ¨Ø© Ø²Ù…Ù†ÙŠÙ‹Ø§ Ù…Ø¹ Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø¹ÙŠØ§Ø± Ù†Ø¬Ø§Ø­ Ù„ÙƒÙ„ Ø®Ø·ÙˆØ©."
                if is_ar else
                "Provide a sequenced implementation plan with priorities and success criteria per step."
            )
        if any(k in q for k in ["template", "ØµÙŠØºØ©", "Ù†Ù…ÙˆØ°Ø¬", "format"]):
            return (
                "Ù‚Ø¯Ù‘Ù… Ù‚Ø§Ù„Ø¨Ù‹Ø§ Ø¬Ø§Ù‡Ø²Ù‹Ø§ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹ Ø­Ù‚ÙˆÙ„ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¹Ø¨Ø¦Ø© Ø³Ø±ÙŠØ¹Ù‹Ø§."
                if is_ar else
                "Provide a ready-to-use template with fillable fields."
            )
        return (
            "Ø£Ø¬Ø¨ Ø¨Ø¥ÙŠØ¬Ø§Ø² Ø§Ø­ØªØ±Ø§ÙÙŠ Ø£ÙˆÙ„Ù‹Ø§ Ø«Ù… Ù†Ù‚Ø§Ø· ØªÙ†ÙÙŠØ°ÙŠØ© Ø¹Ù…Ù„ÙŠØ© Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ù…Ø´Ø±ÙˆØ¹."
            if is_ar else
            "Answer with concise professional summary first, then practical project-specific action bullets."
        )

################################################################################
# FILE: backend\services\constraints_checker.py
################################################################################

"""
Semantic Evaluator for SRS requirements extraction.

Uses LLM to evaluate ambiguity, contradictions, coverage gaps, scope/budget
risks during the conversation.
"""
from __future__ import annotations

import json
import logging
from typing import Any, Dict, List

from pydantic import BaseModel, Field, model_validator

logger = logging.getLogger(__name__)


class SemanticEvaluation(BaseModel):
    is_ambiguous: bool = Field(description="True if the user's answer is vague or lacks specific details.")
    ambiguity_reason: str = Field(description="Reason for ambiguity, if any.")
    missing_scope_risk: bool = Field(description="True if there is a risk of missing scope or budget constraints.")
    contradiction_detected: bool = Field(description="True if the user's answer contradicts previous statements.")
    contradiction_reason: str = Field(description="Reason for contradiction, if any.")

    @model_validator(mode="before")
    @classmethod
    def _unwrap_and_normalise(cls, data: Any) -> Any:
        """
        The LLM sometimes wraps its output in a top-level key like:
            {"evaluation": {"ambiguity": ..., "is_ambiguous": ..., ...}}
        or uses slightly different field names.  This validator:
          1. Unwraps any single-key envelope (evaluation / result / data / response).
          2. Maps common LLM field aliases to the canonical field names.
        """
        if not isinstance(data, dict):
            return data

        # â”€â”€ 1. Unwrap envelope â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ENVELOPE_KEYS = {"evaluation", "result", "data", "response", "output"}
        if len(data) == 1:
            only_key = next(iter(data))
            if only_key.lower() in ENVELOPE_KEYS and isinstance(data[only_key], dict):
                data = data[only_key]

        # â”€â”€ 2. Alias map â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Maps whatever the LLM may send â†’ canonical field name
        ALIASES: Dict[str, str] = {
            # is_ambiguous
            "ambiguous": "is_ambiguous",
            "ambiguity": "is_ambiguous",
            "is_vague": "is_ambiguous",
            # ambiguity_reason
            "ambiguity_details": "ambiguity_reason",
            "ambiguity_explanation": "ambiguity_reason",
            "vagueness_reason": "ambiguity_reason",
            "reason_for_ambiguity": "ambiguity_reason",
            # missing_scope_risk
            "scope_risk": "missing_scope_risk",
            "budget_risk": "missing_scope_risk",
            "scope_budget_risk": "missing_scope_risk",
            "has_scope_risk": "missing_scope_risk",
            # contradiction_detected
            "contradiction": "contradiction_detected",
            "has_contradiction": "contradiction_detected",
            "contradicts": "contradiction_detected",
            # contradiction_reason
            "contradiction_details": "contradiction_reason",
            "contradiction_explanation": "contradiction_reason",
            "reason_for_contradiction": "contradiction_reason",
        }

        normalised: Dict[str, Any] = {}
        for k, v in data.items():
            canonical = ALIASES.get(k.lower(), k)
            normalised[canonical] = v

        # â”€â”€ 3. Fill safe defaults for any still-missing required fields â”€â”€â”€â”€â”€â”€â”€
        normalised.setdefault("is_ambiguous", False)
        normalised.setdefault("ambiguity_reason", "")
        normalised.setdefault("missing_scope_risk", False)
        normalised.setdefault("contradiction_detected", False)
        normalised.setdefault("contradiction_reason", "")

        return normalised


class SemanticEvaluator:
    """LLM-based evaluator for semantic reasoning of user inputs."""

    @classmethod
    async def analyze(
        cls,
        language: str,
        latest_user_answer: str,
        last_summary: Dict[str, Any] | None,
        last_coverage: Dict[str, Any] | None,
    ) -> Dict[str, Any]:
        answer = str(latest_user_answer or "").strip()
        summary_text = json.dumps(last_summary or {}, ensure_ascii=False)

        system_prompt = (
            "You are an expert business analyst evaluating a user's response for a software project requirements gathering interview.\n"
            "Evaluate the user's latest answer against the current summary for ambiguity, scope risks, and contradictions.\n"
            "CRITICAL: Base your evaluation STRICTLY on the provided text. Do not invent context or hallucinate contradictions that do not exist in the text.\n"
            "Return a FLAT JSON object with exactly these keys (no nesting, no wrapper):\n"
            "  is_ambiguous        (bool)\n"
            "  ambiguity_reason    (string)\n"
            "  missing_scope_risk  (bool)\n"
            "  contradiction_detected (bool)\n"
            "  contradiction_reason   (string)"
        )

        user_prompt = (
            f"Language: {language}\n"
            f"Latest Answer: {answer}\n"
            f"Current Summary: {summary_text}\n"
        )

        try:
            from backend.services.interview_service import InterviewService  # avoid circular import
            raw_response, _ = await InterviewService._generate_text_resilient(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.1,
                max_tokens=500,
                breaker_prefix="semantic_evaluator",
                response_format={"type": "json_object"},
            )

            try:
                parsed_json = json.loads(raw_response)
                eval_result = SemanticEvaluation(**parsed_json)
            except Exception as e:
                logger.error("Failed to parse SemanticEvaluation: %s", e)
                eval_result = SemanticEvaluation(
                    is_ambiguous=False,
                    ambiguity_reason="",
                    missing_scope_risk=False,
                    contradiction_detected=False,
                    contradiction_reason="",
                )

            low_areas: List[str] = []
            coverage = last_coverage if isinstance(last_coverage, dict) else {}
            for area in ("discovery", "scope", "users", "features", "constraints"):
                if float(coverage.get(area, 0) or 0) < 35:
                    low_areas.append(area)

            warnings: List[str] = []
            if eval_result.is_ambiguous and eval_result.ambiguity_reason:
                warnings.append(eval_result.ambiguity_reason)
            if eval_result.contradiction_detected and eval_result.contradiction_reason:
                warnings.append(eval_result.contradiction_reason)

            return {
                "ambiguity_detected": eval_result.is_ambiguous,
                "ambiguity_terms": [eval_result.ambiguity_reason] if eval_result.is_ambiguous else [],
                "scope_budget_risk": eval_result.missing_scope_risk,
                "contradiction_detected": eval_result.contradiction_detected,
                "reason": eval_result.contradiction_reason,
                "low_covered_areas": low_areas[:3],
                "warnings": warnings[:4],
            }

        except Exception as e:
            logger.error("SemanticEvaluator failed: %s", e)
            return {
                "ambiguity_detected": False,
                "ambiguity_terms": [],
                "scope_budget_risk": False,
                "contradiction_detected": False,
                "reason": "",
                "low_covered_areas": [],
                "warnings": [],
            }

################################################################################
# FILE: backend\services\file_service.py
################################################################################

"""
File Management Service.
Handles file storage with project-based organization.
"""
import asyncio
import io
import mimetypes
import uuid
import shutil
from pathlib import Path
from typing import Optional, Tuple
from backend.config import settings
import logging
import aiofiles
import boto3

logger = logging.getLogger(__name__)


class FileService:
    """Service for managing uploaded files."""

    SUPPORTED_EXTENSIONS = ['.pdf', '.txt', '.docx']
    
    def __init__(self):
        """Initialize file service."""
        self.object_storage_provider = str(settings.object_storage_provider or "local").strip().lower()
        self.upload_dir = Path(settings.upload_dir)
        self.max_size_bytes = settings.max_file_size_mb * 1024 * 1024
        self._s3_bucket = str(settings.aws_s3_bucket or "").strip()
        self._s3_region = str(settings.aws_s3_region or "us-east-1").strip()
        self._s3_presign_expiry = max(60, int(settings.aws_s3_presign_expiry_seconds or 900))
        self._s3_client = self._build_s3_client() if self._is_s3_enabled() else None
        
        # Create upload directory if it doesn't exist
        if not self._is_s3_enabled():
            self.upload_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"File service initialized in local mode (upload_dir={self.upload_dir})")
        else:
            logger.info("File service initialized in S3 mode (bucket=%s, region=%s)", self._s3_bucket, self._s3_region)

    def _is_s3_enabled(self) -> bool:
        return self.object_storage_provider in {"aws_s3", "s3"}

    def _build_s3_client(self):
        if not self._s3_bucket:
            raise ValueError("AWS_S3_BUCKET must be set when OBJECT_STORAGE_PROVIDER=aws_s3")
        return boto3.client(
            "s3",
            region_name=self._s3_region,
            aws_access_key_id=str(settings.aws_access_key_id or "").strip() or None,
            aws_secret_access_key=str(settings.aws_secret_access_key or "").strip() or None,
            endpoint_url=str(settings.aws_s3_endpoint_url or "").strip() or None,
        )

    def generate_object_key(self, project_id: int, unique_filename: str) -> str:
        return f"projects/{project_id}/documents/{unique_filename}"

    @staticmethod
    def _as_s3_uri(bucket: str, key: str) -> str:
        return f"s3://{bucket}/{key}"

    @staticmethod
    def _parse_s3_uri(file_path: str) -> Tuple[str, str]:
        value = str(file_path or "").strip()
        if not value.startswith("s3://"):
            raise ValueError("Invalid S3 URI")
        body = value[5:]
        bucket, _, key = body.partition("/")
        if not bucket or not key:
            raise ValueError("Invalid S3 URI")
        return bucket, key
    
    def get_project_dir(self, project_id: int) -> Path:
        """
        Get directory path for project.
        
        Args:
            project_id: Project ID
            
        Returns:
            Path to project directory
        """
        project_dir = self.upload_dir / f"project_{project_id}"
        project_dir.mkdir(parents=True, exist_ok=True)
        return project_dir
    
    def generate_unique_filename(self, original_filename: str) -> str:
        """
        Generate unique filename while preserving extension.
        
        Args:
            original_filename: Original file name
            
        Returns:
            Unique filename
        """
        file_ext = Path(original_filename).suffix
        unique_id = uuid.uuid4().hex[:12]
        safe_name = Path(original_filename).stem[:50]  # Limit length
        return f"{safe_name}_{unique_id}{file_ext}"
    
    async def save_upload_file(
        self,
        file_content: bytes,
        filename: str,
        project_id: int
    ) -> tuple[str, str]:
        """
        Save uploaded file to project directory.
        
        Args:
            file_content: File content as bytes
            filename: Original filename
            project_id: Project ID
            
        Returns:
            Tuple of (unique_filename, file_path)
            
        Raises:
            ValueError: If file is too large
        """
        # Check file size
        if len(file_content) > self.max_size_bytes:
            raise ValueError(
                f"File too large ({len(file_content)} bytes). "
                f"Maximum size is {settings.max_file_size_mb}MB"
            )
        
        # Generate unique filename
        unique_filename = self.generate_unique_filename(filename)
        if self._is_s3_enabled():
            key = self.generate_object_key(project_id, unique_filename)
            content_type = mimetypes.guess_type(filename)[0] or "application/octet-stream"

            def _put() -> None:
                assert self._s3_client is not None
                self._s3_client.put_object(
                    Bucket=self._s3_bucket,
                    Key=key,
                    Body=file_content,
                    ContentType=content_type,
                )

            await asyncio.to_thread(_put)
            uri = self._as_s3_uri(self._s3_bucket, key)
            logger.info("Saved file to S3: %s (%s bytes)", uri, len(file_content))
            return unique_filename, uri

        # Local filesystem fallback
        project_dir = self.get_project_dir(project_id)
        file_path = project_dir / unique_filename
        async with aiofiles.open(file_path, 'wb') as f:
            await f.write(file_content)

        logger.info(f"Saved file: {file_path} ({len(file_content)} bytes)")
        return unique_filename, str(file_path)

    async def generate_presigned_upload(
        self,
        *,
        project_id: int,
        filename: str,
        content_type: str,
    ) -> dict:
        if not self._is_s3_enabled():
            raise ValueError("Presigned upload is available only when OBJECT_STORAGE_PROVIDER=aws_s3")

        unique_filename = self.generate_unique_filename(filename)
        key = self.generate_object_key(project_id, unique_filename)
        normalized_content_type = str(content_type or "").strip() or (mimetypes.guess_type(filename)[0] or "application/octet-stream")

        def _presign() -> str:
            assert self._s3_client is not None
            return self._s3_client.generate_presigned_url(
                "put_object",
                Params={
                    "Bucket": self._s3_bucket,
                    "Key": key,
                    "ContentType": normalized_content_type,
                },
                ExpiresIn=self._s3_presign_expiry,
                HttpMethod="PUT",
            )

        upload_url = await asyncio.to_thread(_presign)
        return {
            "upload_url": upload_url,
            "file_key": key,
            "unique_filename": unique_filename,
            "content_type": normalized_content_type,
            "expires_in": self._s3_presign_expiry,
        }
    
    async def delete_file(self, file_path: str) -> bool:
        """
        Delete file from storage.
        
        Args:
            file_path: Path to file
            
        Returns:
            True if deleted successfully
        """
        try:
            if str(file_path or "").startswith("s3://"):
                bucket, key = self._parse_s3_uri(file_path)

                def _delete_s3() -> None:
                    assert self._s3_client is not None
                    self._s3_client.delete_object(Bucket=bucket, Key=key)

                await asyncio.to_thread(_delete_s3)
                logger.info("Deleted S3 object: %s", file_path)
                return True

            path = Path(file_path)
            if path.exists():
                path.unlink()
                logger.info(f"Deleted file: {file_path}")
                return True
            else:
                logger.warning(f"File not found: {file_path}")
                return False
        except Exception as e:
            logger.error(f"Error deleting file: {str(e)}")
            raise
    
    async def delete_project_files(self, project_id: int) -> bool:
        """
        Delete all files for a project.
        
        Args:
            project_id: Project ID
            
        Returns:
            True if deleted successfully
        """
        try:
            if self._is_s3_enabled():
                prefix = f"projects/{project_id}/documents/"

                def _delete_prefix() -> int:
                    assert self._s3_client is not None
                    paginator = self._s3_client.get_paginator("list_objects_v2")
                    deleted = 0
                    for page in paginator.paginate(Bucket=self._s3_bucket, Prefix=prefix):
                        contents = page.get("Contents") or []
                        if not contents:
                            continue
                        objects = [{"Key": obj["Key"]} for obj in contents if obj.get("Key")]
                        if objects:
                            self._s3_client.delete_objects(Bucket=self._s3_bucket, Delete={"Objects": objects})
                            deleted += len(objects)
                    return deleted

                deleted_count = await asyncio.to_thread(_delete_prefix)
                logger.info("Deleted %s S3 objects for project %s", deleted_count, project_id)
                return deleted_count > 0

            project_dir = self.get_project_dir(project_id)
            if project_dir.exists():
                shutil.rmtree(project_dir)
                logger.info(f"Deleted project directory: {project_dir}")
                return True
            return False
        except Exception as e:
            logger.error(f"Error deleting project files: {str(e)}")
            raise
    
    def validate_file(self, filename: str, file_size: int) -> tuple[bool, Optional[str]]:
        """
        Validate file before upload.
        
        Args:
            filename: File name
            file_size: File size in bytes
            
        Returns:
            Tuple of (is_valid, error_message)
        """
        # Check file extension
        ext = Path(filename).suffix.lower()
        if ext not in self.SUPPORTED_EXTENSIONS:
            return False, f"Unsupported file type. Supported: {self.SUPPORTED_EXTENSIONS}"
        
        # Check file size
        if file_size > self.max_size_bytes:
            return False, f"File too large. Maximum size is {settings.max_file_size_mb}MB"
        
        return True, None

    @classmethod
    def get_supported_extensions(cls) -> list[str]:
        return list(cls.SUPPORTED_EXTENSIONS)

    async def extract_text(self, file_path: str) -> str:
        ext = Path(file_path).suffix.lower()
        if str(file_path or "").startswith("s3://"):
            bucket, key = self._parse_s3_uri(file_path)

            def _download() -> bytes:
                assert self._s3_client is not None
                result = self._s3_client.get_object(Bucket=bucket, Key=key)
                body = result.get("Body")
                return body.read() if body is not None else b""

            data = await asyncio.to_thread(_download)
            return await self._extract_text_from_bytes(ext=ext, data=data)

        if ext == '.pdf':
            return await self._load_pdf(file_path)
        if ext == '.txt':
            return await self._load_txt(file_path)
        if ext == '.docx':
            return await self._load_docx(file_path)
        raise ValueError(f"Unsupported file type: {ext}")

    async def _extract_text_from_bytes(self, ext: str, data: bytes) -> str:
        if ext == '.pdf':
            return await self._load_pdf_bytes(data)
        if ext == '.txt':
            return self._load_txt_bytes(data)
        if ext == '.docx':
            return await self._load_docx_bytes(data)
        raise ValueError(f"Unsupported file type: {ext}")

    @staticmethod
    async def _load_pdf(file_path: str) -> str:
        from pypdf import PdfReader

        def extract() -> str:
            reader = PdfReader(file_path)
            parts: list[str] = []
            for page in reader.pages:
                text = page.extract_text()
                if text and text.strip():
                    parts.append(text)
            return "\n\n".join(parts)

        return await asyncio.to_thread(extract)

    @staticmethod
    async def _load_pdf_bytes(file_content: bytes) -> str:
        from pypdf import PdfReader

        def extract() -> str:
            reader = PdfReader(io.BytesIO(file_content))
            parts: list[str] = []
            for page in reader.pages:
                text = page.extract_text()
                if text and text.strip():
                    parts.append(text)
            return "\n\n".join(parts)

        return await asyncio.to_thread(extract)

    @staticmethod
    async def _load_txt(file_path: str) -> str:
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as handle:
                return await handle.read()
        except UnicodeDecodeError:
            async with aiofiles.open(file_path, 'r', encoding='latin-1') as handle:
                return await handle.read()

    @staticmethod
    def _load_txt_bytes(file_content: bytes) -> str:
        try:
            return file_content.decode("utf-8")
        except UnicodeDecodeError:
            return file_content.decode("latin-1", errors="replace")

    @staticmethod
    async def _load_docx(file_path: str) -> str:
        from docx import Document

        def extract() -> str:
            doc = Document(file_path)
            parts: list[str] = []
            for para in doc.paragraphs:
                if para.text and para.text.strip():
                    parts.append(para.text)
            return "\n\n".join(parts)

        return await asyncio.to_thread(extract)

    @staticmethod
    async def _load_docx_bytes(file_content: bytes) -> str:
        from docx import Document

        def extract() -> str:
            doc = Document(io.BytesIO(file_content))
            parts: list[str] = []
            for para in doc.paragraphs:
                if para.text and para.text.strip():
                    parts.append(para.text)
            return "\n\n".join(parts)

        return await asyncio.to_thread(extract)

################################################################################
# FILE: backend\services\interview_service.py
################################################################################

"""
Guided interview service -- smart business analyst agent.

Uses a dynamic slot-filling state machine that classifies information
into the right SRS area on every turn, continuously populating the SRS
JSON during the conversation (not at the end). Tracks coverage per area
and produces structured summaries.
"""
from __future__ import annotations

from copy import deepcopy
import json
import logging
import re
from uuid import uuid4
from typing import Any, Dict, List

from sqlalchemy import func, select
from sqlalchemy.ext.asyncio import AsyncSession

from backend.database.models import ChatMessage, SRSDraft
from backend.providers.llm.factory import LLMProviderFactory
from backend.services.resilience_service import run_with_failover

logger = logging.getLogger(__name__)

_ZERO_COVERAGE = {"discovery": 0, "scope": 0, "users": 0, "features": 0, "constraints": 0}
_MAX_RECENT_MESSAGES = 50
_MAX_MESSAGE_CHARS = 5000
_MAX_CONTEXT_CHARS = 12000
_MAX_SRS_CONTEXT_CHARS = 4000
_MAX_COVERAGE_DECAY_DEFAULT = 8.0
_MAX_COVERAGE_DECAY_RISK = 18.0
_MIN_INTERVIEW_TURNS = 8
_MAX_USER_MESSAGE_CHARS = 3000  # Truncate user messages longer than this before sending to LLM
_COMPLETION_THRESHOLDS = {
    "discovery": 80,
    "scope": 70,
    "users": 70,
    "features": 65,
    "constraints": 60,
}
_OPEN_QUESTION_MARKERS_EN = {"?", "can you", "could you", "which", "what", "when", "where", "who", "how"}
_OPEN_QUESTION_MARKERS_AR = {"ØŸ", "Ù‡Ù„", "Ù…Ø§", "Ù…Ø§Ø°Ø§", "Ù…ØªÙ‰", "Ø§ÙŠÙ†", "Ø£ÙŠÙ†", "Ù…Ù†", "ÙƒÙŠÙ"}

_AMBIGUOUS_TERMS_EN = {"good", "fast", "strong", "normal", "many", "simple", "best", "powerful", "quick"}
_AMBIGUOUS_TERMS_AR = {"ÙƒÙˆÙŠØ³", "Ø³Ø±ÙŠØ¹", "Ù‚ÙˆÙŠ", "Ø¹Ø§Ø¯ÙŠ", "ÙƒØªÙŠØ±", "Ø¨Ø³ÙŠØ·", "Ø£ÙØ¶Ù„", "Ø§Ø­ØªØ±Ø§ÙÙŠ"}
_BUDGET_HINT_PATTERN = re.compile(r"\b(\d{2,})(\s?\$|\s?usd|\s?Ø¯ÙˆÙ„Ø§Ø±)?\b", re.IGNORECASE)
_SCOPE_HINT_EN = {"uber", "marketplace", "real-time", "dashboard", "payments", "multi-tenant"}
_SCOPE_HINT_AR = {"Ø£ÙˆØ¨Ø±", "Ù„ÙˆØ­Ø©", "Ù…Ø¯ÙÙˆØ¹Ø§Øª", "Ø³ÙˆÙ‚", "Ù…ØªØ¹Ø¯Ø¯", "ØªØ·Ø¨ÙŠÙ‚"}
_TOPIC_STOPWORDS = {
        "the", "and", "for", "with", "that", "this", "from", "into", "your", "have", "will", "are",
        "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù„Ù‰", "ÙÙŠ", "Ø¹Ù†", "Ù…Ø¹", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø§Ù„Ù‰", "Ø§Ù„ØªÙŠ", "Ø§Ù„Ø°ÙŠ", "ØªÙ…", "Ø§Ùˆ", "Ø£Ùˆ",
}

_ENTITY_ALIASES = {
        "database": {"database", "db", "postgres", "mysql", "Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª", "Ø¯Ø§ØªØ§Ø¨ÙŠØ²"},
        "reports": {"reports", "reporting", "dashboard", "analytics", "ØªÙ‚Ø§Ø±ÙŠØ±", "Ù„ÙˆØ­Ø©", "ØªØ­Ù„ÙŠÙ„Ø§Øª"},
        "realtime": {"real-time", "realtime", "live", "Ù„Ø­Ø¸ÙŠ", "Ù…Ø¨Ø§Ø´Ø±"},
        "payments": {"payment", "payments", "Ø¯ÙØ¹", "Ù…Ø¯ÙÙˆØ¹Ø§Øª"},
}
_NEGATION_TOKENS = {"no", "without", "exclude", "drop", "remove", "Ø¨Ø¯ÙˆÙ†", "Ø¥Ù„ØºØ§Ø¡", "Ø§Ø³ØªØ¨Ø¹Ø§Ø¯", "Ø­Ø°Ù"}
_ARABIC_CHAR_PATTERN = re.compile(r"[\u0600-\u06FF]")
_LATIN_CHAR_PATTERN = re.compile(r"[A-Za-z]")

_EN_SYSTEM = """\
You are an expert Business Analyst acting as a fast, conversational Exploratory Agent.
Your task is to conduct an interview with a NON-TECHNICAL client to elicit software requirements.

Return ONLY valid JSON with keys:
    "question": "your response to the user",
    "stage": "discovery|scope|users|features|constraints",
    "done": boolean (true if the user has provided enough details to cover the majority of the system)

Interview Techniques & Rules (CRITICAL):
1. The user is non-technical. Absolutely no software engineering jargon.
2. If the user provides a massive, detailed message (like a full list of requirements or a business plan), acknowledge it quickly: e.g., "This is excellent detail, I am saving this right now." and ask ONE follow up question. You DO NOT need to extract the data yourself, a background agent will do it.
3. Playback / Active Listening: Start your 'question' field by briefly paraphrasing what the user just said to validate their input.
4. Jobs-to-be-Done (JTBD): Focus on the business outcome or task the user wants to achieve, not UI elements.
5. The 5 Whys & Avoid Premature Solutions: If the user suggests a specific app clone or a technical solution, gently pivot by asking "Why?".
6. Always respond in the exact same language as the user's latest message.
7. No markdown. No text outside JSON.
8. Anti-Hallucination: Do not invent, assume, or suggest features, constraints, or numbers that the user has not explicitly mentioned. Base your questions strictly on the provided context.
"""

_AR_SYSTEM = """\
Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø£Ø¹Ù…Ø§Ù„ Ø®Ø¨ÙŠØ± ÙˆØªØ¹Ù…Ù„ ÙƒÙˆÙƒÙŠÙ„ Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ Ø³Ø±ÙŠØ¹ (Fast Conversational Agent).
Ù…Ù‡Ù…ØªÙƒ Ø¥Ø¬Ø±Ø§Ø¡ Ù…Ù‚Ø§Ø¨Ù„Ø© Ù…Ø¹ Ø¹Ù…ÙŠÙ„ ØºÙŠØ± ØªÙ‚Ù†ÙŠ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù….

ÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ø¬Ø¹ JSON ØµØ­ÙŠØ­ ÙÙ‚Ø· ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„ØªØ§Ù„ÙŠØ©:
    "question": "Ù†Øµ Ø³Ø¤Ø§Ù„Ùƒ Ø£Ùˆ Ø±Ø¯Ùƒ",
    "stage": "Ø¥Ø­Ø¯Ù‰ Ø§Ù„Ù…Ø±Ø§Ø­Ù„ discovery|scope|users|features|constraints",
    "done": Ù‚ÙŠÙ…Ø© Ù…Ù†Ø·Ù‚ÙŠØ© true ÙÙŠ Ø­Ø§Ù„ Ù‚Ø¯Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„ ØªÙØ§ØµÙŠÙ„ ØªÙƒÙÙŠ Ù„ØªØºØ·ÙŠØ© Ø§Ù„Ù†Ø¸Ø§Ù…

Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„Ø­ÙˆØ§Ø± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (CRITICAL):
1. Ø§Ù„Ø¹Ù…ÙŠÙ„ ØºÙŠØ± ØªÙ‚Ù†ÙŠ: ØªØ¬Ù†Ø¨ ØªÙ…Ø§Ù…Ø§Ù‹ Ø£ÙŠ Ù…ØµØ·Ù„Ø­Ø§Øª Ø¨Ø±Ù…Ø¬ÙŠØ©ØŒ ÙˆØªØ­Ø¯Ø« Ø¨Ù„ØºØ© Ø§Ù„Ø¨ÙŠØ²Ù†Ø³ Ø§Ù„ÙŠÙˆÙ…ÙŠØ©.
2. Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø³Ø±Ø¯ Ø§Ù„ÙƒØ«ÙŠÙ: Ø¥Ø°Ø§ Ù‚Ø§Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨ØªÙ‚Ø¯ÙŠÙ… Ø±Ø³Ø§Ù„Ø© Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹ Ø£Ùˆ Ø¯Ø±Ø§Ø³Ø© Ø¬Ø¯ÙˆÙ‰ØŒ Ù‚Ù… Ø¨Ø§Ù„Ø±Ø¯ Ø§Ù„Ø³Ø±ÙŠØ¹ Ù„Ø´ÙƒØ± Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙˆØ·Ù…Ø£Ù†ØªÙ‡ Ø£Ù†Ùƒ ØªØ³Ø¬Ù„Ù‡Ø§ Ø§Ù„Ø¢Ù† (Ù…Ø«Ù„Ø§Ù‹: "Ù‡Ø°Ù‡ ØªÙØ§ØµÙŠÙ„ Ù…Ù…ØªØ§Ø²Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸Ù‡Ø§ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ÙÙŠ Ù…Ø³ØªÙ†Ø¯ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª.") Ø«Ù… Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹ ÙÙ‚Ø· Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø©. Ù„Ø³Øª Ø¨Ø­Ø§Ø¬Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†ÙØ³ÙƒØŒ Ù‡Ù†Ø§Ùƒ ÙˆÙƒÙŠÙ„ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© ÙŠÙ‚ÙˆÙ… Ø¨Ø°Ù„Ùƒ.
3. Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ (Playback): Ø§Ø¨Ø¯Ø£ Ø±Ø¯Ùƒ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨ØªØ£ÙƒÙŠØ¯ ÙÙ‡Ù…Ùƒ Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨Ø£Ø³Ù„ÙˆØ¨Ùƒ Ø§Ù„Ø®Ø§Øµ Ù„ÙŠØ´Ø¹Ø± Ø¨Ø§Ù„Ø«Ù‚Ø©.
4. Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¥Ù†Ø¬Ø§Ø²Ù‡Ø§ (JTBD): Ø±ÙƒØ² Ø¹Ù„Ù‰ "Ø§Ù„Ù†ØªÙŠØ¬Ø©" Ø§Ù„ØªÙŠ ÙŠØ±ÙŠØ¯ Ø§Ù„Ø¹Ù…ÙŠÙ„ ØªØ­Ù‚ÙŠÙ‚Ù‡Ø§ØŒ ÙˆÙ„ÙŠØ³ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø£Ùˆ Ø§Ù„Ø£Ø²Ø±Ø§Ø±.
5. ØªÙ‚Ù†ÙŠØ© (5 Whys): Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù…ÙŠØ²Ø© Ù…Ø¹ÙŠÙ†Ø© ÙƒØ­Ù„ Ø¬Ø§Ù‡Ø²ØŒ Ø§Ø³Ø£Ù„Ù‡ Ø¨Ù„Ø·Ù "Ù„ÙŠÙ‡ Ù…Ø­ØªØ§Ø¬ÙŠÙ† Ù†Ø¹Ù…Ù„ Ø¯Ù‡ØŸ" Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø¬Ø°Ø±ÙŠ.
6. Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø¯ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ù†ÙØ³ Ù„ØºØ© Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ (Ø¹Ø±Ø¨ÙŠ Ø£Ùˆ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ).
7. Ù…Ù†Ø¹ Ø§Ù„ØªØ£Ù„ÙŠÙ (Anti-Hallucination): Ù„Ø§ ØªÙ‚Ù… Ø¨Ø§Ø®ØªØ±Ø§Ø¹ Ø£Ùˆ Ø§ÙØªØ±Ø§Ø¶ Ø£Ùˆ Ø§Ù‚ØªØ±Ø§Ø­ Ù…ÙŠØ²Ø§Øª Ø£Ùˆ Ø£Ø±Ù‚Ø§Ù… Ù„Ù… ÙŠØ°ÙƒØ±Ù‡Ø§ Ø§Ù„Ø¹Ù…ÙŠÙ„. 
8. Ø¥Ø±Ø¬Ø§Ø¹ JSON ÙÙ‚Ø·.
"""


_SEMANTIC_EXTRACTION_SYSTEM_EN = """\
You are the Extraction node in an agentic requirements interview graph.
Your role is to act as an \"Information Sponge\". Read the latest user answer and project memory, then output ONLY JSON with strict structure.

Required JSON keys:
- slots: object with keys discovery|scope|users|features|constraints, each value is an array of concise requirements.
- ambiguity_detected: boolean.
- contradiction_detected: boolean.
- scope_budget_risk: boolean.
- reason: concise string explaining the main risk/conflict (or empty).
- confidence: number from 0 to 1.

Omnivorous Extraction Rules (CRITICAL):
1. Absolute Assimilation: Capture ANY business or technical requirement mentioned by the user and classify it into the correct slot, EVEN IF it does not directly answer your previous question. Do not ignore unprompted details.
2. Bulk Data Handling: If the user provides a massive dump of information, stories, or examples, extract every single feature, constraint, or user role embedded in the text as a separate item in the slots.
3. Extract implicit requirements from the user's narrative, not just direct answers.
4. Semantic interpretation only; do not rely on exact keywords.
5. Anti-Hallucination: While extracting implicit needs, NEVER invent net-new features or assumptions. Every extracted slot must have a direct basis in the user's narrative.
6. Return JSON only.
"""


_SEMANTIC_EXTRACTION_SYSTEM_AR = """\
Ø£Ù†Øª Ø¹Ù‚Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ (Extraction Node) ÙÙŠ Ù…Ø®Ø·Ø· ÙˆÙƒÙ„Ø§Ø¡ Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª.
Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¹Ù…Ù„ ÙƒÙ€ \"Ø¥Ø³ÙÙ†Ø¬Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\". Ø§Ù‚Ø±Ø£ Ø¢Ø®Ø± Ø±Ø¯ Ù„Ù„Ø¹Ù…ÙŠÙ„ ÙˆØ°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø«Ù… Ø£Ø¹Ø¯ JSON ÙÙ‚Ø· Ø¨Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªØ§Ù„ÙŠ.

Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
- slots: ÙƒØ§Ø¦Ù† ÙŠØ­ØªÙˆÙŠ discovery|scope|users|features|constraints ÙˆÙƒÙ„ Ù‚ÙŠÙ…Ø© Ù…ØµÙÙˆÙØ© Ù…ØªØ·Ù„Ø¨Ø§Øª Ù‚ØµÙŠØ±Ø©.
- ambiguity_detected: Ù‚ÙŠÙ…Ø© Ù…Ù†Ø·Ù‚ÙŠØ©.
- contradiction_detected: Ù‚ÙŠÙ…Ø© Ù…Ù†Ø·Ù‚ÙŠØ©.
- scope_budget_risk: Ù‚ÙŠÙ…Ø© Ù…Ù†Ø·Ù‚ÙŠØ©.
- reason: Ù†Øµ Ù…ÙˆØ¬Ø² ÙŠØ´Ø±Ø­ Ø£Ù‡Ù… Ø®Ø·Ø±/ØªØ¹Ø§Ø±Ø¶ (Ø£Ùˆ ÙØ§Ø±Øº).
- confidence: Ø±Ù‚Ù… Ù…Ù† 0 Ø¥Ù„Ù‰ 1.

Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„ (CRITICAL RULES):
1. Ø§Ù„Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„Ù…Ø·Ù„Ù‚ (Omnivorous Extraction): Ø§Ù„ØªÙ‚Ø· **Ø£ÙŠ** Ù…Ø¹Ù„ÙˆÙ…Ø© ØªÙ‚Ù†ÙŠØ© Ø£Ùˆ ØªØ¬Ø§Ø±ÙŠØ© ÙŠØ°ÙƒØ±Ù‡Ø§ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙˆØµÙ†ÙÙ‡Ø§ ÙÙŠ Ø§Ù„Ù€ slot Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ØŒ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø¥Ø¬Ø§Ø¨ØªÙ‡ Ù„Ø§ Ø¹Ù„Ø§Ù‚Ø© Ù„Ù‡Ø§ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø·Ø±ÙˆØ­. Ù„Ø§ ØªØªØ¬Ø§Ù‡Ù„ Ø£ÙŠ ØªÙØµÙŠÙ„Ø©.
2. Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø³Ø±Ø¯ Ø§Ù„ÙƒØ«ÙŠÙ (Bulk Data): Ø¥Ø°Ø§ Ù‚Ø§Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨Ø°ÙƒØ± ØªÙØ§ØµÙŠÙ„ ÙƒØ«ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© (Ø±ØºÙŠ/Ø£Ù…Ø«Ù„Ø©/Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª)ØŒ Ø§Ø³ØªØ®Ø±Ø¬ ÙƒÙ„ Ù…ÙŠØ²Ø©ØŒ Ø£Ùˆ Ù‚ÙŠØ¯ØŒ Ø£Ùˆ Ø¯ÙˆØ± Ù…Ø³ØªØ®Ø¯Ù… Ø°ÙƒØ± ÙÙŠ Ø§Ù„Ø³Ø±Ø¯ ÙˆØ¶Ø¹Ù‡ ÙƒØ¹Ù†ØµØ± Ù…Ø³ØªÙ‚Ù„ ÙÙŠ Ø§Ù„Ù€ slots.
3. Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø³ØªÙ‚Ù„: Ù„Ø§ ØªØ¹ØªÙ…Ø¯ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ØŒ Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¶Ù…Ù†ÙŠØ© (Implicit Requirements) Ø¯Ø§Ø®Ù„ ÙƒÙ„Ø§Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„.
4. Ø§Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ ÙˆÙ„ÙŠØ³ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø­Ø±ÙÙŠØ©.
5. Ù…Ù†Ø¹ Ø§Ù„ØªØ£Ù„ÙŠÙ (Anti-Hallucination): Ø£Ø«Ù†Ø§Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¶Ù…Ù†ÙŠØ©ØŒ ÙŠÙÙ…Ù†Ø¹ ØªÙ…Ø§Ù…Ø§Ù‹ Ø§Ø®ØªØ±Ø§Ø¹ Ù…ÙŠØ²Ø§Øª Ø£Ùˆ Ø§ÙØªØ±Ø§Ø¶Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù„ÙƒÙ„ Ù…ØªØ·Ù„Ø¨ Ù…Ø³ØªØ®Ø±Ø¬ Ø£Ø³Ø§Ø³ Ù…Ø¨Ø§Ø´Ø± ÙÙŠ Ø³Ø±Ø¯ Ø§Ù„Ø¹Ù…ÙŠÙ„.
6. Ø£Ø¹Ø¯ JSON ÙÙ‚Ø·.
"""


class InterviewService:
    """Smart business analyst agent for free-flowing requirements gathering."""

    @staticmethod
    def _candidate_llm_providers() -> List[str]:
        preferred = [
            "openrouter-gemini-2.0-flash",
            "groq-llama-3.3-70b-versatile",
            "cerebras-llama-3.3-70b",
            "cerebras-llama-3.1-8b",
            "openrouter-free",
            "gemini",
            "gemini-2.5-flash",
        ]
        available = set(LLMProviderFactory.get_available_providers())
        return [name for name in preferred if name in available] or list(available)

    @classmethod
    async def _generate_text_resilient(
        cls,
        *,
        prompt: str,
        system_prompt: str,
        temperature: float,
        max_tokens: int,
        breaker_prefix: str,
        response_format: Dict[str, Any] | None = None,
    ) -> tuple[str, Any]:
        providers = cls._candidate_llm_providers()
        if not providers:
            provider = LLMProviderFactory.create_provider()
            raw = await provider.generate_text(
                prompt=prompt,
                system_prompt=system_prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                response_format=response_format,
            )
            return raw, provider

        provider_calls = []
        for provider_name in providers:
            provider_calls.append((
                provider_name,
                lambda pn=provider_name: cls._provider_generate_text(
                    provider_name=pn,
                    prompt=prompt,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    response_format=response_format,
                ),
            ))

        result, used_provider_name = await run_with_failover(
            provider_calls,
            breaker_prefix=breaker_prefix,
            failure_threshold=2,
            cooldown_seconds=45,
        )
        provider = LLMProviderFactory.create_provider(used_provider_name)
        return result, provider

    @staticmethod
    async def _provider_generate_text(
        *,
        provider_name: str,
        prompt: str,
        system_prompt: str,
        temperature: float,
        max_tokens: int,
        response_format: Dict[str, Any] | None = None,
    ) -> str:
        provider = LLMProviderFactory.create_provider(provider_name)
        return await provider.generate_text(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            response_format=response_format,
        )

    async def _run_orchestrator_turn(
        self,
        language: str,
        conversation: str,
        latest_user_answer: str,
        last_summary: Dict | None,
        last_coverage: Dict | None,
        srs_context: Dict[str, Any] | None,
    ) -> Dict[str, Any]:
        agentic_reflector = await self._run_agentic_graph(
            language=language,
            latest_user_answer=latest_user_answer,
            last_summary=last_summary,
            last_coverage=last_coverage,
        )
        reflector_signals = dict(agentic_reflector)
        reflector_signals["engine"] = "agentic"

        target_stage = reflector_signals.get("target_stage") or self._pick_focus_area(last_coverage or {})
        plan_state = {
            "target_stage": target_stage,
            "question_style": reflector_signals.get("question_style") or "inference-driven",
        }

        system_prompt, user_prompt = self._build_prompt(
            conversation,
            language,
            last_summary,
            last_coverage,
            reflector_signals,
            srs_context=srs_context,
        )
        raw, _ = await self._generate_text_resilient(
            prompt=user_prompt,
            system_prompt=system_prompt,
            temperature=0.2,
            max_tokens=4000,  # Increased to reduce truncation risk
            breaker_prefix="interview_main",
            response_format={"type": "json_object"},
        )
        interviewer_output = self._parse_json(raw)
        interviewer_output = self._normalize_interviewer_output(
            payload=interviewer_output,
            language=language,
            target_stage=target_stage,
            fallback_coverage=last_coverage,
        )

        return {
            "reflector_signals": reflector_signals,
            "plan_state": plan_state,
            "interviewer_output": interviewer_output,
            "slot_analysis": {
                "mode": "agentic_graph",
                "agentic": agentic_reflector,
            },
        }

    @staticmethod
    def _normalize_interviewer_output(
        payload: Dict[str, Any] | None,
        language: str,
        target_stage: str,
        fallback_coverage: Dict[str, Any] | None,
    ) -> Dict[str, Any]:
        data = payload if isinstance(payload, dict) else {}
        question = str(data.get("question") or "").strip()
        if not question:
            question = (
                "Ø´ÙƒØ±Ø§Ù‹ Ø¹Ù„Ù‰ Ù…Ø´Ø§Ø±ÙƒØªÙƒ! Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø®Ø¨Ø§Ø±Ù†Ø§ Ø¨Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø¹Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ØŸ"
                if language == "ar"
                else "Thank you for sharing! Could you tell us more details about what you'd like to build?"
            )
        stage = str(data.get("stage") or target_stage or "discovery").strip().lower()
        if stage not in _ZERO_COVERAGE:
            stage = target_stage if target_stage in _ZERO_COVERAGE else "discovery"

        raw_coverage = data.get("coverage") if isinstance(data.get("coverage"), dict) else {}
        base_coverage = fallback_coverage if isinstance(fallback_coverage, dict) else {}
        merged_coverage: Dict[str, float] = {}
        for area in _ZERO_COVERAGE:
            source_value = raw_coverage.get(area, base_coverage.get(area, 0))
            try:
                numeric = float(source_value)
            except (TypeError, ValueError):
                numeric = float(base_coverage.get(area, 0) or 0)
            merged_coverage[area] = max(0.0, min(100.0, numeric))

        patches = data.get("patches") if isinstance(data.get("patches"), list) else []
        suggested_answers = data.get("suggested_answers") if isinstance(data.get("suggested_answers"), list) else []

        return {
            "question": question,
            "stage": stage,
            "done": bool(data.get("done", False)),
            "suggested_answers": suggested_answers,
            "patches": patches,
            "coverage": merged_coverage,
            "summary": data.get("summary", {}),
        }

    async def _run_agentic_graph(
        self,
        *,
        language: str,
        latest_user_answer: str,
        last_summary: Dict | None,
        last_coverage: Dict | None,
    ) -> Dict[str, Any]:
        extraction = await self._node_extraction(
            language=language,
            latest_user_answer=latest_user_answer,
            last_summary=last_summary,
            last_coverage=last_coverage,
        )
        critique = self._node_critique(
            language=language,
            extraction=extraction,
            last_coverage=last_coverage,
        )
        routing = self._node_routing(
            language=language,
            critique=critique,
            last_coverage=last_coverage,
        )
        return {
            **critique,
            **routing,
            "agentic_graph": {
                "extraction": extraction,
                "critique": critique,
                "routing": routing,
            },
        }

    async def _node_extraction(
        self,
        *,
        language: str,
        latest_user_answer: str,
        last_summary: Dict | None,
        last_coverage: Dict | None,
    ) -> Dict[str, Any]:
        system_prompt = _SEMANTIC_EXTRACTION_SYSTEM_AR if language == "ar" else _SEMANTIC_EXTRACTION_SYSTEM_EN
        prompt = (
            f"Latest answer:\n{latest_user_answer}\n\n"
            f"Current summary:\n{json.dumps(last_summary or {}, ensure_ascii=False)}\n\n"
            f"Current coverage:\n{json.dumps(last_coverage or {}, ensure_ascii=False)}"
        )
        try:
            raw, _ = await self._generate_text_resilient(
                prompt=prompt,
                system_prompt=system_prompt,
                temperature=0.1,
                max_tokens=1200,
                breaker_prefix="interview_semantic_extract",
                response_format={"type": "json_object"},
            )
            parsed = self._parse_json(raw)
            return self._sanitize_extraction_payload(parsed)
        except Exception as exc:  # noqa: BLE001
            logger.warning("Agentic extraction node failed: %s", exc)
            return self._sanitize_extraction_payload({})

    @staticmethod
    def _sanitize_extraction_payload(payload: Dict[str, Any] | None) -> Dict[str, Any]:
        data = payload if isinstance(payload, dict) else {}
        raw_slots = data.get("slots") if isinstance(data.get("slots"), dict) else {}
        slots: Dict[str, List[str]] = {}
        for area in _ZERO_COVERAGE:
            values = raw_slots.get(area)
            if isinstance(values, list):
                slots[area] = [str(item).strip() for item in values if str(item or "").strip()][:8]
            else:
                slots[area] = []

        confidence_raw = data.get("confidence")
        try:
            confidence = float(confidence_raw)
        except (TypeError, ValueError):
            confidence = 0.0
        confidence = max(0.0, min(1.0, confidence))

        return {
            "slots": slots,
            "ambiguity_detected": bool(data.get("ambiguity_detected")),
            "contradiction_detected": bool(data.get("contradiction_detected")),
            "scope_budget_risk": bool(data.get("scope_budget_risk")),
            "reason": str(data.get("reason") or "").strip(),
            "confidence": confidence,
        }

    @staticmethod
    def _node_critique(
        *,
        language: str,
        extraction: Dict[str, Any],
        last_coverage: Dict | None,
    ) -> Dict[str, Any]:
        coverage = last_coverage if isinstance(last_coverage, dict) else {}
        low_covered_areas = [area for area in _ZERO_COVERAGE if float(coverage.get(area, 0) or 0) < 35]

        ambiguity_detected = bool(extraction.get("ambiguity_detected"))
        contradiction_detected = bool(extraction.get("contradiction_detected"))
        scope_budget_risk = bool(extraction.get("scope_budget_risk"))

        reason = str(extraction.get("reason") or "").strip()
        if not reason and (contradiction_detected or scope_budget_risk):
            reason = (
                "ÙŠÙˆØ¬Ø¯ ØªØ¹Ø§Ø±Ø¶ Ø£Ùˆ Ù…Ø®Ø§Ø·Ø±Ø© ÙÙŠ Ø§Ù„Ø±Ø¯ Ø§Ù„Ø£Ø®ÙŠØ± ÙˆÙŠØ­ØªØ§Ø¬ Ù„ØªØ£ÙƒÙŠØ¯ ØµØ±ÙŠØ­ Ù‚Ø¨Ù„ Ø§Ù„ØªÙ‚Ø¯Ù…."
                if language == "ar"
                else "A contradiction or planning risk exists in the latest answer and needs explicit confirmation."
            )

        recommendation = (
            "Ø§Ø·Ù„Ø¨ ØªÙˆØ¶ÙŠØ­Ù‹Ø§ Ù…Ø­Ø¯Ø¯Ù‹Ø§ Ù‚Ø§Ø¨Ù„Ù‹Ø§ Ù„Ù„Ù‚ÙŠØ§Ø³ Ù…Ø¹ Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ."
            if language == "ar"
            else "Ask for one measurable clarification with a concrete example."
        )
        if low_covered_areas:
            joined = ", ".join(low_covered_areas)
            recommendation = (
                f"ÙˆØ¬Ù‘Ù‡ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù‚Ø§Ø¯Ù… Ù„Ø£Ø¶Ø¹Ù ØªØºØ·ÙŠØ© Ø­Ø§Ù„ÙŠØ§Ù‹: {joined}."
                if language == "ar"
                else f"Steer the next question to the weakest-covered area: {joined}."
            )

        return {
            "ambiguity_detected": ambiguity_detected,
            "contradiction_detected": contradiction_detected,
            "scope_budget_risk": scope_budget_risk,
            "reason": reason,
            "recommendation": recommendation,
            "low_covered_areas": low_covered_areas[:3],
            "semantic_confidence": float(extraction.get("confidence") or 0.0),
        }

    @staticmethod
    def _node_routing(
        *,
        language: str,
        critique: Dict[str, Any],
        last_coverage: Dict | None,
    ) -> Dict[str, Any]:
        coverage = last_coverage if isinstance(last_coverage, dict) else {}
        low_covered_areas = critique.get("low_covered_areas") if isinstance(critique.get("low_covered_areas"), list) else []
        target_stage = str(low_covered_areas[0]) if low_covered_areas else InterviewService._pick_focus_area(coverage)
        if target_stage not in _ZERO_COVERAGE:
            target_stage = InterviewService._pick_focus_area(coverage)

        question_style = "inference-driven"
        if critique.get("ambiguity_detected"):
            question_style = "clarify-ambiguity"
        if critique.get("contradiction_detected") or critique.get("scope_budget_risk"):
            question_style = "resolve-conflict"

        return {
            "target_stage": target_stage,
            "question_style": question_style,
            "routing_mode": "semantic",
        }

    @staticmethod
    def _merge_reflector_signals(primary: Dict[str, Any], fallback: Dict[str, Any]) -> Dict[str, Any]:
        p = primary if isinstance(primary, dict) else {}
        f = fallback if isinstance(fallback, dict) else {}
        merged = dict(f)
        if p:
            merged.update({k: v for k, v in p.items() if v is not None and v != ""})
            merged["engine"] = "agentic"
            merged["fallback_engine"] = "rule_based"
        else:
            merged["engine"] = "rule_based"
        return merged

    async def get_chat_response(
        self, db: AsyncSession, project_id: int, language: str = "ar",
        last_summary: Dict | None = None, last_coverage: Dict | None = None,
    ) -> Dict[str, Any]:
        messages = await self._get_project_messages(db, project_id)
        if not messages:
            lang = language if language in {"ar", "en"} else "ar"
            return self._initial_question(lang)

        conversation = self._format_conversation_windowed(messages)
        latest_user_answer = self._latest_user_message(messages)
        language = self._resolve_response_language(language, latest_user_answer)

        if self._is_duplicate_message(messages):
            dup_msg = (
                "ÙŠØ¨Ø¯Ùˆ Ø£Ù†Ùƒ Ø£Ø±Ø³Ù„Øª Ù†ÙØ³ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰. Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø´ÙŠØ¡ Ù…Ø­Ø¯Ø¯ ØªØ±ÙŠØ¯ Ø¥Ø¶Ø§ÙØªÙ‡ Ø£Ùˆ ØªÙˆØ¶ÙŠØ­Ù‡ØŸ"
                if language == "ar"
                else "It looks like you sent the same message again. Is there something specific you'd like to add or clarify?"
            )
            return {"question": dup_msg, "stage": "discovery", "done": False}

        if len(latest_user_answer) > _MAX_USER_MESSAGE_CHARS:
            logger.warning(
                "User answer too long (%d chars) â€” truncating to %d for LLM safety.",
                len(latest_user_answer), _MAX_USER_MESSAGE_CHARS,
            )
            latest_user_answer = latest_user_answer[:_MAX_USER_MESSAGE_CHARS] + "â€¦ [ØªÙ… Ø§Ù‚ØªØ·Ø§Ø¹ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù„Ø£Ø³Ø¨Ø§Ø¨ ØªÙ‚Ù†ÙŠØ©]"

        srs_context = await self._get_latest_srs_context(db=db, project_id=project_id)
        
        system_prompt = _AR_SYSTEM if language == "ar" else _EN_SYSTEM
        user_prompt = f"Target language: {language}\n\n"
        if srs_context:
            user_prompt += f"Existing SRS Context:\n{json.dumps(srs_context, ensure_ascii=False)}\n\n"
        user_prompt += f"Conversation history:\n{conversation}\n\nClient's latest reply: {latest_user_answer}"

        try:
            raw, _ = await self._generate_text_resilient(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.3,
                max_tokens=600,
                breaker_prefix="interview_chatter",
                response_format={"type": "json_object"},
            )
            data = self._parse_json(raw)
        except Exception as error_:
            logger.warning("Chatter agent degraded: %s", error_, exc_info=True)
            data = {}

        question = str(data.get("question") or self._initial_question(language)["question"])
        stage = str(data.get("stage") or self._pick_focus_area(last_coverage or {}))
        done = bool(data.get("done", False))

        question = self._enforce_question_language(question, language)

        return {
            "question": question,
            "stage": stage,
            "done": done,
            "suggested_answers": [],
            "signals": {"engine": "chatter", "fast_mode": True},
            "live_patch": {},
            "cycle_trace": {},
            "topic_navigation": {"target_stage": stage},
        }

    async def extract_background_patches(
        self, project_id: int, language: str, session_factory: Any
    ) -> None:
        """Asynchronous background task to extract semantic patches using LivePatchService."""
        from backend.services.live_patch_service import LivePatchService
        from backend.database.models import SRSDraft, Project
        from sqlalchemy import select
        
        async with session_factory() as db:
            try:
                # 1. Lock project to safely update SRSDraft
                stmt_lock = select(Project).where(Project.id == project_id).with_for_update()
                locked_project = await db.scalar(stmt_lock)
                if not locked_project:
                    return

                messages = await self._get_project_messages(db, project_id)
                if not messages:
                    return

                stmt_draft = select(SRSDraft).where(SRSDraft.project_id == project_id).order_by(SRSDraft.version.desc()).limit(1)
                latest_draft = await db.scalar(stmt_draft)

                draft_content = latest_draft.content if latest_draft and isinstance(latest_draft.content, dict) else {}
                old_summary = draft_content.get("summary", {}) if draft_content else {}
                old_coverage = draft_content.get("coverage", {}) if draft_content else {}

                # 2. Extract using the robust, decoupled LivePatchService!
                result = await LivePatchService.build_from_messages(
                    language=language,
                    messages=messages,
                    last_summary=old_summary,
                    last_coverage=old_coverage,
                )

                # 3. Save new state
                new_content = {
                    "summary": result.get("summary", old_summary),
                    "coverage": result.get("coverage", old_coverage),
                }

                if latest_draft:
                    latest_draft.content = new_content
                    latest_draft.version += 1
                else:
                    new_draft = SRSDraft(project_id=project_id, content=new_content, version=1)
                    db.add(new_draft)

                await db.commit()
                logger.info(f"Background extraction completed for project {project_id}")

            except Exception as e:
                await db.rollback()
                logger.error(f"Background extraction failed for project {project_id}: {e}")
    # Legacy variables unused by Chatter but referenced by other methods
    # We leave _run_orchestrator_turn and related functions here in case they are used elsewhere

    @staticmethod
    def _build_cycle_trace(
        language: str,
        stage: str,
        reflector_signals: Dict[str, Any],
        coverage: Dict[str, Any],
        doc_patch: Dict[str, Any],
    ) -> Dict[str, Any]:
        target_stage = reflector_signals.get("target_stage") or stage
        question_style = reflector_signals.get("question_style") or "inference-driven"
        changed_areas = doc_patch.get("changed_areas") or []
        alerts = doc_patch.get("alerts") or []
        avg_cov = 0.0
        if coverage:
            values = [float(coverage.get(area, 0) or 0) for area in _ZERO_COVERAGE]
            avg_cov = round(sum(values) / max(1, len(values)), 2)

        steps = [
            {
                "name": "reason",
                "status": "done",
                "summary": (
                    "Ø­Ù„Ù‘Ù„Ù†Ø§ Ø§Ù„Ø±Ø¯ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ØºÙ…ÙˆØ¶/Ø§Ù„ØªÙ†Ø§Ù‚Ø¶ ÙˆØ£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ."
                    if language == "ar"
                    else "Analyzed the latest answer for ambiguity/contradictions and next-priority area."
                ),
                "meta": {
                    "ambiguity": bool(reflector_signals.get("ambiguity_detected")),
                    "contradiction": bool(reflector_signals.get("contradiction_detected")),
                    "scope_budget_risk": bool(reflector_signals.get("scope_budget_risk")),
                },
            },
            {
                "name": "plan",
                "status": "done",
                "summary": (
                    f"Ø§Ù„Ø®Ø·Ø©: ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ù„Ù‰ Ù…Ø¬Ø§Ù„ {target_stage} Ø¨Ù†Ù…Ø· {question_style}."
                    if language == "ar"
                    else f"Plan: steer next question to {target_stage} using {question_style} style."
                ),
                "meta": {
                    "target_stage": target_stage,
                    "question_style": question_style,
                },
            },
            {
                "name": "ask",
                "status": "done",
                "summary": (
                    "ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© ÙˆØ®ÙŠØ§Ø±Ø§Øª Ø¥Ø¬Ø§Ø¨Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¨Ø§Ø´Ø±Ø©."
                    if language == "ar"
                    else "Generated a follow-up question and directly usable answer options."
                ),
                "meta": {"stage": stage},
            },
            {
                "name": "update",
                "status": "done",
                "summary": (
                    "ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø­ÙŠ ÙˆØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„ØªØ£Ø«ÙŠØ± ÙˆØ§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ."
                    if language == "ar"
                    else "Updated live summary plus dependency/impact alerts in parallel."
                ),
                "meta": {
                    "changed_areas": len(changed_areas),
                    "alerts": len(alerts),
                    "avg_coverage": avg_cov,
                },
            },
        ]

        confidence = 0.6 + min(0.35, (avg_cov / 100.0) * 0.35)
        if reflector_signals.get("ambiguity_detected"):
            confidence -= 0.07
        if reflector_signals.get("contradiction_detected"):
            confidence -= 0.09

        return {
            "version": "v1",
            "steps": steps,
            "score": {
                "coverage": avg_cov,
                "confidence": round(max(0.1, min(0.95, confidence)), 2),
                "risk_level": "high" if alerts else "medium" if reflector_signals.get("ambiguity_detected") else "low",
            },
        }

    @staticmethod
    def _latest_user_message(messages: List[ChatMessage]) -> str:
        for msg in reversed(messages):
            if str(msg.role or "").lower() == "user":
                return str(msg.content or "").strip()
        return ""

    @staticmethod
    def _infer_message_language(text: str, fallback: str = "ar") -> str:
        value = str(text or "")
        arabic_count = len(_ARABIC_CHAR_PATTERN.findall(value))
        latin_count = len(_LATIN_CHAR_PATTERN.findall(value))

        if arabic_count == 0 and latin_count == 0:
            return fallback if fallback in {"ar", "en"} else "ar"
        if arabic_count >= latin_count:
            return "ar"
        return "en"

    @staticmethod
    def _resolve_response_language(requested_language: str, latest_user_answer: str) -> str:
        fallback = requested_language if requested_language in {"ar", "en"} else "ar"
        return InterviewService._infer_message_language(latest_user_answer, fallback=fallback)

    @staticmethod
    def _is_duplicate_message(messages: List[ChatMessage]) -> bool:
        """Return True if the last two user messages are identical (after normalization).
        Prevents re-processing when a user accidentally sends the same message multiple times.
        """
        user_msgs = [m for m in messages if m.role == "user"]
        if len(user_msgs) < 2:
            return False
        def _norm(text: str) -> str:
            return re.sub(r"\s+", " ", str(text or "").strip().lower())
        return _norm(user_msgs[-1].content) == _norm(user_msgs[-2].content)

    @staticmethod
    def _enforce_question_language(question: str, language: str) -> str:
        """Ensure the question is in the correct language. If a mismatch is detected,
        prepend a translated version so the user always sees their own language."""
        text = str(question or "").strip()
        if not text:
            return text

        arabic_chars = len(_ARABIC_CHAR_PATTERN.findall(text))
        latin_chars = len(_LATIN_CHAR_PATTERN.findall(text))
        total = arabic_chars + latin_chars
        if total == 0:
            return text

        arabic_ratio = arabic_chars / total

        if language == "ar" and arabic_ratio < 0.4:
            # LLM returned English in an Arabic conversation
            logger.warning("Language mismatch detected: expected AR, got mostly EN. Prepending AR note.")
            return (
                "(Ù…Ù„Ø§Ø­Ø¸Ø©: ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø±Ø¯ Ø¨Ù„ØºØ© Ù…Ø®ØªÙ„ÙØ© â€” ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)\n"
                + text
            )
        if language == "en" and arabic_ratio > 0.6:
            # LLM returned Arabic in an English conversation
            logger.warning("Language mismatch detected: expected EN, got mostly AR. Prepending EN note.")
            return (
                "(Note: Response was detected in a different language â€” please reply in English)\n"
                + text
            )
        return text

    @staticmethod
    def _reflect_conversation(
        language: str,
        latest_user_answer: str,
        last_summary: Dict | None,
        last_coverage: Dict | None,
        slot_analysis: Dict[str, Any] | None = None,
    ) -> Dict[str, Any]:
        coverage = last_coverage if isinstance(last_coverage, dict) else {}
        det = slot_analysis if isinstance(slot_analysis, dict) else {}

        ambiguity_detected = bool(det.get("ambiguity_detected"))
        contradiction = bool(det.get("contradiction_detected"))
        scope_budget_risk = bool(det.get("scope_budget_risk"))
        ambiguity_hits = (
            [str(item).strip() for item in det.get("ambiguity_terms", []) if str(item or "").strip()]
            if isinstance(det.get("ambiguity_terms"), list)
            else []
        )
        contradiction_reason = str(det.get("reason") or "").strip()
        low_covered_areas = [area for area in _ZERO_COVERAGE if float(coverage.get(area, 0) or 0) < 35]

        recommendation = (
            "Ø±ÙƒØ² Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù‚Ø§Ø¯Ù… Ø¹Ù„Ù‰ ØªÙØµÙŠÙ„ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ù‚ÙŠØ§Ø³ + Ù‚ÙŠØ¯ ÙˆØ§Ø­Ø¯ ÙˆØ§Ø¶Ø­." if language == "ar"
            else "Focus next question on one measurable detail plus one explicit constraint."
        )
        if low_covered_areas:
            area_text = ", ".join(low_covered_areas)
            recommendation = (
                f"Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ø£Ù‚Ù„ ØªØºØ·ÙŠØ©: {area_text}. ÙˆØ¬Ù‘Ù‡ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù‚Ø§Ø¯Ù… Ù„Ø£Ø¶Ø¹Ù Ù…Ø¬Ø§Ù„ Ù…Ø¹ Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ."
                if language == "ar"
                else f"Lowest-covered areas: {area_text}. Aim next question at the weakest area with a concrete example."
            )
        if contradiction_reason:
            recommendation = (
                f"Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ ÙŠØ­Ø³Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù‚Ø·Ø©: {contradiction_reason}"
                if language == "ar"
                else f"Make the next question explicitly resolve this point: {contradiction_reason}"
            )

        target_stage = low_covered_areas[0] if low_covered_areas else "discovery"
        question_style = "clarify-ambiguity" if ambiguity_detected else "inference-driven"
        if contradiction or scope_budget_risk:
            question_style = "resolve-conflict"

        return {
            "ambiguity_detected": ambiguity_detected,
            "ambiguity_terms": ambiguity_hits[:4],
            "scope_budget_risk": scope_budget_risk,
            "contradiction_detected": contradiction,
            "reason": contradiction_reason,
            "recommendation": recommendation,
            "low_covered_areas": low_covered_areas[:3],
            "question_style": question_style,
            "target_stage": target_stage,
            "slot_analysis": det,
        }

    @classmethod
    def _build_topic_navigation(
        cls,
        language: str,
        summary: Dict[str, Any],
        coverage: Dict[str, Any],
        reflector_signals: Dict[str, Any],
    ) -> Dict[str, Any]:
        token_counts: Dict[str, int] = {}
        token_area: Dict[str, str] = {}

        for area in _ZERO_COVERAGE:
            items = summary.get(area, []) if isinstance(summary.get(area), list) else []
            for item in items:
                text = cls._requirement_value(item).lower()
                tokens = re.findall(r"[\w\u0600-\u06FF]{3,}", text)
                for token in tokens:
                    if token in _TOPIC_STOPWORDS:
                        continue
                    token_counts[token] = token_counts.get(token, 0) + 1
                    token_area.setdefault(token, area)

        ranked = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)
        topics: List[Dict[str, Any]] = []
        for token, hits in ranked[:8]:
            area = token_area.get(token, "discovery")
            base_cov = float(coverage.get(area, 0) or 0)
            depth = round(max(0.1, min(1.0, (hits / 6.0))), 2)
            consistency = 0.86
            if reflector_signals.get("contradiction_detected"):
                consistency -= 0.2
            if reflector_signals.get("ambiguity_detected"):
                consistency -= 0.1
            confidence = round(max(0.1, min(0.98, (base_cov / 100.0) * 0.7 + depth * 0.3)), 2)

            topics.append({
                "id": token,
                "label": token,
                "area": area,
                "coverage": round(min(100.0, base_cov + hits * 4), 2),
                "kpi": {
                    "depth": depth,
                    "consistency": round(max(0.1, min(0.98, consistency)), 2),
                    "confidence": confidence,
                },
            })

        if not topics:
            for area in _ZERO_COVERAGE:
                base_cov = float(coverage.get(area, 0) or 0)
                topics.append({
                    "id": area,
                    "label": area,
                    "area": area,
                    "coverage": base_cov,
                    "kpi": {
                        "depth": round(min(1.0, base_cov / 100.0), 2),
                        "consistency": 0.8,
                        "confidence": round(min(0.95, 0.4 + base_cov / 200.0), 2),
                    },
                })

        avg_depth = round(sum(t["kpi"]["depth"] for t in topics) / max(1, len(topics)), 2)
        avg_consistency = round(sum(t["kpi"]["consistency"] for t in topics) / max(1, len(topics)), 2)
        avg_confidence = round(sum(t["kpi"]["confidence"] for t in topics) / max(1, len(topics)), 2)

        return {
            "mode": "dynamic_topics",
            "topics": topics,
            "overall_kpi": {
                "depth": avg_depth,
                "consistency": avg_consistency,
                "confidence": avg_confidence,
            },
            "next_topic": topics[0]["id"] if topics else "discovery",
            "language": language,
        }

    @classmethod
    def _build_documentation_patch(
        cls,
        language: str,
        stage: str,
        new_summary: Dict[str, Any],
        old_summary: Dict[str, Any],
        new_coverage: Dict[str, Any],
        old_coverage: Dict[str, Any],
        reflector_signals: Dict[str, Any],
    ) -> Dict[str, Any]:
        changed_areas: List[Dict[str, Any]] = []
        patch_events = cls._build_patch_events(old_summary=old_summary, new_summary=new_summary)

        for area in _ZERO_COVERAGE:
            new_items = new_summary.get(area, []) if isinstance(new_summary.get(area), list) else []
            old_items = old_summary.get(area, []) if isinstance(old_summary.get(area), list) else []

            added = [
                item for item in new_items
                if cls._requirement_id(item) and not any(cls._requirement_id(old_item) == cls._requirement_id(item) for old_item in old_items)
            ]

            cov_before = float(old_coverage.get(area, 0) or 0)
            cov_after = float(new_coverage.get(area, 0) or 0)
            cov_delta = round(cov_after - cov_before, 2)

            if added or cov_delta > 0:
                changed_areas.append({
                    "area": area,
                    "added": [cls._requirement_value(item) for item in added[:8]],
                    "coverage_before": cov_before,
                    "coverage_after": cov_after,
                    "coverage_delta": cov_delta,
                    "total_items": len(new_items),
                })

        focus_area = stage if stage in _ZERO_COVERAGE else cls._pick_focus_area(new_coverage)
        warnings: List[str] = []
        if reflector_signals.get("contradiction_detected") or reflector_signals.get("scope_budget_risk"):
            reason = str(reflector_signals.get("reason") or "").strip()
            if reason:
                warnings.append(reason)
        if reflector_signals.get("ambiguity_detected"):
            warnings.append(
                "ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© Ø¨Ù…ØªØ·Ù„Ø¨Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù‚ÙŠØ§Ø³." if language == "ar"
                else "Please replace broad wording with measurable requirements."
            )

        dependency_alerts = cls._compute_dependency_alerts(
            summary=new_summary,
            language=language,
            reflector_signals=reflector_signals,
        )
        next_plan = {
            "target_stage": reflector_signals.get("target_stage") or focus_area,
            "question_style": reflector_signals.get("question_style") or "inference-driven",
            "prompt_hint": reflector_signals.get("recommendation") or "",
        }

        return {
            "mode": "live",
            "focus_area": focus_area,
            "events": patch_events,
            "changed_areas": changed_areas,
            "warnings": warnings[:3],
            "alerts": dependency_alerts[:5],
            "next_plan": next_plan,
            "semantic_graph": cls._build_semantic_graph(new_summary),
        }

    @classmethod
    def _build_patch_events(
        cls,
        old_summary: Dict[str, Any],
        new_summary: Dict[str, Any],
    ) -> List[Dict[str, Any]]:
        events: List[Dict[str, Any]] = []

        for area in _ZERO_COVERAGE:
            old_items = old_summary.get(area, []) if isinstance(old_summary.get(area), list) else []
            new_items = new_summary.get(area, []) if isinstance(new_summary.get(area), list) else []

            old_map = {
                cls._requirement_id(item): cls._requirement_value(item)
                for item in old_items
                if cls._requirement_id(item)
            }
            new_map = {
                cls._requirement_id(item): cls._requirement_value(item)
                for item in new_items
                if cls._requirement_id(item)
            }

            for idx, item in enumerate(new_items):
                req_id = cls._requirement_id(item)
                text = cls._requirement_value(item)
                if not req_id or not text:
                    continue

                if req_id not in old_map:
                    events.append({
                        "op": "added",
                        "field_path": f"{area}.{idx}",
                        "id": req_id,
                        "value": text,
                    })

            for old_idx, old_item in enumerate(old_items):
                old_req_id = cls._requirement_id(old_item)
                old_text = cls._requirement_value(old_item)
                if not old_req_id or not old_text:
                    continue
                if old_req_id not in new_map:
                    events.append({
                        "op": "removed",
                        "field_path": f"{area}.{old_idx}",
                        "id": old_req_id,
                        "value": old_text,
                    })

            for req_id, old_text in old_map.items():
                new_text = new_map.get(req_id)
                if not new_text:
                    continue
                if old_text != new_text:
                    idx = next(
                        (i for i, item in enumerate(new_items) if cls._requirement_id(item) == req_id),
                        0,
                    )
                    events.append({
                        "op": "updated",
                        "field_path": f"{area}.{idx}",
                        "id": req_id,
                        "old_value": old_text,
                        "value": new_text,
                    })

        return events[:60]

    @classmethod
    def _compute_dependency_alerts(
        cls,
        summary: Dict[str, Any],
        language: str,
        reflector_signals: Dict[str, Any],
    ) -> List[Dict[str, Any]]:
        all_items: List[str] = []
        for area in _ZERO_COVERAGE:
            values = summary.get(area, [])
            if isinstance(values, list):
                all_items.extend(cls._requirement_value(v) for v in values)

        merged_text = "\n".join(all_items).lower()
        alerts: List[Dict[str, Any]] = []

        has_reports = cls._mentions_entity(merged_text, "reports")
        has_db = cls._mentions_entity(merged_text, "database")
        denies_db = cls._has_negated_entity(merged_text, "database")
        has_realtime = cls._mentions_entity(merged_text, "realtime")
        has_payments = cls._mentions_entity(merged_text, "payments")
        risky_budget = bool(reflector_signals.get("scope_budget_risk"))

        if has_reports and (denies_db or not has_db):
            alerts.append({
                "type": "dependency_conflict",
                "severity": "high",
                "title": "ØªØ¹Ø§Ø±Ø¶ Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª: Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± ØªØ­ØªØ§Ø¬ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª" if language == "ar" else "Dependency conflict: reports require a database",
                "message": (
                    "ØªÙ… Ø±ØµØ¯ Ø·Ù„Ø¨ ØªÙ‚Ø§Ø±ÙŠØ±/Ù„ÙˆØ­Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø¹ Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø£Ùˆ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ©."
                    if language == "ar"
                    else "Reports/dashboard are requested while database support is missing or excluded. Reconcile scope vs architecture."
                ),
            })

        if has_realtime and risky_budget:
            alerts.append({
                "type": "scope_constraint_mismatch",
                "severity": "medium",
                "title": "Ù…Ø®Ø§Ø·Ø±Ø© Ù†Ø·Ø§Ù‚ Ù…Ù‚Ø§Ø¨Ù„ Ù…ÙŠØ²Ø§Ù†ÙŠØ©" if language == "ar" else "Scope vs budget risk",
                "message": (
                    "Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù„Ø­Ø¸ÙŠØ© ØºØ§Ù„Ø¨Ø§Ù‹ ØªØ²ÙŠØ¯ Ø§Ù„ØªÙƒÙ„ÙØ© ÙˆØ§Ù„ØªØ¹Ù‚ÙŠØ¯. ÙŠÙÙ†ØµØ­ Ø¨ØªØ¹Ø±ÙŠÙ MVP Ø£Ø¨Ø³Ø· Ø£ÙˆÙ„Ø§Ù‹."
                    if language == "ar"
                    else "Real-time requirements usually increase cost and complexity. Consider a simpler MVP first."
                ),
            })

        if has_payments and not has_db:
            alerts.append({
                "type": "missing_foundation",
                "severity": "medium",
                "title": "Ø£Ø³Ø§Ø³ Ù†Ø§Ù‚Øµ Ù„Ù…ÙŠØ²Ø© Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª" if language == "ar" else "Missing foundation for payments",
                "message": (
                    "Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¯ÙØ¹ ØªØ­ØªØ§Ø¬ ØªØ®Ø²ÙŠÙ†Ù‹Ø§ Ù…ÙˆØ«ÙˆÙ‚Ù‹Ø§ ÙˆØªØªØ¨Ø¹Ù‹Ø§ Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§ØªØ› Ù„Ù… ØªØ¸Ù‡Ø± Ù…ØªØ·Ù„Ø¨Ø§Øª Ø¨Ù†ÙŠØ© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯."
                    if language == "ar"
                    else "Payments need reliable persistence and transaction tracking; data-layer requirements are currently weak."
                ),
            })

        return alerts

    @staticmethod
    def _mentions_entity(text: str, entity: str) -> bool:
        aliases = _ENTITY_ALIASES.get(entity, set())
        return any(alias in text for alias in aliases)

    @staticmethod
    def _has_negated_entity(text: str, entity: str) -> bool:
        aliases = _ENTITY_ALIASES.get(entity, set())
        for token in _NEGATION_TOKENS:
            for alias in aliases:
                phrase = f"{token} {alias}"
                if phrase in text:
                    return True
        return False

    @staticmethod
    def _pick_focus_area(coverage: Dict[str, Any]) -> str:
        lowest = "discovery"
        lowest_val = float("inf")
        for area in _ZERO_COVERAGE:
            val = float(coverage.get(area, 0) or 0)
            if val < lowest_val:
                lowest = area
                lowest_val = val
        return lowest

    @classmethod
    def _build_semantic_graph(cls, summary: Dict[str, Any]) -> Dict[str, Any]:
        nodes: List[Dict[str, Any]] = []
        edges: List[Dict[str, Any]] = []
        node_index: Dict[str, str] = {}

        for area in _ZERO_COVERAGE:
            items = summary.get(area, []) if isinstance(summary.get(area), list) else []
            for idx, item in enumerate(items):
                text = cls._requirement_value(item)
                if not text:
                    continue
                req_id = cls._requirement_id(item) or f"{area}:{idx}"
                nid = req_id
                nodes.append({"id": nid, "area": area, "text": text})
                node_index[text.lower()] = nid

        dep_pattern = re.compile(r"(depends on|requires|ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰|ÙŠØªØ·Ù„Ø¨)\s+(.+)", re.IGNORECASE)
        for source in nodes:
            text = source.get("text", "")
            match = dep_pattern.search(text)
            if not match:
                continue
            target_phrase = match.group(2).strip().lower()
            for candidate_text, target_id in node_index.items():
                if candidate_text in target_phrase or target_phrase in candidate_text:
                    edges.append({
                        "from": source["id"],
                        "to": target_id,
                        "type": "DEPENDS_ON",
                    })
                    break

        return {"nodes": nodes[:80], "edges": edges[:120]}

    @staticmethod
    def _merge_coverage(
        new_coverage: Any,
        last_coverage: Dict | None,
        reflector_signals: Dict[str, Any] | None = None,
    ) -> Dict[str, Any]:
        merged = dict(new_coverage) if isinstance(new_coverage, dict) else dict(_ZERO_COVERAGE)
        if not last_coverage:
            return {area: max(0.0, min(100.0, float(merged.get(area, 0) or 0))) for area in _ZERO_COVERAGE}

        signals = reflector_signals if isinstance(reflector_signals, dict) else {}
        high_risk = bool(signals.get("contradiction_detected")) or bool(signals.get("ambiguity_detected"))
        max_decay = _MAX_COVERAGE_DECAY_RISK if high_risk else _MAX_COVERAGE_DECAY_DEFAULT

        for area in _ZERO_COVERAGE:
            old_val = max(0.0, min(100.0, float(last_coverage.get(area, 0) or 0)))
            new_val = max(0.0, min(100.0, float(merged.get(area, 0) or 0)))
            if new_val >= old_val:
                merged[area] = new_val
                continue
            merged[area] = max(new_val, old_val - max_decay)
        return merged

    @staticmethod
    def _normalized_summary(summary: Any) -> Dict[str, List[Dict[str, str]]]:
        out: Dict[str, List[Dict[str, str]]] = {area: [] for area in _ZERO_COVERAGE}
        if not isinstance(summary, dict):
            return out
        for area in _ZERO_COVERAGE:
            items = summary.get(area)
            if isinstance(items, list):
                normalized_items: List[Dict[str, str]] = []
                for item in items:
                    if isinstance(item, dict):
                        value = str(item.get("value") or "").strip()
                        req_id = str(item.get("id") or "").strip() or InterviewService._new_requirement_id()
                        if value:
                            normalized_items.append({"id": req_id, "value": value})
                        continue

                    value = str(item or "").strip()
                    if value:
                        normalized_items.append({"id": InterviewService._new_requirement_id(), "value": value})
                out[area] = normalized_items
        return out

    @staticmethod
    def _new_requirement_id() -> str:
        return f"req_{uuid4().hex[:12]}"

    @staticmethod
    def _requirement_id(item: Any) -> str:
        if isinstance(item, dict):
            return str(item.get("id") or "").strip()
        return ""

    @staticmethod
    def _requirement_value(item: Any) -> str:
        if isinstance(item, dict):
            return str(item.get("value") or "").strip()
        return str(item or "").strip()

    @classmethod
    def _apply_patches(cls, old_summary: Any, patches: Any) -> Dict[str, List[Dict[str, str]]]:
        summary = deepcopy(cls._normalized_summary(old_summary))
        if not isinstance(patches, list):
            return summary

        for patch in patches:
            if not isinstance(patch, dict):
                continue

            op = str(patch.get("op") or "").strip().lower()
            area = str(patch.get("area") or "").strip().lower()
            if area not in _ZERO_COVERAGE:
                continue

            area_items = summary.get(area, [])
            req_id = str(patch.get("id") or "").strip()
            value = str(patch.get("value") or "").strip()

            if op == "add":
                if not value:
                    continue
                if not req_id:
                    req_id = cls._new_requirement_id()
                if not any(cls._requirement_id(item) == req_id for item in area_items):
                    area_items.append({"id": req_id, "value": value})
                summary[area] = area_items
                continue

            if op == "remove":
                if not req_id:
                    continue
                summary[area] = [
                    item for item in area_items
                    if cls._requirement_id(item) != req_id
                ]
                continue

            if op == "update":
                if not req_id or not value:
                    continue
                updated = False
                for idx, item in enumerate(area_items):
                    if cls._requirement_id(item) == req_id:
                        area_items[idx] = {"id": req_id, "value": value}
                        updated = True
                        break
                if not updated:
                    area_items.append({"id": req_id, "value": value})
                summary[area] = [
                    {"id": cls._requirement_id(item) or cls._new_requirement_id(), "value": cls._requirement_value(item)}
                    for item in area_items
                    if cls._requirement_value(item)
                ]

        return summary

    @classmethod
    def _summary_to_patches(cls, new_summary: Any, last_summary: Dict[str, List[Dict[str, str]]]) -> List[Dict[str, str]]:
        if not isinstance(new_summary, dict):
            return []

        patches: List[Dict[str, str]] = []
        for area in _ZERO_COVERAGE:
            old_items = last_summary.get(area, []) if isinstance(last_summary, dict) else []
            incoming = new_summary.get(area, []) if isinstance(new_summary.get(area), list) else []
            for item in incoming:
                value = cls._requirement_value(item)
                if not value:
                    continue
                req_id = cls._requirement_id(item) or cls._new_requirement_id()
                if not any(cls._requirement_id(existing) == req_id for existing in old_items):
                    patches.append({"op": "add", "area": area, "id": req_id, "value": value})
                    old_items = [*old_items, {"id": req_id, "value": value}]
        return patches

    @staticmethod
    async def _count_user_turns(db: AsyncSession, project_id: int) -> int:
        """Count total user messages in this project's interview."""
        stmt = select(func.count(ChatMessage.id)).where(
            ChatMessage.project_id == project_id,
            ChatMessage.role == "user",
        )
        result = await db.execute(stmt)
        return result.scalar_one_or_none() or 0

    @staticmethod
    def _enforce_completion_gate(
        llm_done: bool,
        coverage: Dict[str, Any],
        user_turn_count: int,
        reflector_signals: Dict[str, Any] | None = None,
    ) -> bool:
        """Return True only when the LLM, turn count, AND per-area thresholds all agree."""
        if not llm_done:
            return False
        if user_turn_count < _MIN_INTERVIEW_TURNS:
            return False

        signals = reflector_signals if isinstance(reflector_signals, dict) else {}
        if bool(signals.get("contradiction_detected")):
            return False
        if bool(signals.get("ambiguity_detected")) and user_turn_count < (_MIN_INTERVIEW_TURNS + 2):
            return False

        for area, threshold in _COMPLETION_THRESHOLDS.items():
            if float(coverage.get(area, 0)) < threshold:
                return False
        return True

    @staticmethod
    def _has_open_questions_round(messages: List[ChatMessage]) -> bool:
        """Detect whether the assistant already asked an explicit open-questions checkpoint."""
        # Use specific checkpoint phrases instead of generic question markers
        # to avoid false positives from normal interview questions.
        checkpoint_phrases_en = {
            "before we close the interview",
            "capture open questions",
            "unresolved items",
            "documented as explicit assumptions",
            "out-of-scope boundaries",
        }
        checkpoint_phrases_ar = {
            "Ù‚Ø¨Ù„ Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©",
            "Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ÙØªÙˆØ­Ø©",
            "ØºÙŠØ± Ù…Ø­Ø³ÙˆÙ…Ø©",
            "Ø§ÙØªØ±Ø§Ø¶Ø§Øª Ø£Ùˆ Ù‚Ø±Ø§Ø±Ø§Øª",
            "Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Ø·Ø§Ù‚",
        }
        checkpoint_phrases = checkpoint_phrases_en | checkpoint_phrases_ar
        for message in reversed(messages):
            if message.role != "assistant":
                continue
            text = str(message.content or "").lower()
            if any(phrase in text for phrase in checkpoint_phrases):
                return True
        return False

    @staticmethod
    def _build_open_questions_question(language: str) -> str:
        if language == "ar":
            return (
                "Ù‚Ø¨Ù„ Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©ØŒ Ù†Ø­ØªØ§Ø¬ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ÙØªÙˆØ­Ø©. "
                "Ù…Ø§ Ø£Ù‡Ù… 2â€“3 Ù†Ù‚Ø§Ø· Ù…Ø§ Ø²Ø§Ù„Øª ØºÙŠØ± Ù…Ø­Ø³ÙˆÙ…Ø© (Ù…Ø«Ù„ Ù‚ÙŠÙˆØ¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©/Ø£Ù…Ù†ÙŠØ©ØŒ ØªÙƒØ§Ù…Ù„Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©ØŒ "
                "Ø£Ùˆ Ø¹Ù†Ø§ØµØ± Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Ø·Ø§Ù‚) ÙˆØªØ±ÙŠØ¯ ØªÙˆØ«ÙŠÙ‚Ù‡Ø§ ÙƒØ§ÙØªØ±Ø§Ø¶Ø§Øª Ø£Ùˆ Ù‚Ø±Ø§Ø±Ø§Øª ÙˆØ§Ø¶Ø­Ø©ØŸ"
            )
        return (
            "Before we close the interview, we need to capture open questions. "
            "What are the top 2-3 unresolved items (e.g., legal/security constraints, external integrations, "
            "or out-of-scope boundaries) that should be documented as explicit assumptions or decisions?"
        )

    @staticmethod
    def _apply_soft_clarification_policy(
        language: str,
        question: str,
        latest_user_answer: str,
        reflector_signals: Dict[str, Any] | None,
    ) -> str:
        signals = reflector_signals if isinstance(reflector_signals, dict) else {}
        has_contradiction = bool(signals.get("contradiction_detected"))
        has_ambiguity = bool(signals.get("ambiguity_detected"))
        reason = str(signals.get("reason") or "").strip()

        if has_contradiction:
            return InterviewService._build_conversational_clarification_question(
                language=language,
                latest_user_answer=latest_user_answer,
                reason=reason,
            )

        if has_ambiguity and InterviewService._looks_static_clarification(question):
            answer = str(latest_user_answer or "").strip()
            if language == "ar":
                return (
                    "Ù…Ù…ØªØ§Ø²ØŒ Ø®Ù„Ù‘ÙŠÙ†Ø§ Ù†ÙˆØ¶Ù‘Ø­ Ø§Ù„ØµÙˆØ±Ø© Ø£ÙƒØ«Ø± ÙˆÙ†ÙƒÙ…Ù‘Ù„. "
                    f"Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„ÙŠ Ø°ÙƒØ±ØªÙ‡ ( {answer} )ØŒ Ø¥ÙŠÙ‡ Ø£Ù‡Ù… Ù‡Ø¯Ù Ø£Ùˆ Ù…ÙŠØ²Ø© Ù„Ø§Ø²Ù… Ù†Ø«Ø¨ØªÙ‡Ø§ Ø£ÙˆÙ„Ø§Ù‹ØŸ"
                )
            return (
                "Great, let's clarify this and keep moving. "
                f"Regarding what you mentioned ( {answer} ), what is the most important goal or feature to lock first?"
            )

        return question

    @staticmethod
    def _looks_static_clarification(question: str) -> bool:
        text = str(question or "").strip().lower()
        if not text:
            return True
        static_markers = {
            "instead of repeating",
            "please clarify",
            "could you clarify",
            "ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªÙˆØ¶ÙŠØ­",
            "Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ø§Ù„ØªÙˆØ¶ÙŠØ­",
            "share one new detail",
            "clarify this",
        }
        return any(marker in text for marker in static_markers)

    @staticmethod
    def _build_conversational_clarification_question(
        language: str,
        latest_user_answer: str,
        reason: str,
    ) -> str:
        answer = str(latest_user_answer or "").strip()
        answer_preview = answer[:220].strip()
        if language == "ar":
            if reason:
                return (
                    "ØªÙ…Ø§Ù…ØŒ ÙØ§Ù‡Ù…Ùƒ ÙˆØ¹Ø§ÙŠØ² Ø£ÙˆØ«Ù‚Ù‡Ø§ ØµØ­. "
                    f"ÙÙŠ Ù†Ù‚Ø·Ø© Ù…Ø­ØªØ§Ø¬Ø© ØªÙˆØ¶ÙŠØ­ Ø¨Ø³ÙŠØ·: {reason} "
                    "Ø§Ø®ØªØ§Ø± Ø§Ù„ØµÙŠØ§ØºØ© Ø§Ù„Ø£Ø¯Ù‚ Ø§Ù„Ø¢Ù† ÙˆØ§Ø¯ÙŠÙ†ÙŠ Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ Ø³Ø±ÙŠØ¹ (Ø³Ø·Ø± ÙˆØ§Ø­Ø¯) Ø¹Ø´Ø§Ù† Ù†ÙƒÙ…Ù‘Ù„ Ø¹Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø£Ø³Ø§Ø³."
                )
            return (
                "ØªÙ…Ø§Ù…ØŒ Ø®Ù„Ù‘ÙŠÙ†Ø§ Ù†ÙˆØ¶Ø­ Ø§Ù„Ù†Ù‚Ø·Ø© Ø¯ÙŠ Ø¨Ø³Ø±Ø¹Ø© ÙˆÙ†ÙƒÙ…Ù„. "
                f"Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø§Ù„Ø£Ø®ÙŠØ±Ø© ÙƒØ§Ù†Øª: \"{answer_preview}\". "
                "Ø£ÙŠ Ø¬Ø²Ø¡ Ù†Ø¹ØªÙ…Ø¯Ù‡ ÙƒÙ‚Ø±Ø§Ø± Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ø¢Ù†ØŒ ÙˆØ£ÙŠ Ø¬Ø²Ø¡ Ù†Ø¹ØªØ¨Ø±Ù‡ Ø§ÙØªØ±Ø§Ø¶ Ù…Ø¤Ù‚ØªØŸ"
            )
        if reason:
            return (
                "Understood â€” let's keep this smooth and lock it correctly. "
                f"One point needs clarification: {reason} "
                "Which version should we treat as final now, and can you add one short practical example?"
            )
        return (
            "Understood â€” let's clarify this quickly and continue. "
            f"Your last answer was: \"{answer_preview}\". "
            "Which part should be treated as the final decision, and which part is only a temporary assumption?"
        )

    @staticmethod
    def _initial_question(language: str) -> Dict[str, Any]:
        initial_coverage = dict(_ZERO_COVERAGE)
        if language == "ar":
            return {
                "question": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ. Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø¬Ù…Ø¹ Ù…ØªØ·Ù„Ø¨Ø§Øª Ù…Ø´Ø±ÙˆØ¹Ùƒ. Ù…Ø§Ø°Ø§ ØªÙˆØ¯ Ø£Ù† ØªØ¨Ù†ÙŠ Ø§Ù„ÙŠÙˆÙ…ØŸ",
                "stage": "discovery",
                "done": False,
                "suggested_answers": [],
                "summary": "",
                "coverage": initial_coverage,
            }
        return {
            "question": "Welcome. I'm here to help gather requirements for your project. What would you like to build today?",
            "stage": "discovery",
            "done": False,
            "suggested_answers": [],
            "summary": "",
            "coverage": initial_coverage,
        }

    @staticmethod 
    async def _get_project_messages(
        db: AsyncSession, 
        project_id: int, 
        limit: int = _MAX_RECENT_MESSAGES
    ) -> List[ChatMessage]:
        """
        Fetches a constrained set of recent messages to maintain conversational context.
        
        Refactor: 2026-02-13 - Adel Sobhy OPTIMISATION
        Optimization: Prevents linear expansion of the LLM context window by implementing 
        a sliding window approach. Long-term state is preserved via the cumulative summary 
        rather than raw message history.

        """
        # 1. Fetch the LATEST messages first (descending)
        stmt = (
            select(ChatMessage)
            .where(ChatMessage.project_id == project_id)
            .order_by(ChatMessage.created_at.desc()) 
            .limit(limit)
        )
        result = await db.execute(stmt)
        messages = list(result.scalars().all())

        # 2. Reverse them so they are in chronological order for the LLM
        return messages[::-1]

    @staticmethod
    def _format_conversation_windowed(messages: List[ChatMessage]) -> str:
        total = len(messages)
        recent = messages[-_MAX_RECENT_MESSAGES:]

        lines: List[str] = []
        omitted = total - len(recent)
        if omitted > 0:
            lines.append(
                f"[Older conversation omitted: {omitted} messages. Use previous summary/coverage as source of truth.]"
            )

        for msg in recent:
            role = msg.role.lower()
            prefix = InterviewService._role_prefix(role)
            content = str(msg.content or "").strip()
            if len(content) > _MAX_MESSAGE_CHARS:
                content = f"{content[:_MAX_MESSAGE_CHARS]}â€¦"
            lines.append(f"{prefix}: {content}")

        conversation = "\n".join(lines)
        if len(conversation) > _MAX_CONTEXT_CHARS:
            conversation = f"â€¦\n{conversation[-_MAX_CONTEXT_CHARS:]}"
        return conversation

    @staticmethod
    def _role_prefix(role: str) -> str:
        if role == "user":
            return "User"
        if role == "assistant":
            return "Assistant"
        return "System"

    @staticmethod
    def _normalize_requirement(text: str) -> str:
        value = str(text or "").strip().lower()
        value = re.sub(r"\s+", " ", value)
        value = re.sub(r"[^\w\s\u0600-\u06FF]", "", value)
        return value

    @classmethod
    def _requirements_similar(cls, left: str, right: str) -> bool:
        """Check if two requirements are similar using token-set overlap (Jaccard)."""
        left_norm = cls._normalize_requirement(left)
        right_norm = cls._normalize_requirement(right)

        if not left_norm or not right_norm:
            return False
        if left_norm == right_norm:
            return True

        # Fuzzy match: Jaccard similarity on word tokens
        left_tokens = set(left_norm.split())
        right_tokens = set(right_norm.split())
        if not left_tokens or not right_tokens:
            return False
        intersection = left_tokens & right_tokens
        union = left_tokens | right_tokens
        return (len(intersection) / len(union)) >= 0.6

    @classmethod
    def _contains_similar_requirement(cls, existing_items: List[Any], new_item: str) -> bool:
        candidate = str(new_item or "").strip()
        if not candidate:
            return True
        return any(
            cls._requirements_similar(cls._requirement_value(existing), candidate)
            for existing in existing_items
        )

    # -----------------------------------------------------------------------
    # Domain detection helpers
    # -----------------------------------------------------------------------
    _DOMAIN_SIGNALS: Dict[str, Dict[str, set]] = {
        "ecommerce": {
            "en": {"shop", "store", "ecommerce", "e-commerce", "marketplace", "cart", "checkout",
                   "product", "catalog", "inventory", "shipping", "delivery", "vendor", "seller",
                   "payment", "payments", "stripe", "paypal", "refund", "order", "orders"},
            "ar": {"Ù…ØªØ¬Ø±", "Ø³ÙˆÙ‚", "Ø³Ù„Ø©", "Ù…Ù†ØªØ¬", "Ù…Ù†ØªØ¬Ø§Øª", "Ø´Ø­Ù†", "ØªÙˆØµÙŠÙ„", "Ø¨Ø§Ø¦Ø¹", "Ø·Ù„Ø¨",
                   "Ø·Ù„Ø¨Ø§Øª", "Ø¯ÙØ¹", "Ù…Ø¯ÙÙˆØ¹Ø§Øª", "ÙƒØªØ§Ù„ÙˆØ¬", "Ù…Ø®Ø²ÙˆÙ†"},
        },
        "medical": {
            "en": {"hospital", "clinic", "patient", "doctor", "medical", "health", "prescription",
                   "appointment", "ehr", "emr", "hipaa", "pharmacy", "diagnosis", "lab", "nurse",
                   "telemedicine", "telehealth", "healthcare"},
            "ar": {"Ù…Ø³ØªØ´ÙÙ‰", "Ø¹ÙŠØ§Ø¯Ø©", "Ù…Ø±ÙŠØ¶", "Ø·Ø¨ÙŠØ¨", "ØµØ­Ø©", "ÙˆØµÙØ©", "Ù…ÙˆØ¹Ø¯", "ØªØ´Ø®ÙŠØµ",
                   "ØµÙŠØ¯Ù„ÙŠØ©", "Ù…Ø®ØªØ¨Ø±", "Ù…Ù…Ø±Ø¶", "Ø·Ø¨"},
        },
        "education": {
            "en": {"school", "university", "student", "teacher", "course", "lesson", "exam",
                   "grade", "attendance", "lms", "e-learning", "elearning", "classroom",
                   "assignment", "curriculum", "tutor", "training"},
            "ar": {"Ù…Ø¯Ø±Ø³Ø©", "Ø¬Ø§Ù…Ø¹Ø©", "Ø·Ø§Ù„Ø¨", "Ø·Ù„Ø§Ø¨", "Ù…Ø¹Ù„Ù…", "Ù…Ø¯Ø±Ø³", "Ø¯ÙˆØ±Ø©", "Ø¯Ø±Ø³",
                   "Ø§Ù…ØªØ­Ø§Ù†", "Ø¯Ø±Ø¬Ø©", "Ø­Ø¶ÙˆØ±", "ØºÙŠØ§Ø¨", "ÙØµÙ„", "Ù…Ù†Ø§Ù‡Ø¬"},
        },
        "fintech": {
            "en": {"bank", "banking", "loan", "credit", "debit", "wallet", "transaction",
                   "transfer", "fintech", "investment", "stock", "trading", "insurance",
                   "budget", "accounting", "invoice", "payroll"},
            "ar": {"Ø¨Ù†Ùƒ", "Ø¨Ù†ÙƒÙŠ", "Ù‚Ø±Ø¶", "Ø§Ø¦ØªÙ…Ø§Ù†", "Ù…Ø­ÙØ¸Ø©", "ØªØ­ÙˆÙŠÙ„", "Ù…Ø¯ÙÙˆØ¹Ø§Øª", "Ø§Ø³ØªØ«Ù…Ø§Ø±",
                   "ØªØ¯Ø§ÙˆÙ„", "ØªØ£Ù…ÙŠÙ†", "ÙØ§ØªÙˆØ±Ø©", "Ù…Ø­Ø§Ø³Ø¨Ø©", "Ø±ÙˆØ§ØªØ¨"},
        },
        "hr": {
            "en": {"employee", "employees", "hr", "human resources", "payroll", "recruitment",
                   "attendance", "leave", "vacation", "kpi", "performance", "onboarding",
                   "salary", "staff", "workforce"},
            "ar": {"Ù…ÙˆØ¸Ù", "Ù…ÙˆØ¸ÙÙŠÙ†", "Ù…ÙˆØ§Ø±Ø¯ Ø¨Ø´Ø±ÙŠØ©", "Ø±ÙˆØ§ØªØ¨", "ØªØ¹ÙŠÙŠÙ†", "Ø­Ø¶ÙˆØ±", "Ø¥Ø¬Ø§Ø²Ø©",
                   "Ø£Ø¯Ø§Ø¡", "Ù…Ø¤Ø´Ø±Ø§Øª", "Ù‡ÙŠÙƒÙ„", "ÙƒØ§Ø¯Ø±"},
        },
    }

    _DOMAIN_GUIDANCE_EN: Dict[str, str] = {
        "ecommerce": (
            "## Domain Context: E-Commerce\n"
            "Prioritise exploring: payment gateways (Stripe/PayPal/local), cart & checkout flow, "
            "inventory management, multi-vendor vs single-seller model, shipping integrations, "
            "return & refund policies, and tax/VAT handling. Ask about expected product volume "
            "and peak traffic."
        ),
        "medical": (
            "## Domain Context: Healthcare / Medical\n"
            "Prioritise exploring: patient data privacy regulations (HIPAA, GDPR, local law), "
            "role-based access to medical records, audit logging, appointment scheduling workflow, "
            "integration with lab/pharmacy systems, telemedicine requirements, and data backup "
            "frequency. Ask explicitly about compliance obligations."
        ),
        "education": (
            "## Domain Context: Education / E-Learning\n"
            "Prioritise exploring: student enrollment & authentication, grading & GPA systems, "
            "attendance tracking, course content formats (video/PDF/quiz), assignment submission, "
            "parent/teacher/admin roles, integration with SIS or LMS (Moodle/Canvas), and "
            "offline access requirements."
        ),
        "fintech": (
            "## Domain Context: FinTech / Banking\n"
            "Prioritise exploring: regulatory compliance (PCI-DSS, AML, KYC), transaction "
            "security and fraud detection, multi-currency support, real-time vs batch settlement, "
            "audit trails, two-factor authentication, and integration with banking APIs or SWIFT."
        ),
        "hr": (
            "## Domain Context: HR & Workforce Management\n"
            "Prioritise exploring: payroll calculation rules (overtime, deductions, taxes), "
            "leave/vacation balance policies, performance review cycles, recruitment pipeline, "
            "org chart structure, integration with government social-insurance portals, "
            "and employee self-service portal requirements."
        ),
    }

    _DOMAIN_GUIDANCE_AR: Dict[str, str] = {
        "ecommerce": (
            "## Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø¬Ø§Ù„: Ø§Ù„ØªØ¬Ø§Ø±Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©\n"
            "Ø±ÙƒÙ‘Ø² Ø¹Ù„Ù‰: Ø¨ÙˆØ§Ø¨Ø§Øª Ø§Ù„Ø¯ÙØ¹ (ÙÙŠØ²Ø§/Ù…Ø§Ø³ØªØ±ÙƒØ§Ø±Ø¯/Ø¨Ø§ÙŠ Ù…ÙˆØ¨Ø§ÙŠÙ„)ØŒ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ø³Ù„Ø© ÙˆØ§Ù„Ø¯ÙØ¹ØŒ "
            "Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø²ÙˆÙ†ØŒ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø¦Ø¹ ÙˆØ§Ø­Ø¯ Ø£Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø¨Ø§Ø¦Ø¹ÙŠÙ†ØŒ ØªÙƒØ§Ù…Ù„Ø§Øª Ø§Ù„Ø´Ø­Ù† ÙˆØ§Ù„ØªÙˆØµÙŠÙ„ØŒ "
            "Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØ§Ù„Ø§Ø³ØªØ±Ø¯Ø§Ø¯ØŒ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ ÙˆØ§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¶Ø§ÙØ©. "
            "Ø§Ø³Ø£Ù„ Ø¹Ù† Ø­Ø¬Ù… Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙˆØ°Ø±ÙˆØ© Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª."
        ),
        "medical": (
            "## Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø¬Ø§Ù„: Ø§Ù„Ø±Ø¹Ø§ÙŠØ© Ø§Ù„ØµØ­ÙŠØ© / Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø¨ÙŠ\n"
            "Ø±ÙƒÙ‘Ø² Ø¹Ù„Ù‰: Ø®ØµÙˆØµÙŠØ© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø¶Ù‰ ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØŒ Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø¯ÙˆØ§Ø± "
            "Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø±ÙŠØ¶ØŒ Ø³Ø¬Ù„ Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ (Audit Log)ØŒ Ø³ÙŠØ± Ø¹Ù…Ù„ Ø­Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ØŒ Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ "
            "Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…Ø®ØªØ¨Ø±Ø§Øª ÙˆØ§Ù„ØµÙŠØ¯Ù„ÙŠØ§ØªØŒ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø·Ø¨ Ø¹Ù† Ø¨ÙØ¹Ø¯ØŒ ÙˆØªÙƒØ±Ø§Ø± Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ."
        ),
        "education": (
            "## Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø¬Ø§Ù„: Ø§Ù„ØªØ¹Ù„ÙŠÙ… / Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ\n"
            "Ø±ÙƒÙ‘Ø² Ø¹Ù„Ù‰: ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø·Ù„Ø§Ø¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‡ÙˆÙŠØ©ØŒ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯Ø±Ø¬Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø¯Ù„ØŒ ØªØªØ¨Ø¹ Ø§Ù„Ø­Ø¶ÙˆØ± "
            "ÙˆØ§Ù„ØºÙŠØ§Ø¨ØŒ ØµÙŠØº Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ (ÙÙŠØ¯ÙŠÙˆ/PDF/Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª)ØŒ ØªØ³Ù„ÙŠÙ… Ø§Ù„Ù…Ù‡Ø§Ù…ØŒ ØµÙ„Ø§Ø­ÙŠØ§Øª "
            "Ø§Ù„Ù…Ø¯Ø±Ø³ ÙˆØ§Ù„Ø·Ø§Ù„Ø¨ ÙˆÙˆÙ„ÙŠ Ø§Ù„Ø£Ù…Ø± ÙˆØ§Ù„Ø¥Ø¯Ø§Ø±Ø©ØŒ ÙˆØ§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø£Ù†Ø¸Ù…Ø© Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªØ¹Ù„Ù…."
        ),
        "fintech": (
            "## Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø¬Ø§Ù„: Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù…Ø§Ù„ÙŠØ© / Ø§Ù„Ø¨Ù†ÙˆÙƒ\n"
            "Ø±ÙƒÙ‘Ø² Ø¹Ù„Ù‰: Ø§Ù„Ø§Ù…ØªØ«Ø§Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ (PCI-DSSØŒ Ù…ÙƒØ§ÙØ­Ø© ØºØ³ÙŠÙ„ Ø§Ù„Ø£Ù…ÙˆØ§Ù„ØŒ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‡ÙˆÙŠØ©)ØŒ "
            "Ø£Ù…Ø§Ù† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª ÙˆÙƒØ´Ù Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ØŒ Ø¯Ø¹Ù… Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ Ø§Ù„ØªØ³ÙˆÙŠØ© Ø§Ù„ÙÙˆØ±ÙŠØ© Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø¯ÙØ¹ÙŠØ©ØŒ "
            "Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø© Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ©ØŒ ÙˆØ³Ø¬Ù„Ø§Øª Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚."
        ),
        "hr": (
            "## Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø¬Ø§Ù„: Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ© ÙˆØ¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù‚ÙˆÙ‰ Ø§Ù„Ø¹Ø§Ù…Ù„Ø©\n"
            "Ø±ÙƒÙ‘Ø² Ø¹Ù„Ù‰: Ù‚ÙˆØ§Ø¹Ø¯ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø±ÙˆØ§ØªØ¨ (Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØŒ Ø§Ù„Ø®ØµÙˆÙ…Ø§ØªØŒ Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨)ØŒ Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø¥Ø¬Ø§Ø²Ø§ØªØŒ "
            "Ø¯ÙˆØ±Ø§Øª ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ØŒ Ù…Ø³Ø§Ø± Ø§Ù„ØªÙˆØ¸ÙŠÙØŒ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠØŒ Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø¨ÙˆØ§Ø¨Ø§Øª Ø§Ù„ØªØ£Ù…ÙŠÙ†Ø§Øª "
            "Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ©ØŒ ÙˆÙ…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ø°Ø§ØªÙŠØ© Ù„Ù„Ù…ÙˆØ¸ÙÙŠÙ†."
        ),
    }

    @staticmethod
    def _detect_domain(conversation: str) -> str | None:
        """Detect broad project domain from conversation text; returns key or None."""
        lower = conversation.lower()
        tokens = set(re.split(r"[\sØŒ,ØŒ.ØŸ?!\-/]+", lower))
        scores: Dict[str, int] = {}
        for domain, lang_signals in InterviewService._DOMAIN_SIGNALS.items():
            count = 0
            for word_set in lang_signals.values():
                for w in word_set:
                    if w in tokens or w in lower:
                        count += 1
            if count:
                scores[domain] = count
        if not scores:
            return None
        return max(scores, key=lambda d: scores[d])

    @staticmethod
    def _build_prompt(conversation: str, language: str,
                      last_summary: Dict | None = None,
                      last_coverage: Dict | None = None,
                      reflector_signals: Dict | None = None,
                      srs_context: Dict[str, Any] | None = None) -> tuple[str, str]:
        """Return (system_prompt, user_prompt) so the caller can pass them separately."""
        system = _AR_SYSTEM if language == "ar" else _EN_SYSTEM
        conv_label = "Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©" if language == "ar" else "Conversation"
        json_label = "JSON ÙÙ‚Ø·" if language == "ar" else "JSON only"

        parts: list[str] = []

        # Inject domain-aware guidance
        domain = InterviewService._detect_domain(conversation)
        if domain:
            guidance_map = (
                InterviewService._DOMAIN_GUIDANCE_AR
                if language == "ar"
                else InterviewService._DOMAIN_GUIDANCE_EN
            )
            guidance = guidance_map.get(domain)
            if guidance:
                parts.append(guidance)

        # Inject previous state so LLM builds on it
        if last_summary or last_coverage:
            if language == "ar":
                parts.append("## Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© (Ø§Ø¨Ø¯Ø£ Ù…Ù† Ù‡Ù†Ø§ ÙˆÙ„Ø§ ØªØ­Ø°Ù Ø´ÙŠØ¦Ø§Ù‹)")
            else:
                parts.append("## Previous State (start from here, do NOT remove anything)")

            if last_summary:
                parts.append(f"Previous summary:\n{json.dumps(last_summary, ensure_ascii=False)}")
            if last_coverage:
                parts.append(f"Previous coverage:\n{json.dumps(last_coverage, ensure_ascii=False)}")

        if reflector_signals:
            if language == "ar":
                parts.append("\n## Ø¥Ø´Ø§Ø±Ø§Øª ÙˆÙƒÙŠÙ„ Ø§Ù„Ù†Ù‚Ø¯ (Reflector)")
                parts.append(
                    "Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ù„ØªØ¹Ø¯ÙŠÙ„ Ø£Ø³Ù„ÙˆØ¨Ùƒ Ù‚Ø¨Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ:"
                    f"\n{json.dumps(reflector_signals, ensure_ascii=False)}"
                )
            else:
                parts.append("\n## Reflector Signals")
                parts.append(
                    "Use these signals to adapt your interview question before responding:"
                    f"\n{json.dumps(reflector_signals, ensure_ascii=False)}"
                )

        if srs_context:
            if language == "ar":
                parts.append("\n## Ø³ÙŠØ§Ù‚ SRS Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹")
                parts.append(f"Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ ÙƒØ³ÙŠØ§Ù‚ Ù…Ø±Ø¬Ø¹ÙŠ Ù‚Ø¨Ù„ Ø·Ø±Ø­ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ:\n{InterviewService._stringify_srs_context(srs_context)}")
            else:
                parts.append("\n## Current Project SRS Context")
                parts.append(
                    "Use this as a reference context before asking the next question:"
                    f"\n{InterviewService._stringify_srs_context(srs_context)}"
                )

        parts.append(f"\n{conv_label}:\n{conversation}\n\n{json_label}:")

        return system, "\n".join(parts)

    @staticmethod
    def _stringify_srs_context(srs_context: Dict[str, Any]) -> str:
        text = json.dumps(srs_context, ensure_ascii=False)
        if len(text) <= _MAX_SRS_CONTEXT_CHARS:
            return text
        return f"{text[:_MAX_SRS_CONTEXT_CHARS]}â€¦"

    @staticmethod
    async def _get_latest_srs_context(db: AsyncSession, project_id: int) -> Dict[str, Any] | None:
        stmt = (
            select(SRSDraft)
            .where(SRSDraft.project_id == project_id)
            .order_by(SRSDraft.version.desc(), SRSDraft.created_at.desc())
            .limit(1)
        )
        result = await db.execute(stmt)
        draft = result.scalar_one_or_none()
        if not draft:
            return None

        content = draft.content if isinstance(draft.content, dict) else {}
        if not content:
            return None

        return {
            "version": int(draft.version or 1),
            "status": str(draft.status or "draft"),
            "language": str(draft.language or "ar"),
            "content": content,
        }

    @staticmethod
    def _parse_json(raw: str) -> Dict[str, Any]:
        import json_repair
        payload = str(raw or "").strip()
        if not payload:
            logger.warning("Empty interview JSON response")
            return {}
        try:
            # Use json_repair to handle truncated or malformed JSON
            parsed_response = json_repair.loads(payload)
            if not isinstance(parsed_response, dict):
                raise ValueError("Parsed JSON is not a dictionary")
            return parsed_response
        except Exception as e:
            logger.error(f"Failed to parse SRS JSON even with repair: {e}")
            logger.error(f"Raw response was: {payload}")
            # Return a safe fallback instead of crashing
            return {
                "question": "ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… ØªÙØ§ØµÙŠÙ„ ÙƒØ«ÙŠØ±Ø©ØŒ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§. Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø¥Ø¶Ø§ÙØ§Øª Ø£Ø®Ø±Ù‰ØŸ",
                "stage": "discovery",
                "done": False,
                "suggested_answers": [],
                "patches": [],
                "coverage": {}
            }

    @staticmethod
    def _extract_fenced_json(payload: str) -> str | None:
        match = re.fullmatch(r"```(?:json)?\s*(\{[\s\S]*\}|\[[\s\S]*\])\s*```", payload, flags=re.IGNORECASE)
        if not match:
            return None
        return match.group(1)

    @staticmethod
    def _extract_balanced_json(payload: str) -> str | None:
        start = payload.find("{")
        if start == -1:
            return None

        depth = 0
        in_string = False
        escaped = False
        for idx in range(start, len(payload)):
            char = payload[idx]
            if escaped:
                escaped = False
                continue
            if char == "\\":
                escaped = True
                continue
            if char == '"':
                in_string = not in_string
                continue
            if in_string:
                continue

            if char == "{":
                depth += 1
            elif char == "}":
                depth -= 1
                if depth == 0:
                    return payload[start : idx + 1]
        return None

################################################################################
# FILE: backend\services\judging_service.py
################################################################################

from __future__ import annotations

import asyncio
import json
import logging
import re
from datetime import datetime, timezone
from typing import Any, Dict, Optional

from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession

from backend.database.models import SRSDraft
from backend.providers.llm.factory import LLMProviderFactory
from backend.services.srs_snapshot_cache import SRSSnapshotCache

logger = logging.getLogger(__name__)


class JudgingService:
    """
        Standalone Judging Service for verifying, critiquing, and refining SRS.
        New Feature/Refactor: Consolidated prompts, refined async handling, and structured JSON extraction.
        Author: Adel Sobhy
        Date: 2026-02-15
    """

    def __init__(self, llm_provider=None):
        self.llm_provider = llm_provider or LLMProviderFactory.create_provider()
        logger.info("Judging service initialized")

    async def judge_and_refine(
        self,
        srs_content: Dict[str, Any],
        analysis_content: str = "",
        language: str = "ar",
        store_refined: bool = False,
        db: Optional[AsyncSession] = None,
        project_id: Optional[int] = None,
    ) -> Dict[str, Any]:
        """Judge SRS, refine it, generate summary, and optionally store in DB."""
        logger.info(f"Starting judgment for project {project_id} (lang={language})")

        # --- 1. Parallel Critiques ---
        tech_critique, biz_critique = await asyncio.gather(
            self._get_technical_critique(srs_content, language),
            self._get_business_critique(srs_content, language),
        )

        # --- 2. Refine SRS ---
        refined_srs = await self._refine_srs(srs_content, tech_critique, biz_critique, language)

        # --- 3. Generate Summary ---
        summary = await self._generate_summary(tech_critique, biz_critique, language)

        result = {
            "technical_critique": tech_critique,
            "business_critique": biz_critique,
            "refined_srs": refined_srs,
            "refined_analysis": analysis_content,
            "summary": summary,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

        if store_refined and db is not None and project_id is not None:
            await self._store_refined_draft(db, project_id, language, refined_srs)

        return result

    async def _store_refined_draft(
        self,
        db: AsyncSession,
        project_id: int,
        language: str,
        refined_srs: Dict[str, Any],
    ) -> None:
        next_version_stmt = select(func.coalesce(func.max(SRSDraft.version), 0) + 1).where(
            SRSDraft.project_id == project_id
        )
        result = await db.execute(next_version_stmt)
        next_version = result.scalar_one()

        draft = SRSDraft(
            project_id=project_id,
            version=next_version,
            status="refined",
            language=language,
            content=refined_srs,
        )
        db.add(draft)
        await db.flush()
        await SRSSnapshotCache.set_from_draft(draft)

    # ------------------- AGENT METHODS -------------------

    async def _get_technical_critique(self, srs_content, language):
        prompt = self._build_technical_critique_prompt(srs_content, language)
        raw = await self.llm_provider.generate_text(prompt=prompt, temperature=0.3, max_tokens=3000)
        return self._extract_json(raw) or {"error": "Technical critique failed"}

    async def _get_business_critique(self, srs_content, language):
        prompt = self._build_business_critique_prompt(srs_content, language)
        raw = await self.llm_provider.generate_text(prompt=prompt, temperature=0.3, max_tokens=3000)
        return self._extract_json(raw) or {"error": "Business critique failed"}

    async def _refine_srs(self, srs_content, tech, biz, language):
        prompt = self._build_srs_refinement_prompt(srs_content, tech, biz, language)
        raw = await self.llm_provider.generate_text(
            prompt=prompt,
            temperature=0.2,
            max_tokens=3000
        )

        refined = self._extract_json(raw)   

        if not refined:
            logger.warning("Refinement failed â€” returning original SRS")
            return srs_content

        required_keys = ["summary", "metrics", "sections", "questions", "next_steps"]

        for key in required_keys:
            if key not in refined:
                logger.warning(f"Missing key '{key}' in refined output. Falling back.")
                return srs_content

        return refined


    async def _generate_summary(self, tech, biz, language):
        prompt = self._build_summary_prompt(tech, biz, language)
        raw = await self.llm_provider.generate_text(prompt=prompt, temperature=0.4, max_tokens=2000)
        return self._extract_json(raw) or {"status": "Summary generation failed"}

    # ------------------- PROMPT BUILDERS -------------------

    def _build_technical_critique_prompt(self, srs_content, language):
        srs_json = json.dumps(srs_content, ensure_ascii=False, indent=2)
        if language == "ar":
            return (
                "Ø£Ù†Øª ÙƒØ¨ÙŠØ± Ù…Ù‡Ù†Ø¯Ø³ÙŠ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ§Øª (CTO/Architect) Ø¨Ø®Ø¨Ø±Ø© ÙÙŠ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª.\n"
                "Ù…Ù‡Ù…ØªÙƒ: Ø¥Ø¬Ø±Ø§Ø¡ Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙ‚Ù†ÙŠØ© Ù…ÙØµÙ‘Ù„Ø© Ù„ÙˆØ«ÙŠÙ‚Ø© SRS Ø§Ù„ØªØ§Ù„ÙŠØ©.\n\n"
                "ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©:\n"
                "- Ø¹Ù†Ø¯ ØªØ­Ø¯ÙŠØ¯ Ø£ÙŠ Ù†Ù‚Ø·Ø© Ø¶Ø¹ÙØŒ Ø§Ø°ÙƒØ± Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ù…Ø¹Ù†ÙŠ Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙƒÙ…Ø§ ÙŠØ¸Ù‡Ø± ÙÙŠ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©.\n"
                "- Ø­Ø¯Ø¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø© Ù„ÙƒÙ„ Ù†Ù‚Ø·Ø© Ø¶Ø¹Ù: Critical | High | Medium | Low.\n"
                "- Ù„ÙƒÙ„ Ù†Ù‚Ø·Ø© Ø¶Ø¹ÙØŒ Ù‚Ø¯Ù‘Ù… Ø¥Ø¬Ø±Ø§Ø¡ Ø¥ØµÙ„Ø§Ø­ÙŠ Ù…Ø­Ø¯Ø¯Ù‹Ø§ ÙˆÙ‚Ø§Ø¨Ù„Ø§Ù‹ Ù„Ù„ØªÙ†ÙÙŠØ°.\n"
                "- Ù„Ø§ ØªØ¶Ù Ù…ØªØ·Ù„Ø¨Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©.\n"
                "- Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙŠØ¬Ø¨ Ø£Ù† ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… Ø¨Ø¹ÙŠÙ†Ù‡Ø§.\n\n"
                "Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ØªØ±ÙƒÙŠØ²:\n"
                "Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© ÙˆÙ†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ù…ØµØ§Ø¯Ù‚Ø©ØŒ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆÙ‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹ØŒ "
                "Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙƒØ§Ù…Ù„ØŒ Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©ØŒ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª ØºÙŠØ± Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ© Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©.\n\n"
                "Ø£Ø¹Ø¯ JSON ØµØ§Ù„Ø­ ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:\n"
                '{"strengths": [{"section": "...", "observation": "..."}], '
                '"weaknesses": [{"section": "...", "risk": "Critical|High|Medium|Low", "issue": "...", "fix": "..."}], '
                '"risks": [{"description": "...", "mitigation": "..."}], '
                '"recommendations": ["..."]}\n\n'
                f"ÙˆØ«ÙŠÙ‚Ø© SRS:\n{srs_json}"
            )
        return (
            "You are a Senior Software Architect / CTO with deep experience reviewing software requirements documents.\n"
            "Task: Perform a structured technical review of the SRS below.\n\n"
            "Review instructions:\n"
            "- When identifying a weakness, name the EXACT section title from the document.\n"
            "- Assign a risk level to each weakness: Critical | High | Medium | Low.\n"
            "- For each weakness, provide a specific, actionable remediation step.\n"
            "- Do NOT add new requirements that are absent from the document.\n"
            "- Strengths must reference specific section titles.\n\n"
            "Focus areas:\n"
            "Architecture & data model, Security & authentication, Performance & scalability, "
            "Integration points, Testability of stated requirements, Missing non-functional requirements.\n\n"
            "Return ONLY valid JSON:\n"
            '{"strengths": [{"section": "...", "observation": "..."}], '
            '"weaknesses": [{"section": "...", "risk": "Critical|High|Medium|Low", "issue": "...", "fix": "..."}], '
            '"risks": [{"description": "...", "mitigation": "..."}], '
            '"recommendations": ["..."]}\n\n'
            f"SRS document:\n{srs_json}"
        )

    def _build_business_critique_prompt(self, srs_content, language):
        srs_json = json.dumps(srs_content, ensure_ascii=False, indent=2)
        if language == "ar":
            return (
                "Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø£Ø¹Ù…Ø§Ù„ ÙˆÙ…Ø¯ÙŠØ± Ù…Ù†ØªØ¬ Ø¨Ø®Ø¨Ø±Ø© ÙÙŠ ØªÙ‚ÙŠÙŠÙ… ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª.\n"
                "Ù…Ù‡Ù…ØªÙƒ: ØªÙ‚ÙŠÙŠÙ… ÙˆØ«ÙŠÙ‚Ø© SRS Ù…Ù† Ù…Ù†Ø¸ÙˆØ± Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ ÙˆØªÙˆØ§ÙÙ‚Ù‡Ø§ Ù…Ø¹ Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„.\n\n"
                "ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:\n"
                "- Ø§Ø°ÙƒØ± Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ù…Ø¹Ù†ÙŠ Ø¨Ø§Ù„Ø¶Ø¨Ø· Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ø£ÙŠ Ù…Ø´ÙƒÙ„Ø©.\n"
                "- Ø§ÙƒØ´Ù Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØºØ§Ù…Ø¶Ø© ØºÙŠØ± Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù‚ÙŠØ§Ø³ (Ù…Ø«Ù„: Ø³Ø±ÙŠØ¹ØŒ Ø¬ÙŠØ¯ØŒ Ø¹Ø§Ø¯ÙŠØŒ Ù…Ø±Ù†) ÙˆÙ‚ØªØ±Ø­ Ø¨Ø¯ÙŠÙ„Ø§Ù‹ Ù‚Ø§Ø¨Ù„Ø§Ù‹ Ù„Ù„Ù‚ÙŠØ§Ø³.\n"
                "- Ø­Ø¯Ø¯ ÙˆØ¬Ù‡Ø§Øª Ù†Ø¸Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø£Ùˆ Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©.\n"
                "- Ù‚ÙŠÙ‘Ù… ÙˆØ¶ÙˆØ­ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ ÙˆØ£Ù‡Ø¯Ø§Ù Ø§Ù„Ù†Ø¬Ø§Ø­.\n\n"
                "Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ØªØ±ÙƒÙŠØ²:\n"
                "ÙˆØ¶ÙˆØ­ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ØŒ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù‚ÙŠØ§Ø³ØŒ Ø§ÙƒØªÙ…Ø§Ù„ Ø±Ø­Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ "
                "Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØºØ§Ù…Ø¶Ø©ØŒ ÙˆØ¶ÙˆØ­ Ø§Ù„Ù†Ø·Ø§Ù‚ØŒ ØªÙˆØ§ÙÙ‚ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª.\n\n"
                "Ø£Ø¹Ø¯ JSON ØµØ§Ù„Ø­ ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:\n"
                '{"strengths": [{"section": "...", "observation": "..."}], '
                '"weaknesses": [{"section": "...", "issue": "...", "suggested_fix": "..."}], '
                '"vague_requirements": [{"original": "...", "suggested_replacement": "..."}], '
                '"missing_perspectives": ["..."], '
                '"recommendations": ["..."]}\n\n'
                f"ÙˆØ«ÙŠÙ‚Ø© SRS:\n{srs_json}"
            )
        return (
            "You are a Product Manager / Business Analyst with experience evaluating requirements documents.\n"
            "Task: Evaluate the SRS below for business viability and client alignment.\n\n"
            "Review instructions:\n"
            "- Reference EXACT section titles from the document when identifying issues.\n"
            "- Detect vague, unmeasurable requirements (e.g. fast, good, normal, scalable, flexible) "
            "  and suggest concrete, measurable replacements.\n"
            "- Identify missing stakeholder perspectives or user scenarios.\n"
            "- Assess clarity of business value and success criteria.\n\n"
            "Focus areas:\n"
            "Business value clarity, Measurable success metrics, User journey completeness, "
            "Vague/unmeasurable requirements, Scope clarity, Priority alignment.\n\n"
            "Return ONLY valid JSON:\n"
            '{"strengths": [{"section": "...", "observation": "..."}], '
            '"weaknesses": [{"section": "...", "issue": "...", "suggested_fix": "..."}], '
            '"vague_requirements": [{"original": "...", "suggested_replacement": "..."}], '
            '"missing_perspectives": ["..."], '
            '"recommendations": ["..."]}\n\n'
            f"SRS document:\n{srs_json}"
        )

    def _build_srs_refinement_prompt(self, srs_content, tech, biz, language):
        srs_json = json.dumps(srs_content, ensure_ascii=False, indent=2)
        tech_json = json.dumps(tech, ensure_ascii=False, indent=2)
        biz_json = json.dumps(biz, ensure_ascii=False, indent=2)

        if language == "ar":
            return (
                "Ø£Ù†Øª Ù…Ù‡Ù†Ø¯Ø³ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø£ÙˆÙ„ ÙˆÙ…Ù‡Ù†Ø¯Ø³ Ø¨Ø±Ù…Ø¬ÙŠØ§Øª Ù…ØªÙ…Ø±Ø³.\n"
                "Ù…Ù‡Ù…ØªÙƒ: Ø¥Ù†ØªØ§Ø¬ Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù‘Ù†Ø© Ù…Ù† ÙˆØ«ÙŠÙ‚Ø© SRS Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙˆØ§Ù„ØªØ¬Ø§Ø±ÙŠØ©.\n\n"
                "Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµØ§Ø±Ù…Ø©:\n"
                "1) Ø·Ø¨Ù‘Ù‚ Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ù…Ù† ÙƒÙ„Ø§ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…ÙŠÙ†.\n"
                "2) Ø§Ø³ØªØ¨Ø¯Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„ØºØ§Ù…Ø¶Ø© (Ø³Ø±ÙŠØ¹ØŒ Ø¬ÙŠØ¯ØŒ Ø¹Ø§Ø¯ÙŠØŒ Ù‚ÙŠØ§Ø³ÙŠØŒ Ù…Ø±Ù†) Ø¨Ù‚ÙŠÙ… Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù‚ÙŠØ§Ø³ "
                "   Ù…Ø³ØªÙ…Ø¯Ø© Ù…Ù† Ø³ÙŠØ§Ù‚ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©. Ø¥Ø°Ø§ Ù„Ù… ØªØªÙˆÙØ± Ù‚ÙŠÙ…Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù‚ÙŠØ§Ø³ØŒ Ø£Ø¶Ù Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù„Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ Ù‚Ø³Ù… questions "
                "   Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ø®ØªØ±Ø§Ø¹ Ø±Ù‚Ù….\n"
                "3) Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø¨ØµÙŠØºØ© 'ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙˆÙØ± Ø§Ù„Ù†Ø¸Ø§Ù…...' Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¨Ù‡Ø°Ù‡ Ø§Ù„ØµÙŠØºØ©.\n"
                "4) Ù„Ø§ ØªØ­Ø°Ù Ø£ÙŠ Ù…ØªØ·Ù„Ø¨ Ù…ÙˆØ¬ÙˆØ¯ â€” ÙÙ‚Ø· Ø­Ø³Ù‘Ù†Ù‡ Ø£Ùˆ Ø£Ø¹Ø¯ ØµÙŠØ§ØºØªÙ‡.\n"
                "5) Ø§Ø­ØªÙØ¸ Ø¨Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù€ 7 Ø£Ù‚Ø³Ø§Ù… Ø¨Ø§Ù„Ø¶Ø¨Ø·.\n"
                "6) ÙƒÙ„ Ù†Ù‚Ø·Ø© Ø¶Ø¹Ù Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…ÙŠÙ† ÙŠØ¬Ø¨ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ ÙÙŠ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª.\n"
                "7) Ø£Ø¹Ø¯ JSON ØµØ§Ù„Ø­ ÙÙ‚Ø· â€” Ø¨Ø¯ÙˆÙ† markdown Ø£Ùˆ Ø´Ø±Ø­.\n\n"
                f"Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø§Ù„Ø£ØµÙ„ÙŠ:\n{srs_json}\n\n"
                f"Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªÙ‚Ù†ÙŠ:\n{tech_json}\n\n"
                f"Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ:\n{biz_json}\n\n"
                "Ø£Ø¹Ø¯ Ù†ÙØ³ Ù‡ÙŠÙƒÙ„ JSON Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù‚ÙŠÙ…. Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ©: "
                "summary, metrics, sections, questions, next_steps"
            )

        return (
            "You are a senior requirements engineer and software architect.\n"
            "Task: Produce an improved version of the SRS by applying all critique findings.\n\n"
            "STRICT REFINEMENT RULES:\n"
            "1) Apply concrete fixes from BOTH technical and business critiques.\n"
            "2) Replace ALL vague terms (fast, good, normal, scalable, flexible, standard) with measurable "
            "   values drawn from project context. If no measurable value is available, add a client-facing "
            "   question to 'questions' instead of inventing a number.\n"
            "3) Rewrite requirements in 'The system SHALL...' format if not already.\n"
            "4) Do NOT remove any existing requirement â€” only improve or rephrase.\n"
            "5) Maintain the exact 7-section structure.\n"
            "6) Every weakness cited in the critiques MUST be addressed in the output.\n"
            "7) Return ONLY valid JSON â€” no markdown, no prose.\n\n"
            f"Original SRS:\n{srs_json}\n\n"
            f"Technical critique findings:\n{tech_json}\n\n"
            f"Business critique findings:\n{biz_json}\n\n"
            "Return the same JSON structure with improved values. Required keys: "
            "summary, metrics, sections, questions, next_steps"
        )


    def _build_summary_prompt(self, tech, biz, language):
        tech_json = json.dumps(tech, ensure_ascii=False)
        biz_json = json.dumps(biz, ensure_ascii=False)
        if language == "ar":
            return (
                "ÙˆÙ„Ù‘Ø¯ Ù…Ù„Ø®ØµÙ‹Ø§ ØªÙ†ÙÙŠØ°ÙŠÙ‹Ø§ Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©.\n"
                f"Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªÙ‚Ù†ÙŠ: {tech_json}\n"
                f"Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØ¬Ø§Ø±ÙŠ: {biz_json}\n"
                "Ø£Ø¹Ø¯ JSON ØµØ§Ù„Ø­ ÙÙ‚Ø· Ø¨Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„ØªØ§Ù„ÙŠØ©:\n"
                '{"overall_quality": "Ù…Ù…ØªØ§Ø²|Ø¬ÙŠØ¯|Ù…Ù‚Ø¨ÙˆÙ„|Ø¶Ø¹ÙŠÙ", '
                '"key_strengths": ["..."], '
                '"key_risks": ["..."], '
                '"priority_improvements": ["..."], '
                '"readiness_level": "Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ·ÙˆÙŠØ±|ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©|ÙŠØ­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ø¹Ù…Ù„"}'
            )
        return (
            "Generate an executive summary from the following critique findings.\n"
            f"Technical critique: {tech_json}\n"
            f"Business critique: {biz_json}\n"
            "Return ONLY valid JSON with these keys:\n"
            '{"overall_quality": "Excellent|Good|Acceptable|Poor", '
            '"key_strengths": ["..."], '
            '"key_risks": ["..."], '
            '"priority_improvements": ["..."], '
            '"readiness_level": "Ready for Development|Needs Review|Needs Rework"}'
        )


    @staticmethod
    def _extract_json(response: str) -> Optional[Dict[str, Any]]:
        """Extract JSON even if wrapped in markdown/codeblocks."""
        try:
            clean = re.sub(r"```json\s*|\s*```", "", response).strip()
            start, end = clean.find("{"), clean.rfind("}") + 1
            if start != -1 and end != 0:
                return json.loads(clean[start:end])
        except Exception as e:
            logger.error(f"JSON extraction error: {e}")
        return None


################################################################################
# FILE: backend\services\live_patch_service.py
################################################################################

"""
Live SRS patch builder for all chat flows (not interview-only).
"""
from __future__ import annotations

import json
import logging
from uuid import uuid4
from typing import Any, Dict, List, Union

from pydantic import BaseModel, Field, ValidationError, field_validator

from backend.database.models import ChatMessage
from backend.providers.llm.factory import LLMProviderFactory
from backend.services.constraints_checker import SemanticEvaluator
from backend.services.interview_service import InterviewService

logger = logging.getLogger(__name__)

_AREAS = ["discovery", "scope", "users", "features", "constraints"]

# Stages the LLM might return that are not in our canonical set â€”
# map them to the closest valid stage so validation never fails.
_STAGE_ALIASES: Dict[str, str] = {
    "elaboration": "features",
    "requirement": "features",
    "requirements": "features",
    "define": "scope",
    "definition": "scope",
    "user": "users",
    "constraint": "constraints",
    "explore": "discovery",
    "exploration": "discovery",
}


class LivePatchExtraction(BaseModel):
    # Each area is a list of strings OR dicts â€” we normalise to strings in the validator.
    summary: Dict[str, List[Union[str, Dict[str, Any]]]]
    coverage: Dict[str, float]
    # Accept any string from the LLM; we coerce it to a valid stage ourselves.
    target_stage: str

    @field_validator("summary", mode="before")
    @classmethod
    def _normalise_summary(
        cls, v: Any
    ) -> Dict[str, List[str]]:
        """
        The LLM sometimes returns rich objects like {"id": "...", "value": "..."}
        instead of plain strings.  Flatten everything to str so downstream
        _merge_summary (which re-attaches ids) works correctly.
        """
        if not isinstance(v, dict):
            return v  # let pydantic surface the real error
        out: Dict[str, List[str]] = {}
        for area in _AREAS:
            raw_items = v.get(area, [])
            if not isinstance(raw_items, list):
                raw_items = []
            strings: List[str] = []
            for item in raw_items:
                if isinstance(item, str):
                    strings.append(item)
                elif isinstance(item, dict):
                    # Prefer "value", fall back to "text", then stringify the whole dict
                    text = item.get("value") or item.get("text") or ""
                    if not text:
                        text = " | ".join(str(val) for val in item.values() if val)
                    if text:
                        strings.append(str(text).strip())
            out[area] = strings
        return out

    @field_validator("target_stage", mode="before")
    @classmethod
    def _coerce_target_stage(cls, v: Any) -> str:
        """
        Normalise whatever string the LLM returns to one of the five valid stages.
        Falls back to 'discovery' if nothing matches.
        """
        if not isinstance(v, str):
            return "discovery"
        normalised = v.strip().lower()
        if normalised in _AREAS:
            return normalised
        # Try alias map
        if normalised in _STAGE_ALIASES:
            return _STAGE_ALIASES[normalised]
        # Try prefix match (e.g. "feature_extraction" â†’ "features")
        for area in _AREAS:
            if normalised.startswith(area) or area.startswith(normalised):
                return area
        logger.warning("Unknown target_stage %r from LLM â€” defaulting to 'discovery'", v)
        return "discovery"


class LivePatchService:
    """Builds cumulative summary/coverage + structured patch events from chat history."""

    @classmethod
    async def build_from_messages(
        cls,
        language: str,
        messages: List[ChatMessage],
        last_summary: Dict[str, Any] | None,
        last_coverage: Dict[str, float] | None,
    ) -> Dict[str, Any]:
        old_summary = InterviewService._normalized_summary(last_summary)
        old_coverage = last_coverage if isinstance(last_coverage, dict) else {}

        extracted = await cls._extract_semantic_state(
            language=language,
            messages=messages,
            old_summary=old_summary,
            old_coverage=old_coverage,
        )

        new_summary = cls._merge_summary(old_summary, extracted.get("summary") or {})
        new_coverage = cls._merge_coverage(extracted.get("coverage") or {}, old_coverage)
        stage = extracted.get("target_stage") or InterviewService._pick_focus_area(new_coverage)

        latest_user = ""
        for msg in reversed(messages):
            if str(msg.role or "").lower() == "user":
                latest_user = str(msg.content or "").strip()
                break

        slot_analysis = await SemanticEvaluator.analyze(
            language=language,
            latest_user_answer=latest_user,
            last_summary=new_summary,
            last_coverage=old_coverage,
        )
        reflector_signals = cls._signals_from_semantic(
            slot_analysis=slot_analysis,
            coverage=new_coverage,
            target_stage=stage,
        )

        doc_patch = InterviewService._build_documentation_patch(
            language=language,
            stage=stage,
            new_summary=new_summary,
            old_summary=old_summary,
            new_coverage=new_coverage,
            old_coverage=old_coverage,
            reflector_signals=reflector_signals,
        )

        cycle_trace = InterviewService._build_cycle_trace(
            language=language,
            stage=stage,
            reflector_signals=reflector_signals,
            coverage=new_coverage,
            doc_patch=doc_patch,
        )
        topic_navigation = InterviewService._build_topic_navigation(
            language=language,
            summary=new_summary,
            coverage=new_coverage,
            reflector_signals=reflector_signals,
        )

        return {
            "stage": stage,
            "summary": new_summary,
            "coverage": new_coverage,
            "signals": reflector_signals,
            "live_patch": doc_patch,
            "cycle_trace": cycle_trace,
            "topic_navigation": topic_navigation,
        }

    @classmethod
    async def _extract_semantic_state(
        cls,
        *,
        language: str,
        messages: List[ChatMessage],
        old_summary: Dict[str, Any],
        old_coverage: Dict[str, float],
    ) -> Dict[str, Any]:
        conversation = cls._render_conversation(messages[-40:])
        system_prompt = (
            "You are an enterprise requirements analyst. "
            "Extract semantic project state from conversation and return strict JSON."
        )
        user_prompt = (
            f"Language: {language}\n"
            f"Previous summary: {json.dumps(old_summary, ensure_ascii=False)}\n"
            f"Previous coverage: {json.dumps(old_coverage, ensure_ascii=False)}\n"
            f"Conversation:\n{conversation}\n\n"
            "CRITICAL: Base your extraction STRICTLY on the provided conversation. Do not hallucinate or invent requirements.\n"
            "Return JSON with keys: summary, coverage, target_stage.\n"
            "summary must contain discovery/scope/users/features/constraints as arrays of plain strings "
            "(each string is one concrete requirement â€” no nested objects).\n"
            f"target_stage must be exactly one of: {_AREAS}.\n"
            "coverage values must be 0..100."
        )

        try:
            raw, _ = await InterviewService._generate_text_resilient(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.1,
                max_tokens=1800,
                breaker_prefix="live_patch_extraction",
                response_format={"type": "json_object"},
            )
            parsed = LivePatchExtraction.model_validate_json(raw)
            return {
                "summary": parsed.summary,
                "coverage": parsed.coverage,
                "target_stage": parsed.target_stage,
            }
        except ValidationError as exc:
            logger.warning("Live patch extraction schema validation failed: %s", exc)
        except Exception as exc:  # noqa: BLE001
            logger.warning("Live patch semantic extraction failed: %s", exc)

        return {
            "summary": old_summary,
            "coverage": old_coverage,
            "target_stage": InterviewService._pick_focus_area(old_coverage),
        }

    @staticmethod
    def _render_conversation(messages: List[ChatMessage]) -> str:
        lines: List[str] = []
        for msg in messages:
            role = str(msg.role or "system").lower()
            if role not in {"user", "assistant", "system"}:
                role = "system"
            content = str(msg.content or "").strip()
            if not content:
                continue
            lines.append(f"{role}: {content[:900]}")
        return "\n".join(lines)

    @staticmethod
    def _merge_summary(
        old_summary: Dict[str, Any],
        extracted_summary: Dict[str, List[str]],
    ) -> Dict[str, List[Dict[str, str]]]:
        merged = InterviewService._normalized_summary(old_summary)
        for area in _AREAS:
            values = extracted_summary.get(area) if isinstance(extracted_summary.get(area), list) else []
            for value in values:
                text = str(value or "").strip()
                if not text:
                    continue
                existing = merged.get(area, [])
                if InterviewService._contains_similar_requirement(existing, text):
                    continue
                existing.append({"id": f"req_{uuid4().hex[:12]}", "value": text})
                merged[area] = existing[:24]
        return merged

    @staticmethod
    def _merge_coverage(new_cov: Dict[str, Any], old_cov: Dict[str, float]) -> Dict[str, float]:
        out: Dict[str, float] = {}
        for area in _AREAS:
            try:
                new_value = float(new_cov.get(area, 0) or 0)
            except (TypeError, ValueError):
                new_value = 0.0
            prev = float(old_cov.get(area, 0) or 0)
            out[area] = round(max(0.0, min(100.0, max(prev, new_value))), 2)
        return out

    @staticmethod
    def _signals_from_semantic(
        *,
        slot_analysis: Dict[str, Any],
        coverage: Dict[str, float],
        target_stage: str,
    ) -> Dict[str, Any]:
        reason = str(slot_analysis.get("reason") or "").strip()
        ambiguity = bool(slot_analysis.get("ambiguity_detected"))
        contradiction = bool(slot_analysis.get("contradiction_detected"))
        scope_budget_risk = bool(slot_analysis.get("scope_budget_risk"))
        low_covered_areas = [area for area in _AREAS if float(coverage.get(area, 0) or 0) < 35][:3]

        question_style = "inference-driven"
        if contradiction or scope_budget_risk:
            question_style = "resolve-conflict"
        elif ambiguity:
            question_style = "clarify-ambiguity"

        return {
            "ambiguity_detected": ambiguity,
            "ambiguity_terms": slot_analysis.get("ambiguity_terms") if isinstance(slot_analysis.get("ambiguity_terms"), list) else [],
            "scope_budget_risk": scope_budget_risk,
            "contradiction_detected": contradiction,
            "reason": reason,
            "recommendation": reason,
            "low_covered_areas": low_covered_areas,
            "question_style": question_style,
            "target_stage": target_stage if target_stage in _AREAS else InterviewService._pick_focus_area(coverage),
            "slot_analysis": slot_analysis,
        }

################################################################################
# FILE: backend\services\resilience_service.py
################################################################################

"""
Shared resilience helpers: in-memory circuit breaker and async failover execution.
"""
from __future__ import annotations

from datetime import datetime, timedelta, timezone
from typing import Any, Awaitable, Callable, Iterable, Optional, Tuple

from backend.services.runtime_metrics import record_cb_event, record_cb_open_check


class CircuitBreakerRegistry:
    """Process-local circuit breaker registry."""

    def __init__(self) -> None:
        self._state: dict[str, tuple[int, datetime | None]] = {}

    @staticmethod
    def _group(key: str) -> str:
        return str(key or "unknown").split(":", 1)[0]

    def is_open(self, key: str) -> bool:
        failures, opened_until = self._state.get(key, (0, None))
        now = datetime.now(timezone.utc)

        if opened_until and opened_until > now:
            record_cb_open_check(self._group(key), True)
            return True

        if opened_until and opened_until <= now:
            self._state[key] = (failures, None)

        record_cb_open_check(self._group(key), False)
        return False

    def record_success(self, key: str) -> None:
        self._state[key] = (0, None)
        record_cb_event(self._group(key), "success")

    def record_failure(self, key: str, threshold: int = 3, cooldown_seconds: int = 45) -> None:
        current_failures, opened_until = self._state.get(key, (0, None))
        now = datetime.now(timezone.utc)

        if opened_until and opened_until > now:
            record_cb_event(self._group(key), "failure")
            return

        failures = current_failures + 1
        if failures >= max(1, int(threshold)):
            opened_until = now + timedelta(seconds=max(1, int(cooldown_seconds)))
            record_cb_event(self._group(key), "opened")
        else:
            opened_until = None

        self._state[key] = (failures, opened_until)
        record_cb_event(self._group(key), "failure")


circuit_breakers = CircuitBreakerRegistry()


async def run_with_failover(
    providers: Iterable[Tuple[str, Callable[[], Awaitable[Any]]]],
    *,
    breaker_prefix: str,
    failure_threshold: int = 3,
    cooldown_seconds: int = 45,
) -> Tuple[Any, str]:
    """Try providers in order with circuit-breaker short-circuiting.

    Returns tuple of (result, provider_name).
    Raises RuntimeError with aggregated failure reason if all fail.
    """
    last_error: Optional[Exception] = None

    for provider_name, call in providers:
        key = f"{breaker_prefix}:{provider_name}"
        if circuit_breakers.is_open(key):
            continue
        try:
            result = await call()
            circuit_breakers.record_success(key)
            return result, provider_name
        except Exception as exc:  # noqa: BLE001
            last_error = exc
            error_text = str(exc).lower()
            is_rate_limited = (
                "429" in error_text
                or "quota exceeded" in error_text
                or "rate limit" in error_text
                or "too many requests" in error_text
            )
            effective_threshold = 1 if is_rate_limited else failure_threshold
            effective_cooldown = max(cooldown_seconds, 180) if is_rate_limited else cooldown_seconds
            circuit_breakers.record_failure(
                key,
                threshold=effective_threshold,
                cooldown_seconds=effective_cooldown,
            )

    if last_error is None:
        raise RuntimeError("No available providers (all circuit breakers are open)")
    raise RuntimeError(f"All providers failed: {last_error}")

################################################################################
# FILE: backend\services\runtime_metrics.py
################################################################################

"""
Runtime Prometheus metrics for backend state and resilience flows.
"""
from __future__ import annotations

try:
    from prometheus_client import Counter, Histogram

    _metrics_enabled = True
    _circuit_breaker_open_checks_total = Counter(
        "tawasul_circuit_breaker_open_checks_total",
        "Circuit breaker open-check outcomes",
        ["group", "is_open"],
    )
    _circuit_breaker_events_total = Counter(
        "tawasul_circuit_breaker_events_total",
        "Circuit breaker state transition events",
        ["group", "event"],
    )
    _srs_snapshot_source_total = Counter(
        "tawasul_srs_snapshot_source_total",
        "Source used to return SRS snapshot",
        ["source"],
    )
    _srs_snapshot_lock_contention_total = Counter(
        "tawasul_srs_snapshot_lock_contention_total",
        "Number of SRS snapshot lock-contention events",
    )
    _srs_snapshot_wait_seconds = Histogram(
        "tawasul_srs_snapshot_wait_seconds",
        "Wait time while coalescing SRS snapshot cache misses",
        buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0),
    )
except Exception:  # noqa: BLE001
    _metrics_enabled = False


def _safe_group(group: str) -> str:
    g = str(group or "unknown").strip().lower()
    return g if g else "unknown"


def record_cb_open_check(group: str, is_open: bool) -> None:
    if not _metrics_enabled:
        return
    _circuit_breaker_open_checks_total.labels(
        group=_safe_group(group),
        is_open="true" if is_open else "false",
    ).inc()


def record_cb_event(group: str, event: str) -> None:
    if not _metrics_enabled:
        return
    _circuit_breaker_events_total.labels(
        group=_safe_group(group),
        event=_safe_group(event),
    ).inc()


def record_snapshot_source(source: str) -> None:
    if not _metrics_enabled:
        return
    _srs_snapshot_source_total.labels(source=_safe_group(source)).inc()


def record_snapshot_lock_contention() -> None:
    if not _metrics_enabled:
        return
    _srs_snapshot_lock_contention_total.inc()


def observe_snapshot_wait(seconds: float) -> None:
    if not _metrics_enabled:
        return
    _srs_snapshot_wait_seconds.observe(max(0.0, float(seconds)))

################################################################################
# FILE: backend\services\srs_pdf_html_renderer.py
################################################################################

"""
HTML-to-PDF renderer for SRS exports using Playwright (Node.js).
Falls back to legacy FPDF path when Playwright runtime is unavailable.
"""
from __future__ import annotations

import html
import json
import subprocess
import tempfile
from pathlib import Path
from typing import Any


def render_srs_pdf_html(draft: Any) -> bytes:
    project_root = Path(__file__).resolve().parents[2]
    renderer_script = project_root / "backend" / "tools" / "render_html_to_pdf.cjs"

    if not renderer_script.exists():
        raise RuntimeError(f"Renderer script not found: {renderer_script}")

    html_content = _build_srs_html(draft)

    with tempfile.TemporaryDirectory(prefix="srs-html-export-") as tmp_dir:
        tmp_path = Path(tmp_dir)
        html_path = tmp_path / "srs.html"
        pdf_path = tmp_path / "srs.pdf"

        html_path.write_text(html_content, encoding="utf-8")

        process = subprocess.run(
            [
                "node",
                str(renderer_script),
                str(html_path),
                str(pdf_path),
            ],
            cwd=str(project_root),
            capture_output=True,
            text=True,
            timeout=45,
            check=False,
        )

        if process.returncode != 0:
            stderr = (process.stderr or process.stdout or "unknown renderer error").strip()
            raise RuntimeError(f"HTML-to-PDF renderer failed: {stderr}")

        if not pdf_path.exists():
            raise RuntimeError("HTML-to-PDF renderer did not produce output file")

        return pdf_path.read_bytes()


def _safe(value: Any) -> str:
    if value is None:
        return ""
    if isinstance(value, str):
        return value
    if isinstance(value, dict):
        return json.dumps(value, ensure_ascii=False)
    return str(value)


def _li_items(items: Any) -> str:
    if not isinstance(items, list):
        items = [items]
    cleaned = [html.escape(_safe(item).strip()) for item in items if _safe(item).strip()]
    if not cleaned:
        return "<p class=\"muted\">No items provided.</p>"
    return "<ul>" + "".join(f"<li>{item}</li>" for item in cleaned) + "</ul>"


def _normalize_list(value: Any) -> list[Any]:
    if isinstance(value, list):
        return value
    if value is None:
        return []
    return [value]


def _build_metrics_rows(metrics: list[Any], is_ar: bool) -> str:
    rows = []
    for metric in metrics:
        if not isinstance(metric, dict):
            continue
        label = html.escape(_safe(metric.get("label", "")))
        value = html.escape(_safe(metric.get("value", "")))
        rows.append(f"<tr><td>{label}</td><td>{value}</td></tr>")

    if rows:
        return "".join(rows)

    empty = "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§ÙŠÙŠØ³" if is_ar else "No metrics"
    return f"<tr><td colspan=\"2\" class=\"muted\">{empty}</td></tr>"


def _build_sections_html(sections: list[Any]) -> str:
    blocks: list[str] = []
    for section in sections:
        if not isinstance(section, dict):
            continue
        title = html.escape(_safe(section.get("title", "Section")))
        confidence = html.escape(_safe(section.get("confidence", "")))
        heading = f"{title} <span class=\"confidence\">[{confidence}]</span>" if confidence else title
        body = _li_items(section.get("items", []))
        blocks.append(f"<section class=\"block\"><h3>{heading}</h3>{body}</section>")

    return "".join(blocks)


def _build_activity_diagrams_html(activity_diagrams: list[Any], labels: dict[str, str], is_ar: bool) -> str:
    if not activity_diagrams:
        empty = "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ø§ØµØ±" if is_ar else "No items provided."
        return f"<p class=\"muted\">{empty}</p>"

    blocks: list[str] = []
    for idx, diagram in enumerate(activity_diagrams, 1):
        if not isinstance(diagram, dict):
            continue
        title = html.escape(_safe(diagram.get("title") or f"{labels['activity']} {idx}"))
        body = ""
        mermaid_code = _safe(diagram.get("activity_diagram_mermaid") or "").strip()
        if mermaid_code:
            body += f"<pre class=\"mermaid\">\n{html.escape(mermaid_code)}\n</pre>\n<br>\n"
            
        flow_lines = _normalize_list(diagram.get("activity_diagram"))
        if flow_lines:
            body += _li_items(flow_lines)

        blocks.append(f"<section class=\"block\"><h3>{title}</h3>{body}</section>")

    if not blocks:
        empty = "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ø§ØµØ±" if is_ar else "No items provided."
        return f"<p class=\"muted\">{empty}</p>"
    return "".join(blocks)


def _localized_labels(is_ar: bool) -> dict[str, str]:
    if is_ar:
        return {
        "title": "Ù…ÙˆØ§ØµÙØ§Øª Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ§Øª",
        "subtitle": "ØªÙˆÙ„ÙŠØ¯ Ø°ÙƒÙŠ Ø¹Ø¨Ø± Tawasul AI",
        "lang_label": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
        "project_id": "Ø±Ù‚Ù… Ø§Ù„Ù…Ø´Ø±ÙˆØ¹",
        "version": "Ø§Ù„Ø¥ØµØ¯Ø§Ø±",
        "language": "Ø§Ù„Ù„ØºØ©",
        "status": "Ø§Ù„Ø­Ø§Ù„Ø©",
        "summary": "Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ",
        "summary_empty": "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ø®Øµ",
        "metrics": "Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©",
        "metric_col": "Ø§Ù„Ù…Ù‚ÙŠØ§Ø³",
        "value_col": "Ø§Ù„Ù‚ÙŠÙ…Ø©",
        "activity": "Ù…Ø®Ø·Ø· Ø§Ù„Ù†Ø´Ø§Ø·",
        "questions": "Ø£Ø³Ø¦Ù„Ø© Ù…ÙØªÙˆØ­Ø©",
        "next_steps": "Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©",
        "user_stories": "Ù‚ØµØµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆÙ…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„",
        "user_roles": "Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†",
        "role_col": "Ø§Ù„Ø¯ÙˆØ±",
        "desc_col": "Ø§Ù„ÙˆØµÙ",
        "perm_col": "Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª",
        "as_a": "Ø¨ÙˆØµÙÙŠ",
        "i_want": "Ø£Ø±ÙŠØ¯",
        "so_that": "Ø­ØªÙ‰ Ø£ØªÙ…ÙƒÙ† Ù…Ù†",
        "ac_label": "Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„",
        }

    return {
        "title": "Software Requirements Specification",
        "subtitle": "Generated by Tawasul AI",
        "lang_label": "English",
        "project_id": "Project ID",
        "version": "Version",
        "language": "Language",
        "status": "Status",
        "summary": "Executive Summary",
        "summary_empty": "No summary",
        "metrics": "Key Metrics",
        "metric_col": "Metric",
        "value_col": "Value",
        "activity": "Activity Diagram",
        "questions": "Open Questions",
        "next_steps": "Next Steps",
        "user_stories": "User Stories & Acceptance Criteria",
        "user_roles": "User Roles",
        "role_col": "Role",
        "desc_col": "Description",
        "perm_col": "Permissions",
        "as_a": "As a",
        "i_want": "I want to",
        "so_that": "so that",
        "ac_label": "Acceptance Criteria",
    }


def _build_user_stories_html(user_stories: list[Any], labels: dict[str, str]) -> str:
    if not user_stories:
        return ""
    rows: list[str] = []
    for story in user_stories:
        if not isinstance(story, dict):
            continue
        role = html.escape(_safe(story.get("role", "")))
        action = html.escape(_safe(story.get("action", "")))
        goal = html.escape(_safe(story.get("goal", "")))
        ac_items = story.get("acceptance_criteria")
        ac_html = ""
        if isinstance(ac_items, list) and ac_items:
            items_html = "".join(
                f"<li>âœ“ {html.escape(_safe(ac))}</li>"
                for ac in ac_items
                if _safe(ac).strip()
            )
            if items_html:
                ac_html = (
                    f"<details class=\"ac-details\">"
                    f"<summary>{labels['ac_label']} ({len(ac_items)})</summary>"
                    f"<ul>{items_html}</ul></details>"
                )
        statement = (
            f"<strong>{labels['as_a']}</strong> {role}, "
            f"<strong>{labels['i_want']}</strong> {action}, "
            f"<strong>{labels['so_that']}</strong> {goal}"
        )
        rows.append(
            f"<div class=\"story-card\"><p class=\"story-statement\">{statement}</p>{ac_html}</div>"
        )
    if not rows:
        return ""
    return (
        f"<section class=\"block\"><h3>{labels['user_stories']}</h3>"
        + "".join(rows)
        + "</section>"
    )


def _build_user_roles_html(user_roles: list[Any], labels: dict[str, str]) -> str:
    if not user_roles:
        return ""
    rows: list[str] = []
    for ur in user_roles:
        if not isinstance(ur, dict):
            continue
        role = html.escape(_safe(ur.get("role", "")))
        desc = html.escape(_safe(ur.get("description", "")))
        perms = ur.get("permissions")
        perms_html = ""
        if isinstance(perms, list) and perms:
            perms_html = ", ".join(html.escape(_safe(p)) for p in perms if _safe(p).strip())
        rows.append(f"<tr><td><strong>{role}</strong></td><td>{desc}</td><td>{perms_html}</td></tr>")
    if not rows:
        return ""
    return (
        f"<section class=\"block\"><h3>{labels['user_roles']}</h3>"
        f"<table><thead><tr>"
        f"<th>{labels['role_col']}</th><th>{labels['desc_col']}</th><th>{labels['perm_col']}</th>"
        f"</tr></thead><tbody>"
        + "".join(rows)
        + "</tbody></table></section>"
    )


def _build_srs_html(draft: Any) -> str:
    content = getattr(draft, "content", {}) or {}
    language = getattr(draft, "language", "en")
    is_ar = language == "ar"
    direction = "rtl" if is_ar else "ltr"
    labels = _localized_labels(is_ar)

    summary = html.escape(_safe(content.get("summary", "")))
    metrics = _normalize_list(content.get("metrics"))
    sections = _normalize_list(content.get("sections"))
    questions = _normalize_list(content.get("questions"))
    next_steps = _normalize_list(content.get("next_steps"))
    activity_diagram = _normalize_list(content.get("activity_diagram"))
    activity_diagrams = _normalize_list(content.get("activity_diagrams"))

    if activity_diagrams:
      normalized_multi = []
      for item in activity_diagrams:
        if isinstance(item, dict):
          normalized_multi.append(item)
      activity_diagrams = normalized_multi

    if not activity_diagrams and activity_diagram:
      activity_diagrams = [{
        "title": labels["activity"],
        "activity_diagram": activity_diagram,
      }]

    metrics_rows = _build_metrics_rows(metrics=metrics, is_ar=is_ar)
    sections_html = _build_sections_html(sections)

    activity_html = _build_activity_diagrams_html(activity_diagrams, labels, is_ar)
    questions_html = _li_items(questions)
    next_steps_html = _li_items(next_steps)

    user_stories = _normalize_list(content.get("user_stories"))
    user_roles = _normalize_list(content.get("user_roles"))
    user_stories_html = _build_user_stories_html(user_stories, labels)
    user_roles_html = _build_user_roles_html(user_roles, labels)

    return f"""
<!doctype html>
<html lang=\"{'ar' if is_ar else 'en'}\" dir=\"{direction}\">
<head>
  <meta charset=\"utf-8\" />
  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />
  <style>
    @page {{ size: A4; margin: 18mm 14mm; }}
    body {{ font-family: 'Segoe UI', 'Tahoma', 'Arial', sans-serif; color: #212529; font-size: 12px; line-height: 1.55; }}
    .cover {{ border-bottom: 2px solid #e85d2a; padding-bottom: 12px; margin-bottom: 18px; }}
    h1 {{ margin: 0; font-size: 24px; color: #e85d2a; }}
    h2 {{ margin: 4px 0 0 0; font-size: 13px; color: #6c757d; font-weight: 500; }}
    h3 {{ font-size: 15px; margin: 0 0 8px 0; color: #e85d2a; }}
    .meta {{ margin-top: 10px; font-size: 11px; color: #495057; display: grid; grid-template-columns: 1fr 1fr; gap: 3px 12px; }}
    .block {{ margin: 0 0 14px 0; page-break-inside: avoid; }}
    .summary {{ background: #f8f9fa; border: 1px solid #e9ecef; border-radius: 8px; padding: 10px; white-space: pre-wrap; }}
    table {{ width: 100%; border-collapse: collapse; }}
    th, td {{ border: 1px solid #dee2e6; padding: 7px; text-align: start; vertical-align: top; }}
    th {{ background: #e85d2a; color: #fff; }}
    ul {{ margin: 0; padding-inline-start: 18px; }}
    li {{ margin-bottom: 4px; }}
    .confidence {{ font-size: 11px; color: #6c757d; }}
    .muted {{ color: #6c757d; margin: 0; }}
    .section-title {{ margin-top: 16px; }}
    .story-card {{ background: #f8f9fa; border: 1px solid #e9ecef; border-radius: 6px; padding: 8px 10px; margin-bottom: 8px; }}
    .story-statement {{ margin: 0 0 4px 0; }}
    .ac-details {{ margin-top: 4px; font-size: 11px; }}
    .ac-details summary {{ cursor: pointer; color: #6c757d; }}
  </style>
</head>
<body>
  <section class=\"cover\">
    <h1>{labels['title']}</h1>
    <h2>{labels['subtitle']}</h2>
    <div class=\"meta\">
      <div><strong>{labels['project_id']}:</strong> {html.escape(str(getattr(draft, 'project_id', '')))}</div>
      <div><strong>{labels['version']}:</strong> v{html.escape(str(getattr(draft, 'version', '')))}</div>
      <div><strong>{labels['language']}:</strong> {labels['lang_label']}</div>
      <div><strong>{labels['status']}:</strong> {html.escape(_safe(getattr(draft, 'status', 'draft')))}</div>
    </div>
  </section>

  <section class=\"block\">
    <h3>{labels['summary']}</h3>
    <div class=\"summary\">{summary or labels['summary_empty']}</div>
  </section>

  <section class=\"block\">
    <h3>{labels['metrics']}</h3>
    <table>
      <thead>
        <tr>
          <th>{labels['metric_col']}</th>
          <th>{labels['value_col']}</th>
        </tr>
      </thead>
      <tbody>
        {metrics_rows}
      </tbody>
    </table>
  </section>

  {sections_html}

  {user_stories_html}

  {user_roles_html}

  <section class=\"block section-title\">
    <h3>{labels['activity']}</h3>
    {activity_html}
  </section>

  <section class=\"block\">
    <h3>{labels['questions']}</h3>
    {questions_html}
  </section>

  <section class="block">
    <h3>{labels['next_steps']}</h3>
    {next_steps_html}
  </section>

  <script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
    mermaid.initialize({{ startOnLoad: true, theme: 'default' }});
  </script>
</body>
</html>
"""

################################################################################
# FILE: backend\services\srs_service.py
################################################################################

"""
SRS draft generation and export service.
"""
from __future__ import annotations

import ast
import json
import logging
import re
from datetime import datetime
from typing import Any, Dict, List

from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession

from backend.database.models import Asset, ChatMessage, SRSDraft
from backend.providers.llm.factory import LLMProviderFactory
from backend.services.judging_service import JudgingService
from backend.services.srs_pdf_html_renderer import render_srs_pdf_html
from backend.services.srs_snapshot_cache import SRSSnapshotCache

logger = logging.getLogger(__name__)


class SRSService:
    """Generate and export SRS drafts from project transcripts and chat."""
    def __init__(self):
        self.judging_service = JudgingService()

    async def get_latest_draft(self, db: AsyncSession, project_id: int) -> SRSDraft | None:
        stmt = (
            select(SRSDraft)
            .where(SRSDraft.project_id == project_id)
            .order_by(SRSDraft.version.desc())
            .limit(1)
        )
        result = await db.execute(stmt)
        return result.scalar_one_or_none()

    async def generate_draft(
        self,
        db: AsyncSession,
        project_id: int,
        language: str = "ar",
    ) -> SRSDraft:
        messages = await self._get_project_messages(db, project_id)
        transcript_blocks = await self._get_project_transcripts(db, project_id)
        if not messages and not transcript_blocks:
            raise ValueError("No interview transcripts or chat messages found for this project")

        conversation = self._format_conversation(messages)
        transcripts = self._format_transcripts(transcript_blocks)

        # Quality gate: ensure there is enough content for a meaningful SRS
        self._quality_gate(transcripts, conversation)

        prompt = self._build_prompt(conversation, transcripts, language)

        llm_provider = LLMProviderFactory.create_provider()
        raw = await self._generate_structured_text(
            llm_provider=llm_provider,
            prompt=prompt,
            temperature=0.3,
            max_tokens=3000,
            response_format={"type": "json_object"},
        )
        try:
            content = self._parse_json(raw)
        except ValueError as first_parse_error:
            logger.warning("Initial SRS parse failed for project %s: %s", project_id, first_parse_error)
            repair_prompt = (
                f"{prompt}\n\n"
                "STRICT JSON REPAIR INSTRUCTION:\n"
                "Return ONE valid JSON object only. No markdown, no explanations, no trailing commas."
            )
            repaired_raw = await self._generate_structured_text(
                llm_provider=llm_provider,
                prompt=repair_prompt,
                temperature=0.1,
                max_tokens=3000,
                response_format={"type": "json_object"},
            )
            try:
                content = self._parse_json(repaired_raw)
            except ValueError as second_parse_error:
                logger.error(
                    "SRS parse failed after retry for project %s: %s",
                    project_id,
                    second_parse_error,
                )
                raise ValueError(
                    "Failed to parse SRS JSON after retry. Please retry or switch the AI provider/model."
                ) from second_parse_error

        version = await self._next_version(db, project_id)
        draft = SRSDraft(
            project_id=project_id,
            version=version,
            status="draft",
            language=language,
            content=content,
        )
        db.add(draft)
        await db.flush()

        try:
            refined_result = await self.judging_service.judge_and_refine(
                srs_content=content,
                language=language,
                project_id=project_id,
            )

            refined_srs = refined_result.get("refined_srs") if isinstance(refined_result, dict) else None
            if refined_srs:
                draft.status = "refined"
                draft.content = refined_srs
                logger.info("Draft refined for project %s", project_id)

        except Exception as e:
            logger.error("Failed to refine SRS automatically: %s", e)

        await SRSSnapshotCache.set_from_draft(draft)
        return draft

    @staticmethod
    async def _generate_structured_text(
        *,
        llm_provider: Any,
        prompt: str,
        temperature: float,
        max_tokens: int,
        response_format: Dict[str, Any] | None = None,
    ) -> str:
        try:
            return await llm_provider.generate_text(
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                response_format=response_format,
            )
        except TypeError:
            return await llm_provider.generate_text(
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens,
            )

    def export_pdf(self, draft: SRSDraft) -> bytes:
        try:
            return render_srs_pdf_html(draft)
        except Exception as exc:
            raise RuntimeError(
                "SRS PDF rendering failed. Ensure Node.js and Playwright Chromium are available "
                "(run: npx playwright install chromium)."
            ) from exc

    async def _get_project_messages(self, db: AsyncSession, project_id: int) -> List[ChatMessage]:
        stmt = (
            select(ChatMessage)
            .where(ChatMessage.project_id == project_id)
            .order_by(ChatMessage.created_at.asc())
        )
        result = await db.execute(stmt)
        return list(result.scalars().all())

    async def _get_project_transcripts(self, db: AsyncSession, project_id: int) -> List[Asset]:
        stmt = (
            select(Asset)
            .where(
                Asset.project_id == project_id,
                Asset.extracted_text.isnot(None),
                Asset.extracted_text != "",
            )
            .order_by(Asset.created_at.asc())
        )
        result = await db.execute(stmt)
        return list(result.scalars().all())

    async def _next_version(self, db: AsyncSession, project_id: int) -> int:
        stmt = select(func.max(SRSDraft.version)).where(SRSDraft.project_id == project_id)
        result = await db.execute(stmt)
        current = result.scalar_one_or_none() or 0
        return current + 1

    @staticmethod
    def _format_conversation(messages: List[ChatMessage]) -> str:
        lines = []
        for msg in messages:
            role = msg.role.lower()
            if role == "user":
                prefix = "User"
            elif role == "assistant":
                prefix = "Assistant"
            else:
                prefix = "System"
            lines.append(f"{prefix}: {msg.content}")
        return "\n".join(lines)

    @staticmethod
    def _format_transcripts(transcripts: List[Asset]) -> str:
        blocks: List[str] = []
        for idx, asset in enumerate(transcripts, 1):
            title = asset.original_filename or f"Transcript {idx}"
            text = (asset.extracted_text or "").strip()
            if not text:
                continue
            blocks.append(f"[Transcript {idx} - {title}]\n{text}")
        return "\n\n".join(blocks)

    @staticmethod
    def _build_prompt(conversation: str, transcripts: str, language: str) -> str:
        # Truncate with visible markers so the LLM knows content may be cut
        truncated_transcript = transcripts
        truncated_chat = conversation
        if len(transcripts) > 20000:
            truncated_transcript = (
                transcripts[:20000]
                + "\n\n[TRANSCRIPT TRUNCATED â€” content beyond this point was omitted due to length. "
                "Mark any sections where evidence was insufficient as confidence=low and add a question to the 'questions' array.]"
            )
        if len(conversation) > 8000:
            truncated_chat = conversation[:8000] + "\n\n[CHAT TRUNCATED]"

        evidence_parts = []
        if truncated_transcript.strip():
            evidence_parts.append(f"Interview transcripts:\n{truncated_transcript}")
        if truncated_chat.strip():
            evidence_parts.append(f"Project chat:\n{truncated_chat}")
        evidence = "\n\n".join(evidence_parts)

        if language == "ar":
            return (
                "Ø£Ù†Øª Ù…Ù‡Ù†Ø¯Ø³ Ù…ØªØ·Ù„Ø¨Ø§Øª Ù…Ø¹ØªÙ…Ø¯ Ø¨Ø®Ø¨Ø±Ø© ØªØ²ÙŠØ¯ Ø¹Ù† 20 Ø¹Ø§Ù…Ù‹Ø§ ÙÙŠ ÙƒØªØ§Ø¨Ø© ÙˆØ«Ø§Ø¦Ù‚ SRS Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©.\n\n"
                "## Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø­Ø§Ø³Ù…Ø©\n"
                "ÙƒÙ„ Ù…ØªØ·Ù„Ø¨ ØªÙƒØªØ¨Ù‡ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…Ø³ØªÙ†ÙØ¯Ù‹Ø§ Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹ Ù„ØªØµØ±ÙŠØ­ ØµØ±ÙŠØ­ Ø£Ùˆ Ù…ÙØ¶Ù…ÙŽÙ‘Ù† Ø¨ÙˆØ¶ÙˆØ­ ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚Ø¯ÙŽÙ‘Ù…. "
                "Ù„Ø§ ØªÙØªØ±Ø¶ ÙˆÙ„Ø§ ØªØ®ØªØ±Ø¹ ÙˆÙ„Ø§ ØªÙØ¶ÙŠÙ Ø£Ù†Ù…Ø§Ø·Ù‹Ø§ Ù…Ø¹ÙŠØ§Ø±ÙŠØ© Ù…Ù† Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚. "
                "Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ†Ø§ÙˆÙ„ Ø§Ù„Ù†Øµ Ù…Ù†Ø·Ù‚Ø©Ù‹ Ù…Ø§ØŒ Ø§ØªØ±ÙƒÙ‡Ø§ ÙØ§Ø±ØºØ© ÙˆØ£Ø¶Ù Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù„Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ Ù‚Ø³Ù… questions.\n\n"
                "## Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© (Micro-Details Retention)\n"
                "ÙŠÙÙ…Ù†Ø¹ Ù…Ù†Ø¹Ø§Ù‹ Ø¨Ø§ØªØ§Ù‹ ØªÙ„Ø®ÙŠØµ Ø£Ùˆ ØªØ¬Ø±ÙŠØ¯ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª. Ø¥Ø°Ø§ Ø°ÙƒØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ø³Ù…Ø§Ø¡ Ø£Ù†Ø¸Ù…Ø© ÙØ±Ø¹ÙŠØ© (Ù…Ø«Ù„ POS, HACCP)ØŒ Ø£Ùˆ ØªÙƒØ§Ù…Ù„Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ© (Ù…Ø«Ù„ Talabat)ØŒ Ø£Ùˆ ØªÙØ§ØµÙŠÙ„ Ø¯Ù‚ÙŠÙ‚Ø© (Ù…Ø«Ù„ Ø³Ø¬Ù„ ØªØ¬Ø§Ø±ÙŠØŒ Ø¯ÙØ§Ø¹ Ù…Ø¯Ù†ÙŠ)ØŒ ÙŠØ¬Ø¨ Ù†Ù‚Ù„Ù‡Ø§ Ø­Ø±ÙÙŠØ§Ù‹ ÙˆØ°ÙƒØ±Ù‡Ø§ Ø¨Ø§Ù„Ø§Ø³Ù… Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ© ÙˆØºÙŠØ±Ù‡Ø§. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø¹Ø¨Ø§Ø±Ø§Øª Ø¹Ø§Ù…Ø© ÙˆÙ…Ø¨Ù‡Ù…Ø© Ù…Ø«Ù„ 'Ø¯Ø¹Ù… Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©'.\n\n"
                "## ØªØ¹Ø±ÙŠÙ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©\n"
                "- \"high\": Ø°ÙÙƒØ± ØµØ±Ø§Ø­Ø©Ù‹ ÙÙŠ Ù…ÙˆØ¶Ø¹ÙŠÙ† Ø£Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ù†Øµ.\n"
                "- \"medium\": Ø°ÙÙƒØ± Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©ØŒ Ø£Ùˆ ÙŠÙØ³ØªØ´ÙÙ‘ Ø¨ÙˆØ¶ÙˆØ­ Ù…Ù† ØªØµØ±ÙŠØ­Ø§Øª Ø£Ø®Ø±Ù‰.\n"
                "- \"low\": Ù…Ø³ØªÙ†ØªØ¬ Ù…Ù† Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø¬Ø§Ù„Ø› ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹ ÙÙŠ Ø§Ù„Ù†Øµ.\n\n"
                "## Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ© Ø§Ù„Ø³Ø¨Ø¹Ø© (Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø¨Ø§Ù„Ø¶Ø¨Ø·)\n"
                "1. Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© ÙˆØ£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø´Ø±ÙˆØ¹\n"
                "2. Ø£ØµØ­Ø§Ø¨ Ø§Ù„Ù…ØµÙ„Ø­Ø© ÙˆØ£Ø¯ÙˆØ§Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†\n"
                "3. Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ©\n"
                "4. Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª ØºÙŠØ± Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ©\n"
                "5. Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù†Ø¸Ø§Ù…\n"
                "6. Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Ø·Ø§Ù‚\n"
                "7. Ø£Ø³Ø¦Ù„Ø© Ù…ÙØªÙˆØ­Ø© ÙˆØ§ÙØªØ±Ø§Ø¶Ø§Øª\n\n"
                "## Ù‚ØµØµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆÙ…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„\n"
                "Ù„ÙƒÙ„ Ù…ÙŠØ²Ø© Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ø°ÙƒÙˆØ±Ø© Ù…Ù† Ù‚ÙØ¨Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ¬Ø¨ ØªÙˆÙ„ÙŠØ¯:\n"
                "- Ù‚ØµØ© Ù…Ø³ØªØ®Ø¯Ù… Ø¨ØµÙŠØºØ©: 'Ø¨ÙˆØµÙÙŠ [Ù†ÙˆØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…]ØŒ Ø£Ø±ÙŠØ¯ [Ø§Ù„ÙØ¹Ù„]ØŒ Ø­ØªÙ‰ Ø£Ø³ØªØ·ÙŠØ¹ [Ø§Ù„Ù‡Ø¯Ù]'.\n"
                "- Ù…Ø¹ÙŠØ§Ø± Ù‚Ø¨ÙˆÙ„ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ù‚ÙŠØ§Ø³ Ø¨ØµÙŠØºØ©: 'ÙŠÙØ¹ØªØ¨Ø± Ø§Ù„Ø´Ø±Ø· Ù…Ø­Ù‚Ù‚Ù‹Ø§ Ø¹Ù†Ø¯Ù…Ø§ [Ø­Ø§Ù„Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚]'.\n\n"
                "## Ù‚ÙˆØ§Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª\n"
                "- ÙƒÙ„ Ø¹Ù†ØµØ± ÙÙŠ sections.items ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…ØªØ·Ù„Ø¨Ù‹Ø§ Ø°Ø±ÙŠÙ‹Ø§ ÙƒØ§Ù…Ù„Ø§Ù‹ Ø¨ØµÙŠØºØ©: 'ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙˆÙØ± Ø§Ù„Ù†Ø¸Ø§Ù…...'.\n"
                "- Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ ØªØ¹ÙƒØ³ ÙÙ‚Ø· Ù…Ø§ Ø°ÙƒØ±Ù‡ Ø§Ù„Ø¹Ù…ÙŠÙ„ ØµØ±Ø§Ø­Ø©Ù‹. Ù„Ø§ ØªØ®ØªØ±Ø¹ Ù…Ø¤Ø´Ø±Ø§Øª Ø£Ø¯Ø§Ø¡.\n"
                "- Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…ÙˆØ¬Ù‡Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„ØŒ Ù…Ø­Ø¯Ø¯Ø©ØŒ ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© â€” ÙˆÙ„ÙŠØ³Øª Ø¹Ø§Ù…Ø©.\n"
                "- next_steps Ù‡ÙŠ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ù…ÙØ±ØªÙŽÙ‘Ø¨Ø© Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„ÙØ±ÙŠÙ‚ Ø§Ù„ØªØ·ÙˆÙŠØ±ÙŠ.\n"
                "- activity_diagrams ÙŠØ¬Ø¨ Ø£Ù† ØªØ­ØªÙˆÙŠ Ù…Ø®Ø·Ø· Ù†Ø´Ø§Ø· Ù„ÙƒÙ„ Ù†Ø´Ø§Ø·/ØªØ¯ÙÙ‚ Ø±Ø¦ÙŠØ³ÙŠ ÙÙŠ Ø§Ù„Ù…Ù†ØªØ¬.\n\n"
                "## Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„ØµØ§Ø±Ù…Ø©\n"
                "1) Ø£Ø®Ø±Ø¬ JSON ØµØ§Ù„Ø­ ÙÙ‚Ø· â€” Ø¨Ø¯ÙˆÙ† markdownØŒ Ø¨Ø¯ÙˆÙ† Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ.\n"
                "2) Ù„Ø§ ØªØ¶Ù Ø£Ù‚Ø³Ø§Ù…Ù‹Ø§ Ø®Ø§Ø±Ø¬ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø³Ø¨Ø¹Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.\n"
                "3) Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù‚Ø³Ù… Ø¨Ø¯ÙˆÙ† Ø£Ø¯Ù„Ø© Ù…Ù† Ø§Ù„Ù†ØµØŒ Ø§Ø¬Ø¹Ù„ items Ù‚Ø§Ø¦Ù…Ø© ÙØ§Ø±ØºØ© [] ÙˆØ£Ø¶Ù Ø³Ø¤Ø§Ù„Ø§Ù‹ ÙÙŠ questions.\n"
                "4) ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:\n"
                '   {"summary": "...", "metrics": [{"label": "...", "value": "..."}], '
                '"sections": [{"title": "Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© ÙˆØ£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø´Ø±ÙˆØ¹", "confidence": "high|medium|low", "items": ["ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙˆÙØ± Ø§Ù„Ù†Ø¸Ø§Ù…..."]}], '
                '"user_stories": [{"role": "Ù…Ø³Ø¤ÙˆÙ„", "action": "Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†", "goal": "Ø¶Ù…Ø§Ù† Ø§Ù„Ø£Ù…Ø§Ù†", "acceptance_criteria": ["ÙŠÙØ¹ØªØ¨Ø± Ø§Ù„Ø´Ø±Ø· Ù…Ø­Ù‚Ù‚Ù‹Ø§ Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„..."]}], '
                '"user_roles": [{"role": "Ù…Ø³Ø¤ÙˆÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…", "description": "ÙŠØ¯ÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆØ§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª", "permissions": ["Ø¥Ø¶Ø§ÙØ© Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†", "ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø£Ø¯ÙˆØ§Ø±"]}], '
                '"activity_diagram": ["Ø¨Ø¯Ø§ÙŠØ© -> Ø¯Ø®ÙˆÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… -> ØªØ­Ù‚Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù… -> Ø¹Ø±Ø¶ Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…"], '
                '"activity_diagram_mermaid": "flowchart TD\\n  A[Ø¨Ø¯Ø§ÙŠØ©] --> B[Ø¯Ø®ÙˆÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…] --> C[ØªØ­Ù‚Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù…] --> D[Ø¹Ø±Ø¶ Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…]", '
                '"activity_diagrams": [{"title": "ØªØ¯ÙÙ‚ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„", "activity_diagram": ["Ø¨Ø¯Ø§ÙŠØ© -> Ø¥Ø¯Ø®Ø§Ù„ Ø¨ÙŠØ§Ù†Ø§Øª -> ØªØ­Ù‚Ù‚ -> Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…"], "activity_diagram_mermaid": "flowchart TD\\n  A[Ø¨Ø¯Ø§ÙŠØ©] --> B[Ø¥Ø¯Ø®Ø§Ù„ Ø¨ÙŠØ§Ù†Ø§Øª] --> C[ØªØ­Ù‚Ù‚] --> D[Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…]"}], '
                '"questions": ["..."], "next_steps": ["..."]}\n\n'
                f"Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹:\n{evidence}\n\n"
                "Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ (JSON ÙÙ‚Ø·):"
            )

        return (
            "You are a certified Requirements Engineer with 20+ years writing professional SRS documents.\n\n"
            "## CRITICAL GROUNDING RULE\n"
            "Every requirement you write MUST be directly traceable to an explicit or strongly implied "
            "statement in the provided transcript. Do NOT assume, invent, or pad with standard industry "
            "patterns. If a domain area was not discussed, leave its section items empty and add a "
            "client-facing question to the 'questions' array flagging the gap.\n\n"
            "## Micro-Details Retention Rule (CRITICAL)\n"
            "It is strictly forbidden to over-summarize or abstract requirements. If the user mentions specific subsystems (e.g., POS, HACCP), third-party integrations (e.g., Talabat), or exact details (e.g., tax card, fire safety), you MUST retain them literally and mention them by their specific names within the requirements. Do not use generic or vague abstractions like 'support legal requirements'.\n\n"
            "## Confidence Level Definition\n"
            "- \"high\": explicitly stated in 2+ distinct places in the transcript.\n"
            "- \"medium\": stated once, or clearly implied by other statements.\n"
            "- \"low\": inferred from general domain knowledge; NOT directly in transcript.\n\n"
            "## Required Sections â€” use EXACTLY these 7 titles:\n"
            "1. Project Overview & Objectives\n"
            "2. Stakeholders & User Roles\n"
            "3. Functional Requirements\n"
            "4. Non-Functional Requirements\n"
            "5. System Constraints\n"
            "6. Out of Scope\n"
            "7. Open Questions & Assumptions\n\n"
            "## User Stories & Acceptance Criteria\n"
            "For every major feature mentioned by the client, you MUST generate:\n"
            "- A user story in the format: 'As a [user type], I want [action], so that [goal]'.\n"
            "- At least one measurable acceptance criterion: 'The feature is complete when [verifiable condition]'.\n\n"
            "## Requirement Phrasing Rules\n"
            "- Every item in sections.items must be an atomic, complete requirement in 'The system SHALL...' format.\n"
            "- Metrics must reflect ONLY what the client explicitly mentioned. Do not invent KPIs.\n"
            "- Questions must be client-facing, specific, and answerable â€” not generic.\n"
            "- next_steps must be prioritized action items for the development team.\n"
            "- activity_diagrams must include one activity diagram for each major product activity/flow.\n\n"
            "## Strict Output Rules\n"
            "1) Return valid JSON only â€” no markdown fences, no preamble.\n"
            "2) Do NOT add sections outside the 7 listed above.\n"
            "3) If a section has no evidence in the transcript, set its items to [] and add a question.\n"
            "4) Output format:\n"
            '   {"summary": "...", "metrics": [{"label": "...", "value": "..."}], '
            '"sections": [{"title": "Project Overview & Objectives", "confidence": "high|medium|low", "items": ["The system SHALL..."]}], '
            '"user_stories": [{"role": "Admin", "action": "manage users", "goal": "ensure security", "acceptance_criteria": ["The feature is complete when the admin can add/remove users and role changes are audit-logged"]}], '
            '"user_roles": [{"role": "System Administrator", "description": "Manages users and permissions", "permissions": ["add users", "edit roles", "view audit log"]}], '
            '"activity_diagram": ["Start -> User signs in -> System validates credentials -> Dashboard"], '
            '"activity_diagram_mermaid": "flowchart TD\\n  A[Start] --> B[User signs in] --> C[System validates credentials] --> D[Dashboard]", '
            '"activity_diagrams": [{"title": "Login Flow", "activity_diagram": ["Start -> Enter credentials -> Validate -> Dashboard"], "activity_diagram_mermaid": "flowchart TD\\n  A[Start] --> B[Enter credentials] --> C[Validate] --> D[Dashboard]"}], '
            '"questions": ["..."], "next_steps": ["..."]}\n\n'
            f"Project sources:\n{evidence}\n\n"
            "Output (JSON only):"
        )

    @staticmethod
    def _quality_gate(transcripts: str, conversation: str) -> None:
        """Raise ValueError if combined content is too thin to produce a meaningful SRS."""
        combined = (transcripts + " " + conversation).strip()
        word_count = len(combined.split())
        if word_count < 80:
            raise ValueError(
                f"Insufficient content for SRS generation: {word_count} words found. "
                "A minimum of 80 words is required across the transcript and chat history. "
                "Please complete more interview turns before generating the SRS."
            )

    @staticmethod
    def _parse_json(raw: str) -> Dict[str, Any]:
        cleaned = (raw or "").strip()
        if not cleaned:
            raise ValueError("Failed to parse SRS JSON: empty response")

        # Remove markdown code fences if present.
        cleaned = re.sub(r"^```(?:json)?\s*", "", cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r"\s*```$", "", cleaned).strip()

        parsed = SRSService._try_parse_json_clean(cleaned)
        if parsed is None:
            parsed = SRSService._try_decode_embedded_json(cleaned)
        if parsed is None:
            parsed = SRSService._try_literal_eval_fallback(cleaned)
        if parsed is None:
            extracted = SRSService._extract_balanced_json_object(cleaned)
            if extracted:
                parsed = SRSService._try_parse_json_clean(extracted)
                if parsed is None:
                    parsed = SRSService._try_literal_eval_fallback(extracted)

        if not isinstance(parsed, dict):
            raise ValueError("Failed to parse SRS JSON")

        return SRSService._normalize_srs_content(parsed)

    @staticmethod
    def _try_parse_json_clean(cleaned: str) -> Any:
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            return None

    @staticmethod
    def _try_decode_embedded_json(cleaned: str) -> Any:
        first_brace = cleaned.find("{")
        if first_brace == -1:
            return None

        decoder = json.JSONDecoder()
        try:
            parsed, _ = decoder.raw_decode(cleaned[first_brace:])
            return parsed
        except json.JSONDecodeError:
            return None

    @staticmethod
    def _try_literal_eval_fallback(cleaned: str) -> Any:
        first_brace = cleaned.find("{")
        last_brace = cleaned.rfind("}")
        if first_brace == -1 or last_brace <= first_brace:
            return None

        candidate = cleaned[first_brace : last_brace + 1]
        try:
            return ast.literal_eval(candidate)
        except (ValueError, SyntaxError):
            return None

    @staticmethod
    def _extract_balanced_json_object(text: str) -> str | None:
        start = text.find("{")
        if start == -1:
            return None

        depth = 0
        in_string = False
        escaped = False
        for idx in range(start, len(text)):
            ch = text[idx]
            if escaped:
                escaped = False
                continue
            if ch == "\\":
                escaped = True
                continue
            if ch == '"':
                in_string = not in_string
                continue
            if in_string:
                continue
            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    return text[start : idx + 1]
        return None

    @staticmethod
    def _normalize_srs_content(content: Dict[str, Any]) -> Dict[str, Any]:
        """Guarantee the stored SRS content has the expected top-level shape."""
        summary = content.get("summary")
        if not isinstance(summary, str):
            summary = "" if summary is None else str(summary)

        metrics = content.get("metrics")
        if not isinstance(metrics, list):
            metrics = []

        sections = content.get("sections")
        if not isinstance(sections, list):
            sections = []

        questions = content.get("questions")
        if not isinstance(questions, list):
            questions = []

        activity_diagram = content.get("activity_diagram")
        if not isinstance(activity_diagram, list):
            activity_diagram = []
        activity_diagram = [str(line).strip() for line in activity_diagram if str(line or "").strip()]

        activity_diagram_mermaid = content.get("activity_diagram_mermaid")
        if not isinstance(activity_diagram_mermaid, str):
            activity_diagram_mermaid = ""
        activity_diagram_mermaid = activity_diagram_mermaid.strip()
        if not activity_diagram_mermaid:
            activity_diagram_mermaid = SRSService._build_mermaid_from_flow_lines(activity_diagram)

        activity_diagrams = SRSService._normalize_activity_diagrams(
            content.get("activity_diagrams"),
            fallback_activity_diagram=activity_diagram,
            fallback_mermaid=activity_diagram_mermaid,
        )

        next_steps = content.get("next_steps")
        if not isinstance(next_steps, list):
            next_steps = []

        # Normalize user_stories
        raw_user_stories = content.get("user_stories")
        user_stories: List[Dict[str, Any]] = []
        if isinstance(raw_user_stories, list):
            for story in raw_user_stories:
                if not isinstance(story, dict):
                    continue
                ac = story.get("acceptance_criteria")
                if not isinstance(ac, list):
                    ac = [str(ac).strip()] if ac else []
                else:
                    ac = [str(c).strip() for c in ac if str(c or "").strip()]
                role = str(story.get("role") or "").strip()
                action = str(story.get("action") or "").strip()
                goal = str(story.get("goal") or "").strip()
                if role or action or goal:
                    user_stories.append({
                        "role": role,
                        "action": action,
                        "goal": goal,
                        "acceptance_criteria": ac,
                    })

        # Normalize user_roles
        raw_user_roles = content.get("user_roles")
        user_roles: List[Dict[str, Any]] = []
        if isinstance(raw_user_roles, list):
            for ur in raw_user_roles:
                if not isinstance(ur, dict):
                    continue
                perms = ur.get("permissions")
                if not isinstance(perms, list):
                    perms = []
                else:
                    perms = [str(p).strip() for p in perms if str(p or "").strip()]
                role_name = str(ur.get("role") or "").strip()
                desc = str(ur.get("description") or "").strip()
                if role_name:
                    user_roles.append({
                        "role": role_name,
                        "description": desc,
                        "permissions": perms,
                    })

        return {
            "summary": summary,
            "metrics": metrics,
            "sections": sections,
            "user_stories": user_stories,
            "user_roles": user_roles,
            "activity_diagram": activity_diagram,
            "activity_diagram_mermaid": activity_diagram_mermaid,
            "activity_diagrams": activity_diagrams,
            "questions": questions,
            "next_steps": next_steps,
        }

    @staticmethod
    def _normalize_activity_diagrams(
        value: Any,
        fallback_activity_diagram: List[str],
        fallback_mermaid: str,
    ) -> List[Dict[str, Any]]:
        diagrams: List[Dict[str, Any]] = []

        if isinstance(value, list):
            for idx, raw in enumerate(value):
                normalized = SRSService._normalize_single_activity_diagram(raw, idx)
                if normalized:
                    diagrams.append(normalized)

        if diagrams:
            return diagrams

        if fallback_activity_diagram or fallback_mermaid:
            return [
                {
                    "title": "Main Activity Flow",
                    "activity_diagram": fallback_activity_diagram,
                    "activity_diagram_mermaid": fallback_mermaid,
                }
            ]

        return []

    @staticmethod
    def _normalize_single_activity_diagram(raw: Any, idx: int) -> Optional[Dict[str, Any]]:
        if not isinstance(raw, dict):
            return None

        title = str(raw.get("title") or f"Activity {idx + 1}").strip()

        lines_raw = raw.get("activity_diagram")
        if not isinstance(lines_raw, list):
            lines_raw = []
        lines = [str(line).strip() for line in lines_raw if str(line or "").strip()]

        mermaid = raw.get("activity_diagram_mermaid")
        if not isinstance(mermaid, str):
            mermaid = ""
        mermaid = mermaid.strip()
        if not mermaid:
            mermaid = SRSService._build_mermaid_from_flow_lines(lines)

        if not lines and not mermaid:
            return None

        return {
            "title": title,
            "activity_diagram": lines,
            "activity_diagram_mermaid": mermaid,
        }

    @staticmethod
    def _build_mermaid_from_flow_lines(lines: List[str]) -> str:
        steps: List[str] = []
        for raw_line in lines:
            text = str(raw_line or "").strip()
            if not text:
                continue
            parts = [segment.strip() for segment in text.split("->") if segment.strip()]
            if not parts:
                continue
            for part in parts:
                if part not in steps:
                    steps.append(part)

        if len(steps) < 2:
            return ""

        code_lines = ["flowchart TD"]
        for idx, label in enumerate(steps):
            node = f"N{idx}"
            safe_label = str(label).replace('"', "'")
            code_lines.append(f"  {node}[\"{safe_label}\"]")
        for idx in range(len(steps) - 1):
            code_lines.append(f"  N{idx} --> N{idx + 1}")
        return "\n".join(code_lines)

    @staticmethod
    def _format_dt(value: datetime | None) -> str:
        if not value:
            return ""
        return value.strftime("%Y-%m-%d %H:%M")

################################################################################
# FILE: backend\services\srs_snapshot_cache.py
################################################################################

"""
In-memory cache for latest per-project SRS snapshot.
"""
from __future__ import annotations

import asyncio
from copy import deepcopy
from typing import Any, Dict, Optional

from sqlalchemy import func, select
from sqlalchemy.ext.asyncio import AsyncSession

from backend.database.models import SRSDraft
from backend.services.runtime_metrics import record_snapshot_source


class SRSSnapshotCache:
    """In-memory snapshot cache keyed by project_id for latest SRS snapshot."""

    _cache: dict[int, Dict[str, Any]] = {}
    _locks: dict[int, asyncio.Lock] = {}

    @staticmethod
    async def _latest_db_version(db: AsyncSession, project_key: int) -> int | None:
        version_stmt = select(func.max(SRSDraft.version)).where(SRSDraft.project_id == project_key)
        version_result = await db.execute(version_stmt)
        latest_version = version_result.scalar_one_or_none()
        if latest_version is None:
            return None
        try:
            return int(latest_version)
        except (TypeError, ValueError):
            return None

    @staticmethod
    def _snapshot_matches_version(snapshot: Dict[str, Any] | None, version: int) -> bool:
        if not snapshot:
            return False
        try:
            return int(snapshot.get("version") or 0) == int(version)
        except (TypeError, ValueError):
            return False

    @classmethod
    def _serialize_draft(cls, draft: SRSDraft) -> Dict[str, Any]:
        content = draft.content if isinstance(draft.content, dict) else {}
        return {
            "version": int(draft.version or 1),
            "status": str(draft.status or "draft"),
            "language": str(draft.language or "ar"),
            "content": deepcopy(content),
        }

    @classmethod
    async def _load_latest_from_db(cls, db: AsyncSession, project_key: int) -> Dict[str, Any] | None:
        latest_stmt = (
            select(SRSDraft)
            .where(SRSDraft.project_id == project_key)
            .order_by(SRSDraft.version.desc(), SRSDraft.created_at.desc())
            .limit(1)
        )
        latest_result = await db.execute(latest_stmt)
        draft = latest_result.scalar_one_or_none()
        if not draft:
            await cls.invalidate(project_key)
            record_snapshot_source("none")
            return None

        snapshot = cls._serialize_draft(draft)
        cls._cache[project_key] = deepcopy(snapshot)
        record_snapshot_source("db")
        return deepcopy(snapshot)

    @classmethod
    async def set_from_draft(cls, draft: SRSDraft) -> None:
        if not draft or draft.project_id is None:
            return
        project_id = int(draft.project_id)
        cls._cache[project_id] = cls._serialize_draft(draft)

    @classmethod
    async def invalidate(cls, project_id: int) -> None:
        project_key = int(project_id)
        cls._cache.pop(project_key, None)

    @classmethod
    async def get_latest_snapshot(
        cls,
        db: AsyncSession,
        project_id: int,
    ) -> Optional[Dict[str, Any]]:
        project_key = int(project_id)
        cached_snapshot = cls._cache.get(project_key)

        latest_version_int = await cls._latest_db_version(db, project_key)
        if latest_version_int is None:
            await cls.invalidate(project_key)
            record_snapshot_source("none")
            return None

        if cls._snapshot_matches_version(cached_snapshot, latest_version_int):
            record_snapshot_source("cache")
            return deepcopy(cached_snapshot)

        lock = cls._locks.setdefault(project_key, asyncio.Lock())
        async with lock:
            cached_snapshot = cls._cache.get(project_key)
            if cls._snapshot_matches_version(cached_snapshot, latest_version_int):
                record_snapshot_source("cache")
                return deepcopy(cached_snapshot)
            return await cls._load_latest_from_db(db, project_key)

################################################################################
# FILE: backend\services\stt_service.py
################################################################################

"""
Speech-to-Text service providers (Groq Whisper + OpenAI Whisper fallback).
"""
from __future__ import annotations

import mimetypes
import os
import tempfile
import random
import time
from dataclasses import dataclass
from typing import Any, Dict, List

import httpx


ALLOWED_EXTENSIONS = {"wav", "mp3", "mp4", "mpeg", "mpga", "m4a", "webm", "ogg"}
SUPPORTED_LANGUAGES: Dict[str, str] = {
    "auto": "Auto-detect",
    "ar": "Arabic",
    "en": "English",
    "fr": "French",
    "de": "German",
    "es": "Spanish",
    "it": "Italian",
    "pt": "Portuguese",
    "ru": "Russian",
    "zh": "Chinese",
    "ja": "Japanese",
    "ko": "Korean",
    "tr": "Turkish",
    "nl": "Dutch",
    "pl": "Polish",
    "sv": "Swedish",
    "hi": "Hindi",
    "ur": "Urdu",
    "fa": "Persian",
}


def is_allowed_file(filename: str) -> bool:
    return "." in filename and filename.rsplit(".", 1)[1].lower() in ALLOWED_EXTENSIONS


def _retry_request(fn, max_retries: int = 3, backoff: float = 2.0) -> httpx.Response:
    last_exc = None
    for attempt in range(max_retries):
        try:
            return fn()
        except httpx.HTTPStatusError as exc:
            last_exc = exc
            code = exc.response.status_code
            if code == 429 or code >= 500:
                retry_after = exc.response.headers.get("Retry-After") if exc.response is not None else None
                if retry_after and str(retry_after).strip().isdigit():
                    wait = float(str(retry_after).strip())
                else:
                    wait = backoff * (2 ** attempt)
                wait = min(wait, 8.0) + random.uniform(0, 0.25)
                time.sleep(wait)
                continue
            raise
    raise last_exc


@dataclass
class GroqWhisperProvider:
    api_key: str
    base_url: str = "https://api.groq.com/openai/v1"

    name: str = "groq"
    display_name: str = "Groq Whisper"
    max_file_bytes: int = 24 * 1024 * 1024
    chunk_duration_s: int = 10 * 60

    @staticmethod
    def _compute_quality(body: Dict[str, Any], text: str) -> Dict[str, Any]:
        segments = body.get("segments") if isinstance(body.get("segments"), list) else []
        avg_logprob_values: List[float] = []
        no_speech_values: List[float] = []

        for segment in segments:
            if not isinstance(segment, dict):
                continue
            avg_lp = segment.get("avg_logprob")
            no_speech = segment.get("no_speech_prob")
            if isinstance(avg_lp, (int, float)):
                avg_logprob_values.append(float(avg_lp))
            if isinstance(no_speech, (int, float)):
                no_speech_values.append(float(no_speech))

        avg_logprob = (sum(avg_logprob_values) / len(avg_logprob_values)) if avg_logprob_values else None
        avg_no_speech = (sum(no_speech_values) / len(no_speech_values)) if no_speech_values else None

        words = [w for w in str(text or "").strip().split() if w.strip()]
        word_count = len(words)
        suspicious_tokens = {"???", "[inaudible]", "inaudible", "...", "ØºÙŠØ±_Ù…ÙÙ‡ÙˆÙ…", "ØºÙŠØ±", "Ù…ÙÙ‡ÙˆÙ…"}
        suspicious_count = sum(1 for w in words if w.lower() in suspicious_tokens)

        logprob_component = 0.6
        if avg_logprob is not None:
            logprob_component = max(0.0, min(1.0, (avg_logprob + 1.6) / 1.6))

        no_speech_component = 0.8
        if avg_no_speech is not None:
            no_speech_component = 1.0 - max(0.0, min(1.0, avg_no_speech))

        confidence = round((0.7 * logprob_component) + (0.3 * no_speech_component), 3)
        if suspicious_count > 0:
            confidence = round(max(0.0, confidence - min(0.3, suspicious_count * 0.08)), 3)
        if word_count < 4:
            confidence = round(max(0.0, confidence - 0.15), 3)

        warnings: List[str] = []
        if word_count < 4:
            warnings.append("Transcript is very short and may be incomplete.")
        if suspicious_count > 0:
            warnings.append("Transcript contains uncertain tokens that may indicate recognition errors.")
        if avg_no_speech is not None and avg_no_speech > 0.45:
            warnings.append("High non-speech probability detected; audio may be noisy or unclear.")
        if avg_logprob is not None and avg_logprob < -1.1:
            warnings.append("Low speech confidence from model segments.")

        requires_confirmation = confidence < 0.72 or bool(warnings)
        return {
            "confidence": confidence,
            "requires_confirmation": requires_confirmation,
            "warnings": warnings[:3],
            "stats": {
                "word_count": word_count,
                "avg_logprob": avg_logprob,
                "avg_no_speech_prob": avg_no_speech,
            },
        }

    @property
    def api_url(self) -> str:
        return f"{self.base_url.rstrip('/')}/audio/transcriptions"

    def transcribe(self, file_path: str, language: str = "auto") -> dict:
        if not self.api_key:
            raise RuntimeError("Groq is not configured. Set GROQ_API_KEY in .env")

        filename = os.path.basename(file_path)
        mime = self._mime_type(file_path)

        data = {
            "model": "whisper-large-v3",
            "response_format": "verbose_json",
        }
        if language != "auto":
            data["language"] = language

        headers = {"Authorization": f"Bearer {self.api_key}"}

        with open(file_path, "rb") as file_handle:
            def do_request() -> httpx.Response:
                file_handle.seek(0)
                resp = httpx.post(
                    self.api_url,
                    headers=headers,
                    data=data,
                    files={"file": (filename, file_handle, mime)},
                    timeout=120,
                )
                resp.raise_for_status()
                return resp

            resp = _retry_request(do_request)

        body = resp.json()
        detected = body.get("language", language)
        text = body.get("text", "")
        quality = self._compute_quality(body=body, text=text)
        return {"text": text, "language": detected, "quality": quality}

    def transcribe_auto(self, file_path: str, language: str = "auto") -> dict:
        file_size = os.path.getsize(file_path)
        if file_size <= self.max_file_bytes:
            return self.transcribe(file_path, language)

        return self._chunked_transcribe(file_path, language)

    def _chunked_transcribe(self, file_path: str, language: str) -> dict:
        try:
            from pydub import AudioSegment
        except ImportError as exc:
            raise RuntimeError(
                "File is too large for Groq Whisper. Install pydub to enable chunking: "
                "uv pip install pydub (ffmpeg must be in PATH)."
            ) from exc

        audio = AudioSegment.from_file(file_path)
        chunk_ms = self.chunk_duration_s * 1000
        total_chunks = max(1, (len(audio) + chunk_ms - 1) // chunk_ms)

        all_text: list[str] = []
        detected_lang = language
        quality_scores: List[float] = []
        quality_warnings: List[str] = []
        temp_files: list[str] = []

        try:
            for i, start in enumerate(range(0, len(audio), chunk_ms)):
                chunk = audio[start : start + chunk_ms]

                tmp = tempfile.NamedTemporaryFile(suffix=".mp3", delete=False)
                tmp.close()
                chunk.export(tmp.name, format="mp3", bitrate="128k")
                temp_files.append(tmp.name)

                result = self.transcribe(tmp.name, language)
                all_text.append(result.get("text", ""))
                quality = result.get("quality") if isinstance(result.get("quality"), dict) else {}
                score = quality.get("confidence")
                if isinstance(score, (int, float)):
                    quality_scores.append(float(score))
                warnings = quality.get("warnings") if isinstance(quality.get("warnings"), list) else []
                for warning in warnings:
                    if warning not in quality_warnings:
                        quality_warnings.append(str(warning))

                if i == 0 and result.get("language") and result["language"] != language:
                    detected_lang = result["language"]
                    if language == "auto":
                        language = detected_lang
        finally:
            for temp_file in temp_files:
                try:
                    os.remove(temp_file)
                except OSError:
                    pass

        combined = " ".join(t for t in all_text if t)
        confidence = round(sum(quality_scores) / len(quality_scores), 3) if quality_scores else 0.65
        quality = {
            "confidence": confidence,
            "requires_confirmation": confidence < 0.72 or bool(quality_warnings),
            "warnings": quality_warnings[:3],
            "stats": {
                "chunks": total_chunks,
                "word_count": len([w for w in combined.split() if w.strip()]),
            },
        }
        return {"text": combined, "language": detected_lang, "quality": quality}

    @staticmethod
    def _mime_type(file_path: str) -> str:
        mime_type, _ = mimetypes.guess_type(file_path)
        return mime_type or "audio/mpeg"


@dataclass
class OpenAIWhisperProvider:
    api_key: str
    base_url: str = "https://api.openai.com/v1"
    model_name: str = "whisper-1"

    name: str = "openai"
    display_name: str = "OpenAI Whisper"
    max_file_bytes: int = 24 * 1024 * 1024

    @property
    def api_url(self) -> str:
        return f"{self.base_url.rstrip('/')}/audio/transcriptions"

    def transcribe_auto(self, file_path: str, language: str = "auto") -> dict:
        return self.transcribe(file_path=file_path, language=language)

    def transcribe(self, file_path: str, language: str = "auto") -> dict:
        if not self.api_key:
            raise RuntimeError("OpenAI STT is not configured. Set OPENAI_API_KEY in .env")

        filename = os.path.basename(file_path)
        mime = GroqWhisperProvider._mime_type(file_path)
        data = {
            "model": self.model_name,
            "response_format": "verbose_json",
        }
        if language != "auto":
            data["language"] = language

        headers = {"Authorization": f"Bearer {self.api_key}"}

        with open(file_path, "rb") as file_handle:
            def do_request() -> httpx.Response:
                file_handle.seek(0)
                resp = httpx.post(
                    self.api_url,
                    headers=headers,
                    data=data,
                    files={"file": (filename, file_handle, mime)},
                    timeout=120,
                )
                resp.raise_for_status()
                return resp

            resp = _retry_request(do_request)

        body = resp.json()
        detected = body.get("language", language)
        text = body.get("text", "")
        quality = GroqWhisperProvider._compute_quality(body=body, text=text)
        return {"text": text, "language": detected, "quality": quality}

################################################################################
# FILE: backend\services\telemetry_service.py
################################################################################

"""
Telemetry and evaluation metrics for interview agent quality.
"""
from __future__ import annotations

from typing import Any, Dict
import logging

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from backend.database.models import Project

logger = logging.getLogger(__name__)


class TelemetryService:
    KEY = "agent_telemetry"
    _COUNTER_FIELDS = (
        "interview_turns",
        "ambiguity_detected_count",
        "contradiction_detected_count",
        "ambiguity_cases",
        "ambiguity_resolved",
        "suggestion_offered_turns",
        "suggestion_accepted_count",
    )

    @classmethod
    async def record_interview_turn(
        cls,
        db: AsyncSession,
        project_id: int,
        result: Dict[str, Any],
    ) -> None:
        signals = result.get("signals") if isinstance(result.get("signals"), dict) else {}
        suggested = result.get("suggested_answers") if isinstance(result.get("suggested_answers"), list) else []

        ambiguity_now = bool(signals.get("ambiguity_detected"))
        contradiction_now = bool(signals.get("contradiction_detected"))

        project = await cls._get_project(db, project_id)
        if not project:
            return

        metadata = project.extra_metadata if isinstance(project.extra_metadata, dict) else {}
        telemetry = dict(metadata.get(cls.KEY) or {})
        telemetry.setdefault("interview_turns", 0)
        telemetry.setdefault("ambiguity_detected_count", 0)
        telemetry.setdefault("contradiction_detected_count", 0)
        telemetry.setdefault("ambiguity_cases", 0)
        telemetry.setdefault("ambiguity_resolved", 0)
        telemetry.setdefault("pending_ambiguity", False)
        telemetry.setdefault("suggestion_offered_turns", 0)
        telemetry.setdefault("suggestion_accepted_count", 0)

        telemetry["interview_turns"] += 1
        if ambiguity_now:
            telemetry["ambiguity_detected_count"] += 1
            telemetry["ambiguity_cases"] += 1
            telemetry["pending_ambiguity"] = True
        elif telemetry.get("pending_ambiguity"):
            telemetry["ambiguity_resolved"] += 1
            telemetry["pending_ambiguity"] = False

        if contradiction_now:
            telemetry["contradiction_detected_count"] += 1
        if suggested:
            telemetry["suggestion_offered_turns"] += 1

        metadata = dict(metadata)
        metadata[cls.KEY] = telemetry
        project.extra_metadata = metadata

    @classmethod
    async def record_message_event(
        cls,
        db: AsyncSession,
        project_id: int,
        metadata_payload: Dict[str, Any] | None,
    ) -> None:
        if not isinstance(metadata_payload, dict):
            return
        if not metadata_payload.get("interview_selection"):
            return

        project = await cls._get_project(db, project_id)
        if not project:
            return

        metadata = project.extra_metadata if isinstance(project.extra_metadata, dict) else {}
        telemetry = dict(metadata.get(cls.KEY) or {})
        telemetry.setdefault("suggestion_accepted_count", 0)
        telemetry["suggestion_accepted_count"] += 1

        metadata = dict(metadata)
        metadata[cls.KEY] = telemetry
        project.extra_metadata = metadata

    @classmethod
    async def get_report(cls, db: AsyncSession, project_id: int) -> Dict[str, Any]:
        db_counters = await cls._db_counters(db, project_id)
        if db_counters is None:
            return cls._empty_report()

        turns = int(db_counters.get("interview_turns", 0))
        contradictions = int(db_counters.get("contradiction_detected_count", 0))
        ambiguity_cases = int(db_counters.get("ambiguity_cases", 0))
        ambiguity_resolved = int(db_counters.get("ambiguity_resolved", 0))
        offered = int(db_counters.get("suggestion_offered_turns", 0))
        accepted = int(db_counters.get("suggestion_accepted_count", 0))

        contradiction_catch_rate = round((contradictions / turns) if turns else 0.0, 4)
        ambiguity_resolution_rate = round((ambiguity_resolved / ambiguity_cases) if ambiguity_cases else 0.0, 4)
        suggestion_acceptance_rate = round((accepted / offered) if offered else 0.0, 4)

        return {
            "counters": {
                "interview_turns": turns,
                "contradiction_detected_count": contradictions,
                "ambiguity_cases": ambiguity_cases,
                "ambiguity_resolved": ambiguity_resolved,
                "suggestion_offered_turns": offered,
                "suggestion_accepted_count": accepted,
            },
            "evaluation": {
                "ambiguity_resolution_rate": ambiguity_resolution_rate,
                "contradiction_catch_rate": contradiction_catch_rate,
                "suggestion_acceptance_rate": suggestion_acceptance_rate,
            },
        }

    @classmethod
    async def _db_counters(cls, db: AsyncSession, project_id: int) -> Dict[str, int] | None:
        project = await cls._get_project(db, project_id)
        if not project:
            return None

        metadata = project.extra_metadata if isinstance(project.extra_metadata, dict) else {}
        telemetry = dict(metadata.get(cls.KEY) or {})
        return {
            "interview_turns": int(telemetry.get("interview_turns", 0) or 0),
            "contradiction_detected_count": int(telemetry.get("contradiction_detected_count", 0) or 0),
            "ambiguity_cases": int(telemetry.get("ambiguity_cases", 0) or 0),
            "ambiguity_resolved": int(telemetry.get("ambiguity_resolved", 0) or 0),
            "suggestion_offered_turns": int(telemetry.get("suggestion_offered_turns", 0) or 0),
            "suggestion_accepted_count": int(telemetry.get("suggestion_accepted_count", 0) or 0),
        }

    @staticmethod
    async def _get_project(db: AsyncSession, project_id: int) -> Project | None:
        stmt = select(Project).where(Project.id == project_id)
        result = await db.execute(stmt)
        return result.scalar_one_or_none()

    @staticmethod
    def _empty_report() -> Dict[str, Any]:
        return {
            "counters": {
                "interview_turns": 0,
                "contradiction_detected_count": 0,
                "ambiguity_cases": 0,
                "ambiguity_resolved": 0,
                "suggestion_offered_turns": 0,
                "suggestion_accepted_count": 0,
            },
            "evaluation": {
                "ambiguity_resolution_rate": 0.0,
                "contradiction_catch_rate": 0.0,
                "suggestion_acceptance_rate": 0.0,
            },
        }

################################################################################
# FILE: backend\tools\package.json
################################################################################

{
  "dependencies": {
    "playwright": "^1.58.2"
  }
}

################################################################################
# FILE: frontend\app.js
################################################################################

/**
 * Tawasul Web Frontend
 * Main Application Logic
 */

// Configuration
const API_BASE_URL = `${globalThis.location.protocol}//${globalThis.location.hostname}:8500`;

const inMemoryStorage = {};

function safeStorageGet(key, fallback = null) {
    try {
        const value = globalThis.localStorage.getItem(key);
        return value === null ? fallback : value;
    } catch (error) {
        console.warn('localStorage get failed, using in-memory fallback.', error);
        const value = inMemoryStorage[key];
        return value === undefined ? fallback : value;
    }
}

function safeStorageSet(key, value) {
    inMemoryStorage[key] = String(value);
    try {
        globalThis.localStorage.setItem(key, value);
    } catch (error) {
        console.warn('localStorage set failed, kept in-memory value.', error);
    }
}

function safeStorageRemove(key) {
    delete inMemoryStorage[key];
    try {
        globalThis.localStorage.removeItem(key);
    } catch (error) {
        console.warn('localStorage remove failed, removed in-memory value only.', error);
    }
}

// Translations
const i18n = {
    ar: {
        nav_dashboard: "Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…",
        nav_projects: "Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹",
        nav_chat: "Ù…Ù‚Ø§Ø¨Ù„Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
        nav_templates: "Ø§Ù„Ù†Ù…Ø§Ø°Ø¬",
        interview_mode: "ÙˆØ¶Ø¹ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©",
        nav_srs: "Ù…Ø³ØªÙ†Ø¯Ø§Øª SRS",
        nav_settings: "Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª",
        nav_ai_config: "Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡",
        ai_config_title: "Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
        settings_title: "Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª",
        settings_tab_ai: "Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
        settings_tab_bot: "Ø¨ÙˆØª ØªÙ„ÙŠØ¬Ø±Ø§Ù…",
        project_files_btn: "Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹",
        stat_projects: "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹",
        stat_docs: "Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª",
        stat_chunks: "Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù†ØµÙŠØ©",
        recent_projects: "Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø£Ø®ÙŠØ±Ø©",
        view_all: "Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙ„",
        your_projects: "Ù…Ø´Ø§Ø±ÙŠØ¹Ùƒ",
        project_goal_hint: "Ø£Ø¶Ù Ù‡Ø¯ÙÙ‹Ø§ Ù…Ø®ØªØµØ±Ù‹Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ SRS Ø£ÙØ¶Ù„.",
        welcome_title: "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Tawasul",
        project_name_ph: "Ù…Ø«Ù„Ø§Ù‹: Ø£Ø¨Ø­Ø§Ø« Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
        project_desc_ph: "ÙˆØµÙ Ù…Ø®ØªØµØ± Ù„Ù„Ù…Ø´Ø±ÙˆØ¹...",
        create_project_btn: "Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹",
        upload_title: "Ø±ÙØ¹ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©",
        upload_desc: "Ø§Ø³Ø­Ø¨ Ø§Ù„Ù…Ù„ÙØ§Øª Ù‡Ù†Ø§ Ø£Ùˆ Ø§Ø¶ØºØ· Ù„Ù„Ø§Ø®ØªÙŠØ§Ø±",
        upload_optional: "(Ø§Ø®ØªÙŠØ§Ø±ÙŠ) Ø§Ø±ÙØ¹ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø±Ø¬Ø¹ÙŠØ© Ù„Ø¯Ø¹Ù… Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©",
        start_interview_btn: "Ø§Ø¨Ø¯Ø£ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©",
        interview_cta_hint: "Ø§Ø¨Ø¯Ø£ Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ø°ÙƒÙŠ Ù„Ø¨Ù†Ø§Ø¡ Ù…Ø³ØªÙ†Ø¯ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª",
        docs_title: "Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©",
        bot_settings_title: "Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¨ÙˆØª Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…",
        bot_active_project: "Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ù†Ø´Ø·",
        bot_active_project_desc: "Ø§Ø®ØªØ± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø°ÙŠ Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¨ÙˆØª Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ù‡.",
        save_settings: "Ø­ÙØ¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª",
        bot_profile: "Ù…Ù„Ù Ø§Ù„Ø¨ÙˆØª",
        bot_profile_desc: "ØªØ­Ø¯ÙŠØ« Ø§Ø³Ù… Ø§Ù„Ø¨ÙˆØª Ø¹Ù„Ù‰ ØªÙ„ÙŠØ¬Ø±Ø§Ù….",
        bot_name: "Ø§Ø³Ù… Ø§Ù„Ø¨ÙˆØª",
        update_profile: "ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠ",
        processing_label: "Ø¬Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©",
        ai_settings_title: "Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬",
        ai_settings_desc: "Ø§Ø®ØªØ± Ù…Ø²ÙˆØ¯ Ø§Ù„ØªÙˆÙ„ÙŠØ¯.",
        gen_provider_label: "Ù…Ø²ÙˆØ¯ Ø§Ù„ØªÙˆÙ„ÙŠØ¯",
        select_project_ph: "Ø§Ø®ØªØ± Ù…Ø´Ø±ÙˆØ¹Ø§Ù‹...",
        delete_confirm: "Ù‡Ù„ Ø£Ù†Øª Ù…ØªØ£ÙƒØ¯ØŸ",
        success_saved: "ØªÙ… Ø§Ù„Ø­ÙØ¸ Ø¨Ù†Ø¬Ø§Ø­",
        error_generic: "Ø­Ø¯Ø« Ø®Ø·Ø£ Ù…Ø§",
        embedding_size_label: "Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†",
        config_group_providers: "Ø§Ù„Ù…Ø²ÙˆØ¯ÙˆÙ†",
        config_group_chunking: "Ø§Ù„ØªØ¬Ø²Ø¦Ø©",
        config_group_retrieval: "Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹",
        search_placeholder: "Ø§Ø¨Ø­Ø« Ø¹Ù† Ù…Ø´Ø±ÙˆØ¹ Ø£Ùˆ Ù…Ø³ØªÙ†Ø¯...",
        empty_projects: "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø´Ø§Ø±ÙŠØ¹ Ø¨Ø¹Ø¯",
        empty_projects_desc: "Ø£Ù†Ø´Ø¦ Ø£ÙˆÙ„ Ù…Ø´Ø±ÙˆØ¹ Ù„Ùƒ ÙˆØ§Ø¨Ø¯Ø£ ÙÙŠ Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª.",
        empty_docs: "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø¹Ø¯",
        empty_docs_desc: "Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª PDF Ø£Ùˆ TXT Ø£Ùˆ DOCX Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.",
        copy_btn: "Ù†Ø³Ø®",
        copied_btn: "ØªÙ… Ø§Ù„Ù†Ø³Ø®!",
        srs_title: "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª (SRS)",
        srs_subtitle: "Ù†Ø³Ø®Ø© Ø£ÙˆÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ù…Ø¹ Ù†Ù‚Ø§Ø· ØªØ­ØªØ§Ø¬ ØªÙˆØ¶ÙŠØ­.",
        srs_refresh: "ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø³ÙˆØ¯Ø©",
        srs_export: "ØªØµØ¯ÙŠØ± SRS",
        srs_export_pdf: "ØªØµØ¯ÙŠØ± PDF",
        srs_export_word: "ØªØµØ¯ÙŠØ± Word",
        srs_export_markdown: "ØªØµØ¯ÙŠØ± Markdown",
        srs_summary_title: "Ù…Ù„Ø®Øµ Ø³Ø±ÙŠØ¹",
        srs_open_questions: "Ù†Ù‚Ø§Ø· ØªØ­ØªØ§Ø¬ ØªÙˆØ¶ÙŠØ­",
        srs_next_steps: "Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©",
        srs_book_meeting: "Ø§Ø­Ø¬Ø² Ø¬Ù„Ø³Ø© Ù…Ø¹ Ø§Ù„ÙØ±ÙŠÙ‚",
        srs_confirm: "Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª",
        srs_confirmed: "ØªÙ… Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!",
        book_meeting_title: "Ø­Ø¬Ø² Ø¬Ù„Ø³Ø© Ù…Ø¹ Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ù„ØªÙ‚Ù†ÙŠ",
        book_name: "Ø§Ù„Ø§Ø³Ù…",
        book_email: "Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ",
        book_date: "Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙØ¶Ù„",
        book_time: "Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ÙØ¶Ù„",
        book_notes: "Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©",
        book_submit: "ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ø­Ø¬Ø²",
        book_success: "ØªÙ… Ø§Ù„Ø­Ø¬Ø² Ø¨Ù†Ø¬Ø§Ø­! Ø³ÙŠØªÙ… Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹Ùƒ Ù‚Ø±ÙŠØ¨Ø§Ù‹.",
        book_fill_required: "ÙŠØ±Ø¬Ù‰ Ù…Ù„Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©",
        stage_discovery: "Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù",
        stage_scope: "Ø§Ù„Ù†Ø·Ø§Ù‚",
        stage_users: "Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙˆÙ†",
        stage_features: "Ø§Ù„Ù…ÙŠØ²Ø§Øª",
        stage_constraints: "Ø§Ù„Ù‚ÙŠÙˆØ¯",
        mic_recording: "Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„...",
        mic_transcribing: "Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­ÙˆÙŠÙ„...",
        mic_error: "ØªØ¹Ø°Ø± Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†",
        mic_no_support: "Ø§Ù„Ù…ØªØµÙØ­ Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ",
        live_doc_title: "Ù…Ù„Ø®Øµ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª",
        live_doc_empty: "Ø§Ø¨Ø¯Ø£ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© ÙˆØ³ÙŠØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù„Ø®Øµ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹...",
        start_idea_btn: "Ø§Ø¨Ø¯Ø£ ÙÙƒØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©",
        upload_reference_docs: "Ø±ÙØ¹ Ù…Ø³ØªÙ†Ø¯Ø§Øª",
        start_idea_title: "Ø§Ø¨Ø¯Ø£ ÙÙƒØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©",
        idea_name_ph: "Ù…Ø«Ù„Ø§Ù‹: ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ù‡Ø§Ù…",
        start_idea_submit: "Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø¢Ù†",
        confirm_yes: "Ù†Ø¹Ù…ØŒ Ù…ØªØ£ÙƒØ¯",
        confirm_cancel: "Ø¥Ù„ØºØ§Ø¡",
        offline_banner: "Ø£Ù†Øª ØºÙŠØ± Ù…ØªØµÙ„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª",
        rate_limit_msg: "Ù…Ø­Ø§ÙˆÙ„Ø§Øª ÙƒØ«ÙŠØ±Ø©. Ø§Ù†ØªØ¸Ø± {seconds} Ø«Ø§Ù†ÙŠØ©",
        validation_email_invalid: "Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ØºÙŠØ± ØµØ§Ù„Ø­",
        validation_password_min: "ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ± ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† 6 Ø£Ø­Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„",
        validation_required: "Ù‡Ø°Ø§ Ø§Ù„Ø­Ù‚Ù„ Ù…Ø·Ù„ÙˆØ¨",
        validation_project_name: "ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø´Ø±ÙˆØ¹",
        interview_save_later: "Ø§Ø­ÙØ¸ ÙˆÙƒÙ…Ù„ Ø¨Ø¹Ø¯ÙŠÙ†",
        interview_resume: "Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø¢Ø®Ø± Ù…Ù‚Ø§Ø¨Ù„Ø©",
        interview_select_hint: "Ø§Ø®ØªØ± Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù‚ØªØ±Ø­Ø©",
        interview_select_send: "Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±",
        interview_option_skip: "ØªØ®Ø·Ù‘ÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ",
        interview_option_unsure: "Ù…Ø´ Ù…ØªØ£ÙƒØ¯ - Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¨Ø³Ø§Ø·Ø©",
        interview_review: "Ù…Ø±Ø§Ø¬Ø¹Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„",
        interview_privacy: "Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ù„Ù† ØªÙØ³ØªØ®Ø¯Ù… Ø¥Ù„Ø§ Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙˆØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„.",
        interview_next_step: "Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©: Ø³ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ SRS ÙÙˆØ±Ù‹Ø§ ÙˆÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø±Ø§Ø¬Ø¹ØªÙ‡ ÙˆØ§Ø¹ØªÙ…Ø§Ø¯Ù‡.",
        interview_restored: "ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¢Ø®Ø± Ø¬Ù„Ø³Ø© Ù…Ù‚Ø§Ø¨Ù„Ø© Ù…Ø­ÙÙˆØ¸Ø©.",
        interview_saved: "ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ù„Ø§Ø­Ù‚Ù‹Ø§ Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹.",
        interview_completed: "Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©! Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„.",
        interview_duplicate_guard: "ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙƒØ±Ø±Ø©. Ø£Ø¶Ù ØªÙØµÙŠÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… ØªØ®Ø·Ù‘ÙŠ.",
        interview_progress: "ØªÙ… Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²: {percent}%",
        templates_title: "Ø§Ø¨Ø¯Ø£ Ø¨Ø³Ø±Ø¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ù„Ø¨ Ø¬Ø§Ù‡Ø²",
        template_use: "Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø§Ù„Ø¨",
        template_ecommerce_title: "E-commerce SRS",
        template_ecommerce_desc: "Ù…ØªØ¬Ø± Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ù…Ù†ØªØ¬Ø§ØªØŒ Ø³Ù„Ø© Ø´Ø±Ø§Ø¡ØŒ Ø¯ÙØ¹ØŒ ÙˆØªØªØ¨Ø¹ Ø§Ù„Ø·Ù„Ø¨Ø§Øª.",
        template_saas_title: "SaaS Platform",
        template_saas_desc: "Ù…Ù†ØµØ© SaaS Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ù…Ø¹ Ø®Ø·Ø· Ø§Ø´ØªØ±Ø§Ùƒ ÙˆØµÙ„Ø§Ø­ÙŠØ§Øª ÙˆÙ„ÙˆØ­Ø© ØªÙ‚Ø§Ø±ÙŠØ±.",
        template_mobile_title: "Mobile App",
        template_mobile_desc: "ØªØ·Ø¨ÙŠÙ‚ Ø¬ÙˆØ§Ù„ Ù…Ø¹ ØªØ³Ø¬ÙŠÙ„ Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†ØŒ Ø¥Ø´Ø¹Ø§Ø±Ø§ØªØŒ ÙˆØªÙƒØ§Ù…Ù„ API."
    },
    en: {
        nav_dashboard: "Dashboard",
        nav_projects: "Projects",
        nav_chat: "AI Interview",
        nav_templates: "Templates",
        interview_mode: "Interview mode",
        nav_srs: "SRS Documents",
        nav_settings: "Settings",
        nav_ai_config: "AI Model",
        ai_config_title: "AI Model",
        settings_title: "Settings",
        settings_tab_ai: "AI Model",
        settings_tab_bot: "Telegram Bot",
        project_files_btn: "Project Files",
        stat_projects: "Total Projects",
        stat_docs: "Documents",
        stat_chunks: "Text Chunks",
        recent_projects: "Recent Projects",
        view_all: "View All",
        your_projects: "Your Projects",
        project_goal_hint: "Add a brief goal to help our AI generate a better SRS.",
        welcome_title: "Welcome to Tawasul",
        project_name_ph: "Ex: AI Research",
        project_desc_ph: "Short description...",
        create_project_btn: "Create Project",
        upload_title: "Upload New Documents",
        upload_desc: "Drag files here or click to select",
        upload_optional: "(Optional) Upload reference documents to support the interview",
        start_interview_btn: "Start Smart Interview",
        interview_cta_hint: "Chat with the AI Business Analyst to build your requirements document",
        docs_title: "Current Documents",
        bot_settings_title: "Telegram Bot Settings",
        bot_active_project: "Active Project",
        bot_active_project_desc: "Select the project the bot will answer from.",
        save_settings: "Save Settings",
        bot_profile: "Bot Profile",
        bot_profile_desc: "Update Bot Name on Telegram.",
        bot_name: "Bot Name",
        update_profile: "Update Profile",
        processing_label: "Processing",
        ai_settings_title: "Model Settings",
        ai_settings_desc: "Select the generation provider.",
        gen_provider_label: "Generation Provider",
        select_project_ph: "Select a project...",
        delete_confirm: "Are you sure?",
        success_saved: "Saved successfully",
        error_generic: "Something went wrong",
        embedding_size_label: "Embedding Dimensions",
        config_group_providers: "Providers",
        config_group_chunking: "Chunking",
        config_group_retrieval: "Retrieval",
        search_placeholder: "Search projects or documents...",
        empty_projects: "No projects yet",
        empty_projects_desc: "Create your first project and start uploading documents.",
        empty_docs: "No documents yet",
        empty_docs_desc: "Upload PDF, TXT, or DOCX files to start processing.",
        copy_btn: "Copy",
        copied_btn: "Copied!",
        srs_title: "SRS Review",
        srs_subtitle: "A first draft of requirements with items that need clarification.",
        srs_refresh: "Refresh Draft",
        srs_export: "Export SRS",
        srs_export_pdf: "Export PDF",
        srs_export_word: "Export Word",
        srs_export_markdown: "Export Markdown",
        srs_summary_title: "Quick Summary",
        srs_open_questions: "Open Questions",
        srs_next_steps: "Next Steps",
        srs_book_meeting: "Book a team session",
        srs_confirm: "Approve Requirements",
        srs_confirmed: "Requirements approved successfully!",
        book_meeting_title: "Book a session with the technical team",
        book_name: "Name",
        book_email: "Email",
        book_date: "Preferred Date",
        book_time: "Preferred Time",
        book_notes: "Additional Notes",
        book_submit: "Confirm Booking",
        book_success: "Booked successfully! We will contact you soon.",
        book_fill_required: "Please fill all required fields",
        stage_discovery: "Discovery",
        stage_scope: "Scope",
        stage_users: "Users",
        stage_features: "Features",
        stage_constraints: "Constraints",
        mic_recording: "Recording...",
        mic_transcribing: "Transcribing...",
        mic_error: "Cannot access microphone",
        mic_no_support: "Browser does not support audio recording",
        live_doc_title: "Requirements Summary",
        live_doc_empty: "Start the interview and the summary will update automatically...",
        start_idea_btn: "Start New Idea",
        upload_reference_docs: "Upload Docs",
        start_idea_title: "Start a New Idea",
        idea_name_ph: "Ex: Task Management App",
        start_idea_submit: "Start Now",
        confirm_yes: "Yes, confirm",
        confirm_cancel: "Cancel",
        offline_banner: "You are offline",
        rate_limit_msg: "Too many attempts. Wait {seconds} seconds",
        validation_email_invalid: "Invalid email address",
        validation_password_min: "Password must be at least 6 characters",
        validation_required: "This field is required",
        validation_project_name: "Please enter a project name",
        interview_save_later: "Save & continue later",
        interview_resume: "Resume last interview",
        interview_select_hint: "Choose a suggested answer",
        interview_select_send: "Send selection",
        interview_option_skip: "Skip this question",
        interview_option_unsure: "Not sure - please rephrase simply",
        interview_review: "Review before submit",
        interview_privacy: "Your data is used only for project requirements preparation and analysis quality.",
        interview_next_step: "Next step: SRS will be generated immediately for your review.",
        interview_restored: "Your latest saved interview session was restored.",
        interview_saved: "Interview progress saved. You can resume later from this project.",
        interview_completed: "Interview complete! Review final answers before submit.",
        interview_duplicate_guard: "This answer looks repeated. Add a new detail or use Skip.",
        interview_progress: "Completed: {percent}%",
        templates_title: "Start faster with ready templates",
        template_use: "Use this template",
        template_ecommerce_title: "E-commerce SRS",
        template_ecommerce_desc: "Online store with catalog, cart, checkout, payments, and order tracking.",
        template_saas_title: "SaaS Platform",
        template_saas_desc: "Multi-tenant SaaS platform with subscriptions, roles, and reporting dashboard.",
        template_mobile_title: "Mobile App",
        template_mobile_desc: "Mobile application with onboarding, notifications, and API integration."
    }
};

const INTERVIEW_AREAS = ['discovery', 'scope', 'users', 'features', 'constraints'];
const ADMIN_ONLY_VIEWS = new Set([]);
const IDEA_TEMPLATES = {
    ecommerce: {
        ar: {
            title: 'E-commerce SRS',
            description: 'Ù…ØªØ¬Ø± Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ù†ØªØ¬Ø§ØªØŒ Ø³Ù„Ø© Ø§Ù„Ø´Ø±Ø§Ø¡ØŒ Ø§Ù„Ø¯ÙØ¹ØŒ ÙˆØªØªØ¨Ø¹ Ø§Ù„Ø·Ù„Ø¨Ø§Øª.',
            prompt: 'Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: Ù…ØªØ¬Ø± Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ B2C. Ø±ÙƒÙ‘Ø² Ø¹Ù„Ù‰ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ù†ØªØ¬Ø§ØªØŒ Ø§Ù„Ø³Ù„Ø©ØŒ Ø§Ù„Ø¯ÙØ¹ØŒ ÙˆØªØªØ¨Ø¹ Ø§Ù„Ø´Ø­Ù†ØŒ ÙˆØ³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹.'
        },
        en: {
            title: 'E-commerce SRS',
            description: 'Online store with catalog, cart, payments, and order tracking.',
            prompt: 'Project scope: B2C e-commerce. Focus on product catalog, cart, checkout, shipping tracking, and returns policy.'
        }
    },
    saas: {
        ar: {
            title: 'SaaS Platform',
            description: 'Ù…Ù†ØµØ© SaaS Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ù…Ø¹ Ø§Ø´ØªØ±Ø§ÙƒØ§Øª ÙˆØµÙ„Ø§Ø­ÙŠØ§Øª ÙˆÙ„ÙˆØ­Ø§Øª ØªÙ‚Ø§Ø±ÙŠØ±.',
            prompt: 'Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: Ù…Ù†ØµØ© SaaS Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡. Ø±ÙƒÙ‘Ø² Ø¹Ù„Ù‰ Ø®Ø·Ø· Ø§Ù„Ø§Ø´ØªØ±Ø§ÙƒØŒ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ÙØ±Ù‚ØŒ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§ØªØŒ ÙˆÙ„ÙˆØ­Ø© ØªÙ‚Ø§Ø±ÙŠØ± Ù„Ù„Ø¥Ø¯Ø§Ø±Ø©.'
        },
        en: {
            title: 'SaaS Platform',
            description: 'Multi-tenant SaaS platform with subscriptions, roles, and dashboards.',
            prompt: 'Project scope: multi-tenant SaaS. Focus on subscription plans, team management, role permissions, and admin reporting dashboard.'
        }
    },
    'mobile-app': {
        ar: {
            title: 'Mobile App',
            description: 'ØªØ·Ø¨ÙŠÙ‚ Ø¬ÙˆØ§Ù„ Ù…Ø¹ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†ØŒ Ø¥Ø´Ø¹Ø§Ø±Ø§ØªØŒ ÙˆØªÙƒØ§Ù…Ù„ API.',
            prompt: 'Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: ØªØ·Ø¨ÙŠÙ‚ Ø¬ÙˆØ§Ù„ iOS/Android. Ø±ÙƒÙ‘Ø² Ø¹Ù„Ù‰ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ØŒ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†ØŒ Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±Ø§ØªØŒ ÙˆØªØ¬Ø±Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø³Ø±ÙŠØ¹Ø©.'
        },
        en: {
            title: 'Mobile App',
            description: 'Mobile app with onboarding, notifications, and API integration.',
            prompt: 'Project scope: iOS/Android mobile app. Focus on onboarding, profile management, notifications, and fast UX flows.'
        }
    }
};

// State Management
const state = {
    currentView: 'projects',
    projects: [],
    stats: null,
    selectedProject: null,
    chatMessages: [],
    isUploading: false,
    docPoller: null,
    interviewMode: false,
    interviewStage: 'discovery',
    isRecording: false,
    mediaRecorder: null,
    pendingProjectSelect: null,
    chatProjectId: null,
    srsRefreshing: false,
    previousSummary: null,
    lastCoverage: null,
    lastInterviewSignals: null,
    lastLivePatch: null,
    lastCycleTrace: null,
    lastTopicNavigation: null,
    lastInterviewTelemetry: null,
    interviewDraftMeta: null,
    lastAssistantQuestion: '',
    lastUserInterviewAnswer: '',
    lastRenderedSrsDraft: null,
    pendingInterviewSelectionMeta: null,
    pendingSttMeta: null,
    summaryCollapsed: true,
    authToken: safeStorageGet('authToken', null),
    currentUser: JSON.parse(safeStorageGet('currentUser', 'null')),
    lang: safeStorageGet('lang', 'ar'),
    theme: safeStorageGet('theme', 'light')
};

let _msgIdCounter = 0;

function getFallbackSrsDraft() {
    return {
        status: state.lang === 'ar' ? 'Ù…Ø³ÙˆØ¯Ø© Ø£ÙˆÙ„ÙŠØ©' : 'First Draft',
        updated: state.lang === 'ar' ? 'Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: Ø§Ù„ÙŠÙˆÙ…' : 'Last updated: today',
        summary: state.lang === 'ar'
            ? 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ÙˆØ¯Ø© Ø¨Ø¹Ø¯. Ø­Ø¯Ù‘Ø« Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø£ÙˆÙ„Ø§Ù‹ Ø«Ù… Ø§Ø¶ØºØ· ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø³ÙˆØ¯Ø©.'
            : 'No draft yet. Start a chat and click refresh to generate one.',
        metrics: [],
        sections: [],
        activity_diagram: [],
        questions: [],
        nextSteps: []
    };
}

// DOM Elements
const elements = {
    viewContainer: document.getElementById('view-container'),
    navItems: document.querySelectorAll('.sidebar-nav li'),
    newProjectBtn: document.getElementById('new-project-btn'),
    modalOverlay: document.getElementById('modal-overlay'),
    modalTitle: document.getElementById('modal-title'),
    modalBody: document.getElementById('modal-body'),
    closeModalBtn: document.querySelector('.close-modal'),
    themeToggle: document.getElementById('theme-toggle'),
    langToggle: document.getElementById('lang-toggle'),
    sidebarToggleBtn: document.getElementById('sidebar-toggle-btn')
};

function isAdminUser() {
    return Boolean(state.currentUser?.role === 'admin');
}

function applyRoleBasedNavigation() {
    const isAdmin = isAdminUser();
    elements.navItems.forEach((item) => {
        const viewName = item.dataset.view;
        if (!viewName) return;
        if (!ADMIN_ONLY_VIEWS.has(viewName)) return;
        item.style.display = isAdmin ? '' : 'none';
    });
}

// --- API Client ---

function authHeaders(extra = {}) {
    const headers = { ...extra };
    if (state.authToken) {
        headers['Authorization'] = `Bearer ${state.authToken}`;
    }
    return headers;
}

// --- Network Offline Banner helpers (3.5) ---

function showOfflineBanner() {
    const banner = document.getElementById('offline-banner');
    if (banner) banner.classList.add('visible');
}

function hideOfflineBanner() {
    const banner = document.getElementById('offline-banner');
    if (banner) banner.classList.remove('visible');
}

async function throwIfNotOk(response, fallbackMessage = 'Request failed') {
    if (!response.ok) {
        const contentType = response.headers.get('content-type') || '';
        const err = contentType.includes('application/json') ? await response.json() : null;
        throw new Error(err?.detail || fallbackMessage);
    }
    return response;
}

const api = {
    async get(endpoint, requestOptions = {}) {
        try {
            const response = await fetch(`${API_BASE_URL}${endpoint}`, {
                headers: authHeaders(),
                ...requestOptions
            });
            if (response.status === 401) {
                clearAuthState();
                showAuthScreen();
                throw new Error(state.lang === 'ar' ? 'Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø¬Ù„Ø³Ø©ØŒ ÙŠØ±Ø¬Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰' : 'Session expired, please login again');
            }
            await throwIfNotOk(response);
            hideOfflineBanner();
            return await response.json();
        } catch (error) {
            console.error(`API Get Error (${endpoint}):`, error);
            if (error instanceof TypeError) {
                showOfflineBanner();
            } else {
                showNotification(error.message || (state.lang === 'ar' ? 'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø³ÙŠØ±ÙØ±' : 'Server Connection Error'), 'error');
            }
            throw error;
        }
    },

    async post(endpoint, data, isFormData = false, requestOptions = {}) {
        try {
            const headers = isFormData
                ? authHeaders()
                : authHeaders({ 'Content-Type': 'application/json' });
            const options = {
                method: 'POST',
                headers,
                body: isFormData ? data : JSON.stringify(data),
                ...requestOptions
            };

            const response = await fetch(`${API_BASE_URL}${endpoint}`, options);
            if (response.status === 401) {
                clearAuthState();
                showAuthScreen();
                throw new Error(state.lang === 'ar' ? 'Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø¬Ù„Ø³Ø©ØŒ ÙŠØ±Ø¬Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰' : 'Session expired, please login again');
            }
            await throwIfNotOk(response);
            hideOfflineBanner();
            return await response.json();
        } catch (error) {
            console.error(`API Post Error (${endpoint}):`, error);
            if (error instanceof TypeError) {
                showOfflineBanner();
            } else {
                showNotification(error.message, 'error');
            }
            throw error;
        }
    },

    async delete(endpoint, requestOptions = {}) {
        try {
            const response = await fetch(`${API_BASE_URL}${endpoint}`, {
                method: 'DELETE',
                headers: authHeaders(),
                ...requestOptions
            });
            if (response.status === 401) {
                clearAuthState();
                showAuthScreen();
                throw new Error(state.lang === 'ar' ? 'Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø¬Ù„Ø³Ø©ØŒ ÙŠØ±Ø¬Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰' : 'Session expired, please login again');
            }
            await throwIfNotOk(response);
            return true;
        } catch (error) {
            console.error(`API Delete Error (${endpoint}):`, error);
            showNotification(error.message, 'error');
            throw error;
        }
    }
};

// --- View Rendering ---

const views = {
    async dashboard() {
        renderTemplate('dashboard-template');
        showLoader();

        try {
            const [projects, stats] = await Promise.all([
                api.get('/projects/'),
                api.get('/stats/')
            ]);
            state.projects = projects;

            // Animate stats
            animateCounter('stat-projects-dashboard', stats.projects);
            animateCounter('stat-docs-dashboard', stats.documents);

            // Render recent projects
            const list = document.getElementById('recent-projects-list-dashboard');
            list.innerHTML = '';
            if (projects.length === 0) {
                list.innerHTML = createEmptyState('fa-folder-open', 'empty_projects', 'empty_projects_desc');
            } else {
                projects.slice(0, 3).forEach(project => {
                    list.appendChild(createProjectCard(project));
                });
            }

            // View All link -> switch to projects
            const viewAllLink = document.querySelector('.section-header .link');
            if (viewAllLink) {
                viewAllLink.onclick = (e) => { e.preventDefault(); switchView('projects'); };
            }

            applyTranslations();
        } catch (error) {
            console.error('Dashboard Load Error:', error);
        } finally {
            hideLoader();
        }
    },

    async projects() {
        renderTemplate('projects-template');
        showLoader();

        try {
            const projectsNewBtn = document.getElementById('projects-new-project-btn');
            if (projectsNewBtn) {
                projectsNewBtn.onclick = handleNewProject;
            }

            const [projects, stats] = await Promise.all([
                api.get('/projects/'),
                api.get('/stats/')
            ]);
            state.projects = projects;

            animateCounter('stat-projects', stats.projects || projects.length || 0);
            animateCounter('stat-docs', stats.documents || 0);

            const list = document.getElementById('all-projects-list');
            list.innerHTML = '';
            if (projects.length === 0) {
                list.innerHTML = createEmptyState('fa-folder-open', 'empty_projects', 'empty_projects_desc');
            } else {
                projects.forEach(project => {
                    list.appendChild(createProjectCard(project));
                });
            }
            applyTranslations();
        } catch (error) {
            console.error('Projects Load Error:', error);
        } finally {
            hideLoader();
        }
    },

    async chat() {
        renderTemplate('chat-template');

        const select = document.getElementById('chat-project-select');
        const projects = await api.get('/projects/');

        projects.forEach(p => {
            const opt = document.createElement('option');
            opt.value = p.id;
            opt.textContent = p.name;
            select.appendChild(opt);
        });

        // Auto-select project if coming from "Start New Idea"
        if (state.pendingProjectSelect) {
            select.value = state.pendingProjectSelect;
            state.chatProjectId = state.pendingProjectSelect;
            updateChatProjectHeader(state.pendingProjectSelect);
            loadChatHistory(state.pendingProjectSelect);
            state.pendingProjectSelect = null;
        } else if (state.chatProjectId) {
            select.value = state.chatProjectId;
            updateChatProjectHeader(state.chatProjectId);
            loadChatHistory(state.chatProjectId);
        } else if (projects.length > 0) {
            const lastProjectId = Number(projects[projects.length - 1].id);
            select.value = String(lastProjectId);
            state.chatProjectId = lastProjectId;
            updateChatProjectHeader(lastProjectId);
            loadChatHistory(lastProjectId);
        } else {
            // No projects at all â€” show empty state CTA
            const messagesContainer = document.getElementById('chat-messages');
            if (messagesContainer) {
                updateChatProjectHeader(null);
                messagesContainer.innerHTML = `
                    <div class="welcome-msg-pro">
                        <div class="welcome-icon"><i class="fas fa-folder-plus"></i></div>
                        <h2>${state.lang === 'ar' ? 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø´Ø§Ø±ÙŠØ¹ Ø¨Ø¹Ø¯' : 'No projects yet'}</h2>
                        <p>${state.lang === 'ar' ? 'Ø£Ù†Ø´Ø¦ Ù…Ø´Ø±ÙˆØ¹Ùƒ Ø§Ù„Ø£ÙˆÙ„ Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©' : 'Create your first project to start an interview'}</p>
                        <button class="btn btn-primary mt-4" id="chat-new-project-cta">
                            <i class="fas fa-plus"></i>
                            <span>${state.lang === 'ar' ? 'Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯' : 'Create a project'}</span>
                        </button>
                    </div>
                `;
                const cta = document.getElementById('chat-new-project-cta');
                if (cta) cta.onclick = handleNewProject;
            }
        }

        if (select.value) {
            const draft = await loadInterviewDraft(Number.parseInt(select.value, 10));
            if (draft) {
                state.previousSummary = draft.summary || null;
                state.lastCoverage = draft.coverage || null;
                state.lastInterviewSignals = draft.signals || null;
                state.lastLivePatch = draft.livePatch || null;
                state.lastCycleTrace = draft.cycleTrace || null;
                state.lastTopicNavigation = draft.topicNavigation || null;
                state.interviewStage = draft.stage || 'discovery';
                state.lastAssistantQuestion = draft.lastAssistantQuestion || '';
                state.interviewDraftMeta = draft;
                if (draft.mode) state.interviewMode = true;
                showNotification(i18n[state.lang].interview_restored, 'info');
            }
        }

        // Load chat history when switching projects
        select.onchange = async () => {
            if (select.value) {
                state.chatProjectId = Number.parseInt(select.value, 10);
                updateChatProjectHeader(state.chatProjectId);
                loadChatHistory(state.chatProjectId);
                const draft = await loadInterviewDraft(state.chatProjectId);
                if (draft) {
                    state.previousSummary = draft.summary || null;
                    state.lastCoverage = draft.coverage || null;
                    state.lastInterviewSignals = draft.signals || null;
                    state.lastLivePatch = draft.livePatch || null;
                    state.lastCycleTrace = draft.cycleTrace || null;
                    state.lastTopicNavigation = draft.topicNavigation || null;
                    state.interviewStage = draft.stage || 'discovery';
                    state.lastAssistantQuestion = draft.lastAssistantQuestion || '';
                    state.interviewDraftMeta = draft;
                    if (draft.mode) state.interviewMode = true;
                    if (interviewToggle) interviewToggle.checked = state.interviewMode;
                    showNotification(i18n[state.lang].interview_restored, 'info');
                } else {
                    state.previousSummary = null;
                    state.lastCoverage = null;
                    state.lastInterviewSignals = null;
                    state.lastLivePatch = null;
                    state.lastCycleTrace = null;
                    state.lastTopicNavigation = null;
                    state.interviewDraftMeta = null;
                    state.lastAssistantQuestion = '';
                }
                updateInterviewProgress(state.lastCoverage, false);
                updateInterviewAssistBar(state.lastCoverage);
                if (typeof updateResumeButtonState === 'function') {
                    updateResumeButtonState();
                }
            }
        };

        const sendBtn = document.getElementById('send-btn');
        const chatInput = document.getElementById('chat-input');
        const langSelect = document.getElementById('chat-lang');
        const clearBtn = document.getElementById('clear-chat-btn');
        const interviewToggle = document.getElementById('chat-interview-toggle');
        const summaryToggleBtn = document.getElementById('summary-toggle-btn');

        // Enable/disable send button based on input
        chatInput.oninput = () => {
            sendBtn.disabled = !chatInput.value.trim();
            autoResizeTextarea(chatInput);
        };

        sendBtn.onclick = handleChatSubmit;
        chatInput.onkeydown = (e) => {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                if (chatInput.value.trim()) handleChatSubmit();
            }
        };

        // Mic button handler
        const micBtn = document.getElementById('mic-btn');
        if (micBtn) {
            micBtn.onclick = () => {
                if (state.isRecording) {
                    stopRecording();
                } else {
                    startRecording();
                }
            };
        }

        if (interviewToggle) {
            interviewToggle.checked = state.interviewMode;
            interviewToggle.onchange = () => {
                state.interviewMode = interviewToggle.checked;
                if (!state.interviewMode) {
                    state.lastInterviewSignals = null;
                    state.lastLivePatch = null;
                    state.lastCycleTrace = null;
                }
                updateInterviewProgress(null, false);
                updateInterviewAssistBar(state.lastCoverage);
                const hint = state.lang === 'ar'
                    ? 'ÙˆØ¶Ø¹ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©: Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø³ØªÙƒÙˆÙ† Ù…ÙˆØ¬Ù‘Ù‡Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª.'
                    : 'Interview mode: guided questions to capture requirements.';
                showNotification(hint, 'info');
            };
        }

        const resumeBtn = document.getElementById('interview-resume-btn');
        const saveLaterBtn = document.getElementById('interview-save-later-btn');
        const reviewBtn = document.getElementById('interview-review-btn');

        const updateResumeButtonState = () => {
            if (!resumeBtn) return;
            const projectId = Number.parseInt(select.value || '0', 10);
            const localDraft = projectId ? loadInterviewDraftLocal(projectId) : null;
            resumeBtn.disabled = !(projectId && localDraft);
        };

        const restoreDraftForCurrentProject = async () => {
            const projectId = Number.parseInt(select.value || '0', 10);
            if (!projectId) {
                showNotification(state.lang === 'ar' ? 'Ø§Ø®ØªØ± Ù…Ø´Ø±ÙˆØ¹Ø§Ù‹ Ø£ÙˆÙ„Ø§Ù‹' : 'Select a project first', 'warning');
                return;
            }

            const draft = await loadInterviewDraft(projectId);
            if (!draft) {
                showNotification(state.lang === 'ar' ? 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­ÙØ¸ Ø³Ø§Ø¨Ù‚ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©.' : 'No saved interview found for this project.', 'warning');
                updateResumeButtonState();
                return;
            }

            state.previousSummary = draft.summary || null;
            state.lastCoverage = draft.coverage || null;
            state.lastInterviewSignals = draft.signals || null;
            state.lastLivePatch = draft.livePatch || null;
            state.lastCycleTrace = draft.cycleTrace || null;
            state.lastTopicNavigation = draft.topicNavigation || null;
            state.interviewStage = draft.stage || 'discovery';
            state.lastAssistantQuestion = draft.lastAssistantQuestion || '';
            state.interviewDraftMeta = draft;
            if (draft.mode) {
                state.interviewMode = true;
                if (interviewToggle) interviewToggle.checked = true;
            }

            updateInterviewProgress(state.lastCoverage, false);
            updateInterviewAssistBar(state.lastCoverage);
            showNotification(i18n[state.lang].interview_restored, 'success');
            updateResumeButtonState();
        };

        if (resumeBtn) {
            const label = resumeBtn.querySelector('span');
            const resumeText = i18n[state.lang].interview_resume;
            if (label) label.textContent = resumeText;
            resumeBtn.title = resumeText;
            resumeBtn.dataset.tooltip = resumeText;
            resumeBtn.onclick = restoreDraftForCurrentProject;
        }

        if (saveLaterBtn) {
            const label = saveLaterBtn.querySelector('span');
            const saveLaterText = i18n[state.lang].interview_save_later;
            if (label) label.textContent = saveLaterText;
            saveLaterBtn.title = saveLaterText;
            saveLaterBtn.dataset.tooltip = saveLaterText;
            saveLaterBtn.onclick = async () => {
                const projectId = Number.parseInt(select.value || '0', 10);
                if (!projectId) {
                    showNotification(state.lang === 'ar' ? 'Ø§Ø®ØªØ± Ù…Ø´Ø±ÙˆØ¹Ø§Ù‹ Ø£ÙˆÙ„Ø§Ù‹' : 'Select a project first', 'warning');
                    return;
                }
                await saveInterviewDraft(projectId);
                showNotification(i18n[state.lang].interview_saved, 'success');
                updateResumeButtonState();
            };
        }

        if (reviewBtn) {
            const label = reviewBtn.querySelector('span');
            const reviewText = i18n[state.lang].interview_review;
            if (label) label.textContent = reviewText;
            reviewBtn.title = reviewText;
            reviewBtn.dataset.tooltip = reviewText;
            reviewBtn.onclick = async () => {
                const projectId = Number.parseInt(select.value || '0', 10);
                if (!projectId) return;
                await openInterviewReviewModal(projectId, langSelect.value, {
                    summary: state.previousSummary || {},
                    stage: state.interviewStage || 'discovery',
                    coverage: state.lastCoverage || {},
                    done: false
                });
            };
        }

        // Init interview progress bar state
        updateChatProjectHeader(Number.parseInt(select.value || '0', 10) || null);
        applySummaryDrawerState();
        updateInterviewProgress(state.lastCoverage, false);
        updateInterviewAssistBar(state.lastCoverage);
        updateResumeButtonState();

        // Live doc close button
        const liveDocClose = document.getElementById('live-doc-close');
        if (liveDocClose) {
            liveDocClose.onclick = () => {
                state.summaryCollapsed = true;
                applySummaryDrawerState();
            };
        }

        if (summaryToggleBtn) {
            const summaryText = state.lang === 'ar' ? 'Ù…Ù„Ø®Øµ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª' : 'Requirements Summary';
            const summaryLabel = summaryToggleBtn.querySelector('span');
            if (summaryLabel) summaryLabel.textContent = summaryText;
            summaryToggleBtn.title = summaryText;
            summaryToggleBtn.dataset.tooltip = summaryText;
            summaryToggleBtn.onclick = () => {
                state.summaryCollapsed = !state.summaryCollapsed;
                applySummaryDrawerState();
            };
        }

        // Project Files drawer toggle
        const filesBtn = document.getElementById('chat-files-btn');
        const filesDrawer = document.getElementById('files-drawer');
        const filesDrawerClose = document.getElementById('files-drawer-close');

        const openFilesDrawer = async () => {
            const projectId = Number.parseInt(select.value || '0', 10);
            if (!projectId) {
                showNotification(state.lang === 'ar' ? 'Ø§Ø®ØªØ± Ù…Ø´Ø±ÙˆØ¹Ø§Ù‹ Ø£ÙˆÙ„Ø§Ù‹' : 'Select a project first', 'warning');
                return;
            }
            filesDrawer.style.display = '';
            setupUploadZone(projectId);
            try {
                const docs = await api.get(`/projects/${projectId}/documents`);
                renderDocsList(docs);
                startDocPolling(projectId, docs);
            } catch (e) {
                console.error('Files Drawer Load Error:', e);
            }
        };

        if (filesBtn) filesBtn.onclick = openFilesDrawer;
        if (filesDrawerClose) filesDrawerClose.onclick = () => { filesDrawer.style.display = 'none'; };

        // Clear chat handler
        if (clearBtn) {
            clearBtn.onclick = async () => {
                const messagesContainer = document.getElementById('chat-messages');
                const projectId = Number.parseInt(select.value || '0', 10);

                if (projectId) {
                    try {
                        await api.delete(`/projects/${projectId}/messages`);
                    } catch (error) {
                        console.error('Clear chat failed:', error);
                        showNotification(state.lang === 'ar' ? 'ØªØ¹Ø°Ø± Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ±ÙØ±' : 'Failed to clear chat on server', 'error');
                        return;
                    }
                }

                messagesContainer.innerHTML = `
                    ${getChatWelcomeMarkup(projectId)}
                `;
                bindSuggestionChips();
                state.chatMessages = [];
                showNotification(state.lang === 'ar' ? 'ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©' : 'Chat cleared', 'success');
            };
        }

        // Suggestion chips handler
        bindSuggestionChips();

        applyTranslations();
    },

    async srs() {
        renderTemplate('srs-template');
        showLoader();

        try {
            const projects = await api.get('/projects/');
            const select = document.getElementById('srs-project-select');
            const exportFormat = document.getElementById('srs-export-format');
            const refreshBtn = document.getElementById('srs-refresh-btn');
            const exportBtn = document.getElementById('srs-export-btn');
            const bookBtn = document.getElementById('srs-book-btn');

            if (exportFormat) {
                updateExportButtonLabel(exportFormat.value || 'pdf');
                exportFormat.onchange = () => updateExportButtonLabel(exportFormat.value || 'pdf');
            }

            projects.forEach(p => {
                const opt = document.createElement('option');
                opt.value = p.id;
                opt.textContent = p.name;
                select.appendChild(opt);
            });

            // Support navigation from project cards (pending selection), then fallback to selected project
            if (state.pendingProjectSelect && select) {
                select.value = state.pendingProjectSelect;
                state.pendingProjectSelect = null;
            } else if (state.selectedProject && select) {
                select.value = state.selectedProject.id;
            }

            refreshBtn.onclick = async () => {
                if (!select.value) {
                    showNotification(state.lang === 'ar' ? 'Ø§Ø®ØªØ± Ù…Ø´Ø±ÙˆØ¹Ø§Ù‹ Ø£ÙˆÙ„Ø§Ù‹' : 'Select a project first', 'warning');
                    return;
                }
                setButtonLoading(refreshBtn, true);
                await loadSrsDraft(select.value, true);
                setButtonLoading(refreshBtn, false);
            };
            exportBtn.onclick = async () => {
                if (!select.value) {
                    showNotification(state.lang === 'ar' ? 'Ø§Ø®ØªØ± Ù…Ø´Ø±ÙˆØ¹Ø§Ù‹ Ø£ÙˆÙ„Ø§Ù‹' : 'Select a project first', 'warning');
                    return;
                }
                setButtonLoading(exportBtn, true);
                const selectedFormat = exportFormat?.value || 'pdf';
                await exportSrsDocument(select.value, selectedFormat);
                setButtonLoading(exportBtn, false);
            };
            bookBtn.onclick = () => openBookingModal();

            const confirmBtn = document.getElementById('srs-confirm-btn');
            if (confirmBtn) {
                confirmBtn.onclick = async () => {
                    if (!select.value) {
                        showNotification(state.lang === 'ar' ? 'Ø§Ø®ØªØ± Ù…Ø´Ø±ÙˆØ¹Ø§Ù‹ Ø£ÙˆÙ„Ø§Ù‹' : 'Select a project first', 'warning');
                        return;
                    }
                    setButtonLoading(confirmBtn, true);
                    try {
                        const clientName = state.currentUser ? state.currentUser.name : '';
                        const clientEmail = state.currentUser ? state.currentUser.email : '';
                        await api.post(`/projects/${select.value}/handoff`, {
                            client_name: clientName,
                            client_email: clientEmail,
                            notes: ''
                        });
                    } catch (error) {
                        console.error('Handoff error:', error);
                    }
                    setButtonLoading(confirmBtn, false);
                    showNotification(i18n[state.lang].srs_confirmed, 'success');
                    confirmBtn.disabled = true;
                    confirmBtn.innerHTML = `<i class="fas fa-check-double"></i> <span>${state.lang === 'ar' ? 'ØªÙ… Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯' : 'Approved'}</span>`;
                    confirmBtn.classList.remove('btn-primary');
                    confirmBtn.classList.add('btn-confirmed');
                };
            }

            exportBtn.disabled = !select.value;
            select.onchange = async () => {
                exportBtn.disabled = !select.value;
                if (select.value) {
                    await loadSrsDraft(select.value, false);
                } else {
                    renderSrsDraft(getFallbackSrsDraft());
                }
            };

            if (select.value) {
                await loadSrsDraft(select.value, false);
            } else {
                renderSrsDraft(getFallbackSrsDraft());
            }
            applyTranslations();
        } catch (error) {
            console.error('SRS View Error:', error);
        } finally {
            hideLoader();
        }
    },

    async settings() {
        renderTemplate('settings-template');
        showLoader();

        try {
            const [projects, botConfig, aiConfig] = await Promise.all([
                api.get('/projects/'),
                api.get('/bot/config'),
                api.get('/config/providers'),
            ]);

            const aiSelect = document.getElementById('settings-ai-gen-provider');
            const aiSaveBtn = document.getElementById('settings-save-ai-config-btn');

            const genProviders = aiConfig.available?.llm || [];
            const labelMap = {
                gemini: 'Gemini 2.5 Flash',
                'gemini-2.5-lite-flash': 'Gemini 2.5 Lite Flash',
                'openrouter-gemini-2.0-flash': 'OpenRouter: Gemini 2.0 Flash',
                'openrouter-free': 'OpenRouter: Free',
                'groq-llama-3.3-70b-versatile': 'Groq: Llama 3.3 70B',
                'cerebras-llama-3.3-70b': 'Cerebras: Llama 3.3 70B',
                'cerebras-llama-3.1-8b': 'Cerebras: Llama 3.1 8B',
                cohere: 'Cohere',
            };

            genProviders.forEach((name) => {
                const opt = document.createElement('option');
                opt.value = name;
                opt.textContent = labelMap[name] || name;
                if (aiConfig.llm_provider === name) opt.selected = true;
                aiSelect.appendChild(opt);
            });

            if (aiSaveBtn) {
                aiSaveBtn.onclick = async () => {
                    setButtonLoading(aiSaveBtn, true);
                    try {
                        await api.post('/config/providers', { llm_provider: aiSelect.value });
                        showNotification(i18n[state.lang].success_saved, 'success');
                    } catch (e) {
                        console.error(e);
                    } finally {
                        setButtonLoading(aiSaveBtn, false);
                    }
                };
            }

            // --- Bot config ---
            const botSelect = document.getElementById('bot-active-project');
            projects.forEach(p => {
                const opt = document.createElement('option');
                opt.value = p.id;
                opt.textContent = p.name;
                if (botConfig.active_project_id == p.id) opt.selected = true;
                botSelect.appendChild(opt);
            });

            document.getElementById('save-bot-config-btn').onclick = async () => {
                const projectId = botSelect.value;
                if (!projectId) return;
                const btn = document.getElementById('save-bot-config-btn');
                setButtonLoading(btn, true);
                try {
                    await api.post('/bot/config', { active_project_id: Number.parseInt(projectId, 10) });
                    showNotification(i18n[state.lang].success_saved, 'success');
                } catch (e) {
                    console.error(e);
                } finally {
                    setButtonLoading(btn, false);
                }
            };

            document.getElementById('update-bot-profile-btn').onclick = async () => {
                const name = document.getElementById('bot-name-input').value;
                if (!name) return;
                const btn = document.getElementById('update-bot-profile-btn');
                setButtonLoading(btn, true);
                const formData = new FormData();
                formData.append('name', name);
                try {
                    await api.post('/bot/profile', formData, true);
                    showNotification(i18n[state.lang].success_saved, 'success');
                } catch (e) {
                    console.error(e);
                } finally {
                    setButtonLoading(btn, false);
                }
            };

            // --- Telegram Bot Config ---
            const tgTokenInput = document.getElementById('tg-bot-token');
            const tgAdminInput = document.getElementById('tg-admin-id');
            const tgEmailInput = document.getElementById('tg-api-email');
            const tgPasswordInput = document.getElementById('tg-api-password');
            const tgUrlInput = document.getElementById('tg-api-url');
            const tgSaveBtn = document.getElementById('save-tg-config-btn');
            const tgTokenToggle = document.getElementById('tg-token-toggle');
            const tgStatus = document.getElementById('tg-config-status');

            // Load current Telegram config
            try {
                const tgConfig = await api.get('/bot/telegram-config');
                if (tgConfig.telegram_bot_token) tgTokenInput.placeholder = tgConfig.telegram_bot_token;
                if (tgConfig.telegram_admin_id) tgAdminInput.value = tgConfig.telegram_admin_id;
                if (tgConfig.bot_api_email) tgEmailInput.value = tgConfig.bot_api_email;
                if (tgConfig.api_base_url) tgUrlInput.value = tgConfig.api_base_url;

                if (tgConfig.has_token) {
                    tgStatus.style.display = 'block';
                    tgStatus.style.background = 'var(--success-bg, #d4edda)';
                    tgStatus.style.color = 'var(--success, #155724)';
                    tgStatus.innerHTML = '<i class="fas fa-check-circle"></i> Bot token is configured. Leave the token field empty to keep current token.';
                }
            } catch (e) {
                console.error('Failed to load Telegram config:', e);
            }

            // Toggle token visibility
            if (tgTokenToggle) {
                tgTokenToggle.onclick = () => {
                    const isPassword = tgTokenInput.type === 'password';
                    tgTokenInput.type = isPassword ? 'text' : 'password';
                    tgTokenToggle.querySelector('i').className = isPassword ? 'fas fa-eye-slash' : 'fas fa-eye';
                };
            }

            // Save Telegram config
            if (tgSaveBtn) {
                tgSaveBtn.onclick = async () => {
                    setButtonLoading(tgSaveBtn, true);
                    try {
                        const payload = {};
                        if (tgTokenInput.value.trim()) payload.telegram_bot_token = tgTokenInput.value.trim();
                        if (tgAdminInput.value.trim()) payload.telegram_admin_id = tgAdminInput.value.trim();
                        if (tgEmailInput.value.trim()) payload.bot_api_email = tgEmailInput.value.trim();
                        if (tgPasswordInput.value.trim()) payload.bot_api_password = tgPasswordInput.value.trim();
                        if (tgUrlInput.value.trim()) payload.api_base_url = tgUrlInput.value.trim();

                        if (Object.keys(payload).length === 0) {
                            showNotification('Please fill in at least one field', 'warning');
                            return;
                        }

                        await api.post('/bot/telegram-config', payload);
                        showNotification('Telegram config saved! Restart the bot to apply.', 'success');

                        // Update status indicator
                        tgStatus.style.display = 'block';
                        tgStatus.style.background = 'var(--success-bg, #d4edda)';
                        tgStatus.style.color = 'var(--success, #155724)';
                        tgStatus.innerHTML = '<i class="fas fa-check-circle"></i> Config saved. Restart the bot process to apply changes.';

                        // Clear password field after save
                        tgPasswordInput.value = '';
                        tgTokenInput.value = '';
                    } catch (e) {
                        console.error('Telegram config save error:', e);
                        showNotification('Failed to save Telegram config', 'error');
                    } finally {
                        setButtonLoading(tgSaveBtn, false);
                    }
                };
            }

            applyTranslations();
        } catch (error) {
            console.error('Settings Error:', error);
        } finally {
            hideLoader();
        }
    }
};

// --- Helpers ---

function renderTemplate(templateId) {
    const template = document.getElementById(templateId);
    const clone = template.content.cloneNode(true);
    elements.viewContainer.innerHTML = '';
    elements.viewContainer.appendChild(clone);
    elements.viewContainer.classList.toggle('chat-mode', templateId === 'chat-template');
}

function applySidebarCollapsedState() {
    const appContainer = document.getElementById('app-container');
    if (!appContainer) return;

    const isCollapsed = safeStorageGet('sidebarCollapsed', '0') === '1';
    appContainer.classList.toggle('sidebar-collapsed', isCollapsed);
}

function toggleSidebarCollapsed() {
    const appContainer = document.getElementById('app-container');
    if (!appContainer) return;

    const next = !appContainer.classList.contains('sidebar-collapsed');
    appContainer.classList.toggle('sidebar-collapsed', next);
    safeStorageSet('sidebarCollapsed', next ? '1' : '0');
}

function showLoader() {
    const loader = document.createElement('div');
    loader.className = 'loader-container';
    loader.innerHTML = '<div class="loader"></div>';
    elements.viewContainer.appendChild(loader);
}

function hideLoader() {
    const loader = elements.viewContainer.querySelector('.loader-container');
    if (loader) loader.remove();
}

function interviewStorageKey(projectId) {
    return `interviewDraft:${projectId}`;
}

function buildInterviewDraftPayload() {
    return {
        summary: state.previousSummary,
        coverage: state.lastCoverage,
        signals: state.lastInterviewSignals,
        livePatch: state.lastLivePatch,
        cycleTrace: state.lastCycleTrace,
        topicNavigation: state.lastTopicNavigation,
        stage: state.interviewStage,
        mode: state.interviewMode,
        lastAssistantQuestion: state.lastAssistantQuestion,
        savedAt: new Date().toISOString(),
        lang: state.lang
    };
}

async function saveInterviewDraft(projectId) {
    if (!projectId) return;
    const payload = buildInterviewDraftPayload();
    safeStorageSet(interviewStorageKey(projectId), JSON.stringify(payload));
    state.interviewDraftMeta = payload;

    try {
        const response = await api.post(`/projects/${projectId}/interview/draft`, payload);
        if (response?.draft) {
            state.interviewDraftMeta = response.draft;
        }
    } catch (error) {
        console.warn('Failed to save interview draft on server, local draft kept.', error);
    }
}

function parseDraftDate(value) {
    if (!value) return 0;
    const ts = Date.parse(value);
    return Number.isNaN(ts) ? 0 : ts;
}

function pickLatestDraft(localDraft, serverDraft) {
    if (localDraft && !serverDraft) return localDraft;
    if (!localDraft && serverDraft) return serverDraft;
    if (!localDraft && !serverDraft) return null;

    const localTime = parseDraftDate(localDraft.savedAt);
    const serverTime = parseDraftDate(serverDraft.savedAt);
    return serverTime >= localTime ? serverDraft : localDraft;
}

function loadInterviewDraftLocal(projectId) {
    if (!projectId) return null;
    const raw = safeStorageGet(interviewStorageKey(projectId), null);
    if (!raw) return null;
    try {
        return JSON.parse(raw);
    } catch (error) {
        console.warn('Invalid local interview draft JSON, ignoring.', error);
        return null;
    }
}

function getProjectProgressInfo(project) {
    const draft = loadInterviewDraftLocal(project.id);
    const coverage = draft?.coverage || null;
    const progress = coverage ? getAverageCoverage(coverage) : 0;
    const done = progress >= 85;
    if (!draft) {
        return {
            progress: 0,
            label: state.lang === 'ar' ? 'Ù„Ù… ØªØ¨Ø¯Ø£ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©' : 'Interview not started'
        };
    }
    if (done) {
        return {
            progress: 100,
            label: state.lang === 'ar' ? 'SRS Ø¬Ø§Ù‡Ø² Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©' : 'SRS ready for review'
        };
    }
    return {
        progress,
        label: state.lang === 'ar' ? `Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ù‚ÙŠØ¯ Ø§Ù„ØªÙ‚Ø¯Ù… ${progress}%` : `Interview in progress ${progress}%`
    };
}

async function loadInterviewDraft(projectId) {
    if (!projectId) return null;

    const localDraft = loadInterviewDraftLocal(projectId);
    let serverDraft = null;

    try {
        const response = await api.get(`/projects/${projectId}/interview/draft`);
        serverDraft = response?.draft || null;
    } catch (error) {
        console.warn('Failed to load interview draft from server, using local fallback.', error);
    }

    const selected = pickLatestDraft(localDraft, serverDraft);
    if (selected) {
        safeStorageSet(interviewStorageKey(projectId), JSON.stringify(selected));
    }
    return selected;
}

async function clearInterviewDraft(projectId) {
    if (!projectId) return;
    safeStorageRemove(interviewStorageKey(projectId));
    try {
        await api.delete(`/projects/${projectId}/interview/draft`);
    } catch (error) {
        console.warn('Failed to clear interview draft on server.', error);
    }
}

function normalizeInterviewText(value) {
    return String(value || '')
        .trim()
        .toLowerCase()
        .replaceAll(/\s+/g, ' ')
        .replaceAll(/[?.!ØŒØ›:]+$/g, '');
}

function getAverageCoverage(coverage) {
    if (!coverage) return 0;
    const values = INTERVIEW_AREAS.map((area) => Number(coverage[area] || 0));
    const sum = values.reduce((acc, val) => acc + val, 0);
    return Math.round(sum / INTERVIEW_AREAS.length);
}

function parseSuggestedAnswers(rawSuggestions) {
    if (Array.isArray(rawSuggestions)) return rawSuggestions;

    if (typeof rawSuggestions === 'string') {
        const trimmed = rawSuggestions.trim();
        if (!trimmed) return [];

        const parsedList = parseSuggestedAnswersFromJson(trimmed);
        if (parsedList.length) return parsedList;

        return trimmed
            .split(/\n|â€¢|-\s+/)
            .map((line) => line.trim())
            .filter(Boolean);
    }

    if (rawSuggestions && typeof rawSuggestions === 'object') {
        return getSuggestionArray(rawSuggestions);
    }

    return [];
}

function getSuggestionArray(value) {
    if (!value || typeof value !== 'object') return [];
    if (Array.isArray(value.suggested_answers)) return value.suggested_answers;
    if (Array.isArray(value.options)) return value.options;
    if (Array.isArray(value.answers)) return value.answers;
    return [];
}

function parseSuggestedAnswersFromJson(trimmed) {
    try {
        const parsed = JSON.parse(trimmed);
        if (Array.isArray(parsed)) return parsed;
        return getSuggestionArray(parsed);
    } catch (error_) {
        console.warn('Failed to parse suggested answers payload.', error_);
        return [];
    }
}

function getQuestionAwareFallbackOptions(questionText = '', stage = 'discovery') {
    const q = normalizeInterviewText(questionText);
    const isAr = state.lang === 'ar';

    const byStage = {
        ar: {
            discovery: [
                'Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø£Ù†Ù†Ø§ Ù†Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø®Ø·ÙˆØ§Øª ÙŠØ¯ÙˆÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© ØªØ¤Ø®Ø± Ø§Ù„ØªÙ†ÙÙŠØ° ÙˆØªØ²ÙŠØ¯ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.',
                'Ø§Ù„Ø¬Ù…Ù‡ÙˆØ± Ø§Ù„Ù…ØªØ£Ø«Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù‡Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆÙØ±ÙŠÙ‚ Ø§Ù„ØªØ´ØºÙŠÙ„ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ù„Ø²Ù…Ù†.',
                'Ù‡Ø¯Ù Ø§Ù„Ù†Ø¬Ø§Ø­ Ù‡Ùˆ ØªÙ‚Ù„ÙŠÙ„ Ø²Ù…Ù† Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² ÙˆØ±ÙØ¹ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØµØ­ÙŠØ­ Ù…Ù† Ø£ÙˆÙ„ Ù…Ø±Ø©.'
            ],
            scope: [
                'ÙÙŠ MVP Ù†Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙŠ ØªØ¹Ø·ÙŠ Ù‚ÙŠÙ…Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù….',
                'Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø­Ø§Ù„ÙŠØ§Ù‹ Ø£ÙŠ ØªÙƒØ§Ù…Ù„Ø§Øª Ù…Ø¹Ù‚Ø¯Ø© Ø£Ùˆ Ø®ØµØ§Ø¦Øµ ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠØ© Ù„Ù„Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„Ø£ÙˆÙ„.',
                'Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ØªØ´Ù…Ù„ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ØªÙˆØ³Ø¹ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø£ÙˆÙ„.'
            ],
            users: [
                'Ø§Ù„Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù‡ÙŠ Ù…Ø³ØªØ®Ø¯Ù… Ù†Ù‡Ø§Ø¦ÙŠØŒ Ù…ÙˆØ¸Ù ØªØ´ØºÙŠÙ„ØŒ ÙˆÙ…Ø´Ø±Ù Ø¨ØµÙ„Ø§Ø­ÙŠØ§Øª Ø£ÙˆØ³Ø¹.',
                'Ù†Ø­ØªØ§Ø¬ ØµÙ„Ø§Ø­ÙŠØ§Øª ÙˆØ§Ø¶Ø­Ø© Ù„ÙƒÙ„ Ø¯ÙˆØ± Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ­ÙƒÙ… ÙˆØ§Ù„Ø£Ù…Ø§Ù† ÙˆØ¹Ø¯Ù… ØªØ¶Ø§Ø±Ø¨ Ø§Ù„Ù…Ù‡Ø§Ù….',
                'Ø±Ø­Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø© Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø­ØªÙ‰ Ø¥ØªÙ…Ø§Ù… Ø§Ù„Ø·Ù„Ø¨.'
            ],
            features: [
                'Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù‡ÙŠ Ø¥Ø¯Ø§Ø±Ø© Ø¯ÙˆØ±Ø© Ø§Ù„Ø·Ù„Ø¨ ÙƒØ§Ù…Ù„Ø© Ù…Ø¹ ØªØªØ¨Ø¹ ÙˆØ§Ø¶Ø­ Ù„Ù„Ø­Ø§Ù„Ø©.',
                'Ù…Ø¹ÙŠØ§Ø± Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ø£Ù† ØªÙ†ÙØ° Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ø¯ÙˆÙ† ØªØ¹Ù‚ÙŠØ¯ ÙˆØ¨Ø²Ù…Ù† Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù….',
                'ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠØ© Ù†Ø­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ© ÙˆØªØ³Ø¬ÙŠÙ„ ÙƒØ§Ù…Ù„ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡.'
            ],
            constraints: [
                'Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø¹Ù†Ø¯Ù†Ø§ Ù‡ÙŠ Ø§Ù„Ø²Ù…Ù†ØŒ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©ØŒ ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù†.',
                'Ù†Ø­ØªØ§Ø¬ Ø­Ù„ Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø¨Ø¯ÙˆÙ† ØªØ¹Ø·ÙŠÙ„ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù….',
                'Ù…Ù† Ø§Ù„Ù…Ù‡Ù… Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø§Ù…ØªØ«Ø§Ù„ ÙˆØªÙˆØ«ÙŠÙ‚ ÙƒÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©.'
            ]
        },
        en: {
            discovery: [
                'The current workflow relies on multiple manual steps that slow delivery and increase errors.',
                'Primary impacted groups are end users and operations staff, so speed and usability must improve.',
                'Success target is faster turnaround and higher first-pass completion quality.'
            ],
            scope: [
                'For MVP, we should prioritize core high-value flows needed for initial launch.',
                'Out of scope for now are complex integrations and non-essential enhancements.',
                'Phase two can include scalability and advanced capabilities after stability is proven.'
            ],
            users: [
                'Key roles are end user, operations agent, and supervisor with broader permissions.',
                'Role-based permissions are required to control access and reduce operational risk.',
                'The primary user journey should be short, clear, and easy to complete end to end.'
            ],
            features: [
                'The core feature is full workflow lifecycle management with clear status tracking.',
                'Acceptance criteria should confirm smooth flow completion with practical response-time targets.',
                'For edge cases, we need retries and reliable failure logging without data loss.'
            ],
            constraints: [
                'Main constraints are timeline, budget, and mandatory security controls.',
                'The solution should integrate with existing systems without disrupting current operations.',
                'Compliance requirements must be enforced with traceability for sensitive actions.'
            ]
        }
    };

    const intentFallback = isAr
        ? [
            {
                keys: ['Ù…Ù‚ÙŠØ§Ø³', 'Ù†Ø¬Ø§Ø­', 'kpi', 'Ù…Ø¤Ø´Ø±'], choices: [
                    'Ù…Ø¤Ø´Ø± Ø§Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: ØªÙ‚Ù„ÙŠÙ„ Ø²Ù…Ù† Ø§Ù„ØªÙ†ÙÙŠØ° ÙˆØ±ÙØ¹ Ø¯Ù‚Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ø¶Ù…Ù† ÙØªØ±Ø© Ø²Ù…Ù†ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©.',
                    'Ø³Ù†Ù‚ÙŠØ³ Ø§Ù„Ù†Ø¬Ø§Ø­ Ø¹Ø¨Ø± KPI Ø£Ø³Ø¨ÙˆØ¹ÙŠ ÙˆØ§Ø¶Ø­: Ø²Ù…Ù† Ø§Ù„Ø¯ÙˆØ±Ø©ØŒ Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ØŒ ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ù€SLA.',
                    'Ø§Ù„Ù‡Ø¯Ù Ø§Ù„ÙƒÙ…ÙŠ Ø§Ù„Ù…Ù‚ØªØ±Ø­: Ø®ÙØ¶ Ø²Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ±ÙØ¹ Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨ØµÙˆØ±Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù‚ÙŠØ§Ø³.'
                ]
            },
            {
                keys: ['Ù‚Ø¨ÙˆÙ„', 'Ù…Ø¹ÙŠØ§Ø±', 'acceptance'], choices: [
                    'Ù…Ø¹ÙŠØ§Ø± Ø§Ù„Ù‚Ø¨ÙˆÙ„: ØªÙ†ÙÙŠØ° Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙƒØ§Ù…Ù„Ù‹Ø§ Ø¨Ø¯ÙˆÙ† ØªØ¯Ø®Ù„ ÙŠØ¯ÙˆÙŠ ØºÙŠØ± Ù…Ø®Ø·Ø·.',
                    'ÙŠÙÙ‚Ø¨Ù„ Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø¥Ø°Ø§ Ø£Ù†Ù‡Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø¹Ø¯Ø¯ Ø®Ø·ÙˆØ§Øª Ù…Ø­Ø¯ÙˆØ¯ ÙˆØ¨Ù†Ø³Ø¨Ø© Ø£Ø®Ø·Ø§Ø¡ Ù…Ù†Ø®ÙØ¶Ø©.',
                    'Ø§Ù„Ù…Ø¹ÙŠØ§Ø± ÙŠØ´Ù…Ù„ ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©ØŒ ÙˆØªØªØ¨Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« ÙÙŠ Ø³Ø¬Ù„ Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚.'
                ]
            }
        ]
        : [
            {
                keys: ['metric', 'success', 'kpi'], choices: [
                    'Primary success metric is measurable cycle-time reduction with improved completion quality.',
                    'We should track weekly KPIs: cycle time, error rate, and SLA adherence.',
                    'A strong target is quantifiable speed and quality improvement within a defined period.'
                ]
            },
            {
                keys: ['acceptance', 'criteria'], choices: [
                    'Acceptance criteria should validate full end-to-end completion without unplanned manual workarounds.',
                    'A flow is accepted when users complete it in limited steps with low error rate.',
                    'Criteria must include data correctness, response targets, and audit traceability.'
                ]
            }
        ];

    const matchedIntent = intentFallback.find((entry) => entry.keys.some((key) => q.includes(normalizeInterviewText(key))));
    if (matchedIntent) return matchedIntent.choices;

    return (byStage[isAr ? 'ar' : 'en'][stage] || byStage[isAr ? 'ar' : 'en'].discovery);
}

function getInterviewAnswerOptions(suggestedAnswers = [], questionText = '', stage = 'discovery') {
    const questionTokens = normalizeInterviewText(questionText)
        .split(' ')
        .map((token) => token.trim())
        .filter((token) => token.length >= 3)
        .filter((token) => !['what', 'which', 'when', 'where', 'ÙƒÙŠÙ', 'Ø§ÙŠÙ‡', 'Ù…Ø§', 'Ù‡Ùˆ', 'Ù‡ÙŠ', 'Ø¹Ù„Ù‰', 'ÙÙŠ', 'Ù…Ù†'].includes(token));

    const isRelevantOption = (optionText) => {
        if (!questionTokens.length) return true;
        const normalized = normalizeInterviewText(optionText);
        if (!normalized) return false;
        const overlap = questionTokens.filter((token) => normalized.includes(token)).length;
        return overlap >= 1;
    };

    const parsed = parseSuggestedAnswers(suggestedAnswers);
    const cleaned = parsed
        .map((item) => String(item || '').trim())
        .filter(Boolean);

    const unique = [];
    const seen = new Set();
    cleaned.forEach((item) => {
        const key = normalizeInterviewText(item);
        if (!key || seen.has(key)) return;
        seen.add(key);
        unique.push(item);
    });

    const relevant = unique.filter(isRelevantOption);

    const coreChoices = relevant.length >= 2
        ? relevant.slice(0, 2)
        : getQuestionAwareFallbackOptions(questionText, stage).slice(0, 2);

    return [
        ...coreChoices,
        i18n[state.lang].interview_option_skip,
        i18n[state.lang].interview_option_unsure
    ];
}

function attachInterviewSelectToMessage(messageId, suggestedAnswers = [], questionText = '', stage = 'discovery') {
    if (!state.interviewMode) return;
    const msgDiv = document.getElementById(`msg-${messageId}`);
    if (!msgDiv) return;

    // Disable all previous selection wrappers
    document.querySelectorAll('.interview-answer-select-wrap').forEach((node) => {
        node.querySelectorAll('.suggestion-card').forEach(card => card.classList.add('disabled'));
        const btnEl = node.querySelector('button');
        if (btnEl) btnEl.disabled = true;
    });

    const options = getInterviewAnswerOptions(suggestedAnswers, questionText, stage);
    const wrapper = document.createElement('div');
    wrapper.className = 'interview-answer-select-wrap';

    // Build card chips
    const cardsHtml = options.map((opt, idx) => `
        <div class="suggestion-card" data-idx="${idx}" data-value="${escapeHtml(opt)}" role="radio" aria-checked="false" tabindex="0">
            <span class="suggestion-card-radio"></span>
            <span class="suggestion-card-text">${escapeHtml(opt)}</span>
        </div>
    `).join('');

    wrapper.innerHTML = `
        <div class="suggestion-cards-list" role="radiogroup" aria-label="${escapeHtml(i18n[state.lang].interview_select_hint)}">
            ${cardsHtml}
        </div>
        <button class="btn btn-secondary interview-mini-btn" disabled>${escapeHtml(i18n[state.lang].interview_select_send)}</button>
    `;

    const sendBtn = wrapper.querySelector('button');
    let selectedValue = null;

    wrapper.querySelectorAll('.suggestion-card').forEach(card => {
        const activate = () => {
            wrapper.querySelectorAll('.suggestion-card').forEach(c => {
                c.classList.remove('selected');
                c.setAttribute('aria-checked', 'false');
            });
            card.classList.add('selected');
            card.setAttribute('aria-checked', 'true');
            selectedValue = card.dataset.value;
            if (sendBtn) sendBtn.disabled = false;
        };
        card.onclick = activate;
        card.onkeydown = (e) => { if (e.key === 'Enter' || e.key === ' ') activate(); };
    });

    if (sendBtn) {
        sendBtn.onclick = () => {
            if (!selectedValue) return;
            const input = document.getElementById('chat-input');
            if (!input) return;
            state.pendingInterviewSelectionMeta = {
                interview_selection: true,
                source: 'suggested_answer',
                stage,
                question: questionText || ''
            };
            input.value = selectedValue;
            input.dispatchEvent(new Event('input', { bubbles: true }));
            handleChatSubmit();
            wrapper.querySelectorAll('.suggestion-card').forEach(c => c.classList.add('disabled'));
            sendBtn.disabled = true;
        };
    }

    msgDiv.querySelector('.msg-body')?.appendChild(wrapper);

    // Scroll chat to show the newly appended cards
    const container = document.getElementById('chat-messages');
    if (container) {
        requestAnimationFrame(() => {
            container.scrollTop = container.scrollHeight;
        });
    }
}

function updateInterviewAssistBar(coverage) {
    const assistBar = document.getElementById('interview-assist-bar');
    const reviewBtn = document.getElementById('interview-review-btn');
    if (!reviewBtn) return;

    if (!state.interviewMode) {
        if (assistBar) assistBar.style.display = 'none';
        return;
    }

    const avg = getAverageCoverage(coverage || state.lastCoverage || {});
    if (assistBar) assistBar.style.display = 'none';

    reviewBtn.disabled = avg < 60;
}

async function refreshInterviewTelemetry(projectId) {
    try {
        const telemetry = await api.get(`/projects/${projectId}/interview/telemetry`);
        if (!telemetry || typeof telemetry !== 'object') {
            state.lastInterviewTelemetry = null;
            return;
        }
        state.lastInterviewTelemetry = telemetry;
    } catch (error) {
        state.lastInterviewTelemetry = null;
        console.error('Interview Telemetry Refresh Error:', error);
    }
}

// --- Button Loading State ---
function setButtonLoading(btn, loading) {
    if (!btn) return;
    if (loading) {
        btn.dataset.originalHtml = btn.innerHTML;
        btn.classList.add('loading');
        btn.disabled = true;
    } else {
        btn.classList.remove('loading');
        btn.disabled = false;
        if (btn.dataset.originalHtml) {
            btn.innerHTML = btn.dataset.originalHtml;
            delete btn.dataset.originalHtml;
        }
    }
}

// --- Custom Confirm Dialog ---
function showConfirmDialog(message) {
    return new Promise((resolve) => {
        const overlay = document.getElementById('confirm-overlay');
        const msgEl = document.getElementById('confirm-message');
        const okBtn = document.getElementById('confirm-ok-btn');
        const cancelBtn = document.getElementById('confirm-cancel-btn');

        msgEl.textContent = message;
        okBtn.textContent = i18n[state.lang].confirm_yes;
        cancelBtn.textContent = i18n[state.lang].confirm_cancel;

        overlay.classList.remove('hidden');

        function cleanup() {
            overlay.classList.add('hidden');
            okBtn.onclick = null;
            cancelBtn.onclick = null;
            overlay.onclick = null;
            document.removeEventListener('keydown', handleEsc);
        }

        function handleEsc(e) {
            if (e.key === 'Escape') { cleanup(); resolve(false); }
        }

        okBtn.onclick = () => { cleanup(); resolve(true); };
        cancelBtn.onclick = () => { cleanup(); resolve(false); };
        overlay.onclick = (e) => {
            if (e.target === overlay) { cleanup(); resolve(false); }
        };
        document.addEventListener('keydown', handleEsc);
        cancelBtn.focus();
    });
}

// --- Form Validation ---
function showFieldError(input, message) {
    const group = input.closest('.form-group');
    if (!group) return;
    group.classList.add('has-error');
    let errorEl = group.querySelector('.field-error');
    if (!errorEl) {
        errorEl = document.createElement('div');
        errorEl.className = 'field-error';
        group.appendChild(errorEl);
    }
    errorEl.textContent = message;
}

function clearFieldError(input) {
    const group = input.closest('.form-group');
    if (!group) return;
    group.classList.remove('has-error');
    const errorEl = group.querySelector('.field-error');
    if (errorEl) errorEl.textContent = '';
}

function isValidEmail(email) {
    return /^[^\s@]+@[^\s@]+\.[^\s@]+$/.test(email);
}

function setupFieldValidation(input, validateFn) {
    input.addEventListener('blur', () => {
        const error = validateFn(input.value);
        if (error) showFieldError(input, error);
    });
    input.addEventListener('input', () => clearFieldError(input));
}

// --- Rate Limiter ---
const rateLimiter = {
    attempts: [],
    lockedUntil: null,
    maxAttempts: 5,
    windowMs: 60000,
    lockoutMs: 30000,

    recordFailure() {
        const now = Date.now();
        this.attempts.push(now);
        this.attempts = this.attempts.filter(t => now - t < this.windowMs);
        if (this.attempts.length >= this.maxAttempts) {
            this.lockedUntil = now + this.lockoutMs;
            this.attempts = [];
        }
    },

    reset() {
        this.attempts = [];
        this.lockedUntil = null;
    },

    isLocked() {
        if (!this.lockedUntil) return false;
        if (Date.now() > this.lockedUntil) {
            this.lockedUntil = null;
            return false;
        }
        return true;
    },

    getRemainingSeconds() {
        if (!this.lockedUntil) return 0;
        return Math.ceil((this.lockedUntil - Date.now()) / 1000);
    }
};

function startLockoutCountdown(errorEl) {
    const interval = setInterval(() => {
        if (!rateLimiter.isLocked()) {
            clearInterval(interval);
            errorEl.style.display = 'none';
            return;
        }
        const secs = rateLimiter.getRemainingSeconds();
        errorEl.textContent = i18n[state.lang].rate_limit_msg.replace('{seconds}', secs);
        errorEl.style.display = 'block';
    }, 1000);
}

// --- SRS Progress Stepper (3.1) ---

function showSrsProgress(currentStep) {
    const sectionsEl = document.getElementById('srs-sections');
    if (!sectionsEl) return;

    const isAr = state.lang === 'ar';
    const steps = isAr
        ? ['ØªØ­Ù„ÙŠÙ„ Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©', 'Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª', 'Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙ‚Ù†ÙŠØ©', 'Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø£Ø¹Ù…Ø§Ù„', 'ØªÙ†Ù‚ÙŠØ­ ÙˆØ¥ØªÙ…Ø§Ù… Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©']
        : ['Analyzing interview transcript', 'Structuring requirements', 'Technical review', 'Business review', 'Refining & finalizing'];

    const stepIcons = steps.map((label, idx) => {
        if (idx < currentStep) return `<span class="srs-step done"><i class="fas fa-check-circle"></i> ${label}</span>`;
        if (idx === currentStep) return `<span class="srs-step active"><i class="fas fa-spinner fa-spin"></i> ${label}</span>`;
        return `<span class="srs-step pending"><i class="far fa-circle"></i> ${label}</span>`;
    }).join('');

    sectionsEl.innerHTML = `
        <div class="srs-progress-stepper">
            <p class="srs-progress-title">${isAr ? 'Ø¬Ø§Ø±Ù Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³ØªÙ†Ø¯ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª...' : 'Generating your SRS...'}</p>
            <div class="srs-steps">${stepIcons}</div>
        </div>
    `;
}

function hideSrsProgress() {
    // Stepper will be overwritten naturally by renderSrsDraft or guidance banner
}

// --- SRS Quality-Gate Guidance Banner (3.3) ---

function showSrsGuidanceBanner(message) {
    const sectionsEl = document.getElementById('srs-sections');
    if (!sectionsEl) return;
    const isAr = state.lang === 'ar';
    sectionsEl.innerHTML = `
        <div class="srs-guidance-banner">
            <div class="srs-guidance-icon"><i class="fas fa-triangle-exclamation"></i></div>
            <div class="srs-guidance-body">
                <strong>${isAr ? 'Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± ÙƒØ§ÙÙ Ø¨Ø¹Ø¯' : 'Not enough content yet'}</strong>
                <p>${message}</p>
            </div>
            <div class="srs-guidance-actions">
                <button class="btn btn-primary btn-sm" id="srs-go-interview-btn">
                    ${isAr ? 'Ø§Ù„Ø°Ù‡Ø§Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©' : 'Go to Interview'}
                    <i class="fas fa-arrow-left" style="margin-${isAr ? 'right' : 'left'}:6px"></i>
                </button>
                <button class="btn btn-sm srs-guidance-dismiss" id="srs-guidance-dismiss-btn">${isAr ? 'ØªØ¬Ø§Ù‡Ù„' : 'Dismiss'}</button>
            </div>
        </div>
    `;
    const goBtn = document.getElementById('srs-go-interview-btn');
    if (goBtn) goBtn.onclick = () => switchView('chat');
    const dismissBtn = document.getElementById('srs-guidance-dismiss-btn');
    if (dismissBtn) dismissBtn.onclick = () => { sectionsEl.innerHTML = ''; };
}

async function loadSrsDraft(projectId, forceRefresh = false) {
    if (state.srsRefreshing) return;
    state.srsRefreshing = true;

    let progressTimers = [];

    try {
        let draft = await tryLoadExistingSrs(projectId, forceRefresh);
        if (!draft) {
            draft = await generateSrsDraftWithProgress(projectId);
        }

        renderSrsDraft(draft.content, draft);
    } catch (error) {
        clearSrsProgressTimers(progressTimers);
        progressTimers = [];
        console.error('SRS Load Error:', error);
        handleSrsLoadError(error);
    } finally {
        state.srsRefreshing = false;
    }
}

async function tryLoadExistingSrs(projectId, forceRefresh) {
    if (forceRefresh) return null;
    try {
        return await api.get(`/projects/${projectId}/srs`);
    } catch (error) {
        if (error.status !== 404) throw error;
        return null;
    }
}

async function generateSrsDraftWithProgress(projectId) {
    showSrsProgress(0);
    const progressTimers = scheduleSrsProgressTimers();
    try {
        return await api.post(`/projects/${projectId}/srs/refresh`, {
            language: state.lang
        });
    } finally {
        clearSrsProgressTimers(progressTimers);
        hideSrsProgress();
    }
}

function handleSrsLoadError(error) {
    const errMsg = error?.message || '';
    hideSrsProgress();

    if (isSrsInsufficientContentError(errMsg)) {
        showSrsGuidanceBanner(
            state.lang === 'ar'
                ? 'Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© ØªØ­ØªØ§Ø¬ Ù…Ø²ÙŠØ¯Ø§Ù‹ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„. Ø£ÙƒÙ…Ù„ 3â€“5 Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø«Ù… Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.'
                : 'The interview needs more detail. Complete 3â€“5 more turns, then try again.'
        );
        return;
    }

    renderSrsDraft(getFallbackSrsDraft(), null);
    showNotification(state.lang === 'ar' ? 'ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ÙˆØ¯Ø©' : 'Failed to load draft', 'error');
}

function scheduleSrsProgressTimers() {
    const timers = [];
    const stepDelays = [0, 3000, 8000, 14000, 20000];
    stepDelays.forEach((delay, step) => {
        const timerId = setTimeout(() => showSrsProgress(step), delay);
        timers.push(timerId);
    });
    return timers;
}

function clearSrsProgressTimers(timers) {
    timers.forEach(timerId => clearTimeout(timerId));
}

function isSrsInsufficientContentError(message) {
    const errMsg = String(message || '');
    return errMsg.toLowerCase().includes('insufficient content')
        || errMsg.includes('minimum of 80 words')
        || errMsg.includes('80 ÙƒÙ„Ù…Ø©');
}

function renderSrsDraft(content, draftMeta) {
    const draft = content || getFallbackSrsDraft();
    state.lastRenderedSrsDraft = { content: draft, meta: draftMeta || null };
    const statusEl = document.getElementById('srs-status');
    const updatedEl = document.getElementById('srs-updated');
    const summaryEl = document.getElementById('srs-summary-text');
    const metricsEl = document.getElementById('srs-metrics');
    const sectionsEl = document.getElementById('srs-sections');
    const questionsEl = document.getElementById('srs-open-questions');
    const nextStepsEl = document.getElementById('srs-next-steps');

    if (statusEl) statusEl.textContent = draft.status || (state.lang === 'ar' ? 'Ù…Ø³ÙˆØ¯Ø© Ø£ÙˆÙ„ÙŠØ©' : 'First Draft');
    if (updatedEl) {
        const fallbackTime = state.lang === 'ar' ? 'Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: Ø§Ù„Ø¢Ù†' : 'Last updated: now';
        const updatedPrefix = state.lang === 'ar' ? 'Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«:' : 'Last updated:';
        updatedEl.textContent = draftMeta?.created_at
            ? `${updatedPrefix} ${new Date(draftMeta.created_at).toLocaleString()}`
            : draft.updated || fallbackTime;
    }
    if (summaryEl) {
        summaryEl.textContent = draft.summary;
        summaryEl.setAttribute('dir', 'auto');
    }

    if (metricsEl) {
        metricsEl.innerHTML = (draft.metrics || [])
            .map(item => `
                <div class="metric-chip">
                    <span class="metric-label">${escapeHtml(item.label)}</span>
                    <span class="metric-value">${escapeHtml(item.value)}</span>
                </div>
            `)
            .join('');
    }

    if (sectionsEl) {
        const sections = draft.sections || [];
        const activityDiagram = Array.isArray(draft.activity_diagram) ? draft.activity_diagram : [];
        const activityMermaid = typeof draft.activity_diagram_mermaid === 'string'
            ? draft.activity_diagram_mermaid.trim()
            : '';
        const activityDiagrams = Array.isArray(draft.activity_diagrams) ? draft.activity_diagrams : [];
        const activityTitle = state.lang === 'ar' ? 'Ù…Ø®Ø·Ø· Ø§Ù„Ù†Ø´Ø§Ø·' : 'Activity Diagram';
        const textFlowLabel = state.lang === 'ar' ? 'Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù†ØµÙŠ' : 'Show text flow';
        const emptyDraftTitle = state.lang === 'ar' ? 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ÙˆØ¯Ø© Ø¨Ø¹Ø¯' : 'No draft yet';
        const emptyDraftDesc = state.lang === 'ar'
            ? 'Ø£Ø¬Ø±Ù Ù…Ù‚Ø§Ø¨Ù„Ø© Ù…Ø¹ Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ø°ÙƒÙŠ Ø«Ù… Ø§Ø¶ØºØ· ØªØ­Ø¯ÙŠØ«.'
            : 'Your SRS will appear here. Start an interview to generate it.';
        const startInterviewLabel = state.lang === 'ar' ? 'Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© â†' : 'Start Interview â†’';

        const activityDiagramHtml = activityDiagram.length
            ? `
                <article class="srs-section srs-activity-diagram">
                    <div class="srs-section-header">
                        <h3>${activityTitle}</h3>
                    </div>
                    <ul class="activity-diagram-list">
                        ${activityDiagram
                .map((line) => `<li dir="auto">${escapeHtml(String(line).replaceAll(/\s*->\s*/g, ' â†’ '))}</li>`)
                .join('')}
                    </ul>
                </article>
            `
            : '';

        const fallbackDiagramHtml = activityDiagram.length
            ? `
                        <details class="srs-mermaid-fallback">
                            <summary>${textFlowLabel}</summary>
                            <ul class="activity-diagram-list">
                                ${activityDiagram
                .map((line) => `<li dir="auto">${escapeHtml(String(line).replaceAll(/\s*->\s*/g, ' â†’ '))}</li>`)
                .join('')}
                            </ul>
                        </details>
                    `
            : '';

        const activityMermaidHtml = activityMermaid
            ? `
                <article class="srs-section srs-activity-diagram">
                    <div class="srs-section-header">
                        <h3>${activityTitle} (Mermaid)</h3>
                    </div>
                    <div class="srs-mermaid-surface" id="srs-mermaid-surface"></div>
                    ${fallbackDiagramHtml}
                </article>
            `
            : '';

        const multiActivityHtml = activityDiagrams
            .map((diagram, idx) => createActivityDiagramArticle(diagram, idx, activityTitle, textFlowLabel))
            .filter(Boolean)
            .join('');
        const hasMultiActivity = multiActivityHtml.length > 0;
        const activityHeaderHtml = hasMultiActivity ? multiActivityHtml : (activityMermaidHtml || activityDiagramHtml);
        const sectionsHtml = sections
            .map((section, idx) => `
                <article class="srs-section" data-confidence="${escapeHtml(section.confidence)}" data-idx="${idx}">
                    <div class="srs-section-header">
                        <h3>${escapeHtml(section.title)}</h3>
                        <div class="srs-section-actions">
                            <span class="confidence-badge">${escapeHtml(section.confidence)}</span>
                            <button class="srs-edit-btn" data-idx="${idx}" title="${state.lang === 'ar' ? 'ØªØ¹Ø¯ÙŠÙ„' : 'Edit'}">
                                <i class="fas fa-pen"></i>
                            </button>
                        </div>
                    </div>
                    <ul>
                        ${section.items.map((item, iIdx) => `<li data-section="${idx}" data-item="${iIdx}" dir="auto">${escapeHtml(item)}</li>`).join('')}
                    </ul>
                </article>
            `)
            .join('');

        sectionsEl.innerHTML = sections.length
            ? `${activityHeaderHtml}${sectionsHtml}`
            : `<div class="empty-state">
                    <div class="empty-state-icon"><i class="fas fa-file-circle-xmark"></i></div>
                    <h3>${emptyDraftTitle}</h3>
                    <p>${emptyDraftDesc}</p>
                    <button class="btn btn-primary mt-4" onclick="switchView('chat')">
                        <i class="fas fa-comments"></i>
                        <span>${startInterviewLabel}</span>
                    </button>
                </div>`;

        // Attach inline edit handlers
        attachSrsEditHandlers(sectionsEl);

        if (hasMultiActivity) {
            renderAllSrsMermaid(activityDiagrams);
        } else if (activityMermaidHtml) {
            renderSrsMermaid(activityMermaid);
        }
    }

    // --- User Stories ---
    const userStories = Array.isArray(draft.user_stories) ? draft.user_stories : [];
    let userStoriesContainer = document.getElementById('srs-user-stories');
    if (!userStoriesContainer && sectionsEl) {
        userStoriesContainer = document.createElement('div');
        userStoriesContainer.id = 'srs-user-stories';
        sectionsEl.parentNode.insertBefore(userStoriesContainer, sectionsEl.nextSibling);
    }
    if (userStoriesContainer) {
        if (userStories.length) {
            const storiesTitle = state.lang === 'ar' ? 'Ù‚ØµØµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆÙ…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„' : 'User Stories & Acceptance Criteria';
            const asLabel = state.lang === 'ar' ? 'Ø¨ÙˆØµÙÙŠ' : 'As a';
            const wantLabel = state.lang === 'ar' ? 'Ø£Ø±ÙŠØ¯' : 'I want to';
            const soLabel = state.lang === 'ar' ? 'Ø­ØªÙ‰ Ø£ØªÙ…ÙƒÙ† Ù…Ù†' : 'so that';
            const acLabel = state.lang === 'ar' ? 'Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„' : 'Acceptance Criteria';
            userStoriesContainer.innerHTML = `
                <article class="srs-section srs-user-stories">
                    <div class="srs-section-header"><h3>${storiesTitle}</h3></div>
                    ${userStories.map((s, i) => `
                        <div class="user-story-card">
                            <div class="user-story-statement" dir="auto">
                                <strong>${asLabel}</strong> ${escapeHtml(s.role || '')},
                                <strong>${wantLabel}</strong> ${escapeHtml(s.action || '')},
                                <strong>${soLabel}</strong> ${escapeHtml(s.goal || '')}
                            </div>
                            ${Array.isArray(s.acceptance_criteria) && s.acceptance_criteria.length ? `
                            <details class="acceptance-criteria">
                                <summary>${acLabel} (${s.acceptance_criteria.length})</summary>
                                <ul>${s.acceptance_criteria.map(ac => `<li dir="auto">âœ“ ${escapeHtml(ac)}</li>`).join('')}</ul>
                            </details>` : ''}
                        </div>
                    `).join('')}
                </article>`;
        } else {
            userStoriesContainer.innerHTML = '';
        }
    }

    // --- User Roles ---
    const userRoles = Array.isArray(draft.user_roles) ? draft.user_roles : [];
    let userRolesContainer = document.getElementById('srs-user-roles');
    if (!userRolesContainer && sectionsEl) {
        userRolesContainer = document.createElement('div');
        userRolesContainer.id = 'srs-user-roles';
        if (userStoriesContainer) {
            sectionsEl.parentNode.insertBefore(userRolesContainer, userStoriesContainer.nextSibling);
        } else {
            sectionsEl.parentNode.insertBefore(userRolesContainer, sectionsEl.nextSibling);
        }
    }
    if (userRolesContainer) {
        if (userRoles.length) {
            const rolesTitle = state.lang === 'ar' ? 'Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†' : 'User Roles';
            const permLabel = state.lang === 'ar' ? 'Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª' : 'Permissions';
            userRolesContainer.innerHTML = `
                <article class="srs-section srs-user-roles">
                    <div class="srs-section-header"><h3>${rolesTitle}</h3></div>
                    <table class="user-roles-table">
                        <thead><tr>
                            <th>${state.lang === 'ar' ? 'Ø§Ù„Ø¯ÙˆØ±' : 'Role'}</th>
                            <th>${state.lang === 'ar' ? 'Ø§Ù„ÙˆØµÙ' : 'Description'}</th>
                            <th>${permLabel}</th>
                        </tr></thead>
                        <tbody>
                            ${userRoles.map(r => `<tr>
                                <td dir="auto"><strong>${escapeHtml(r.role || '')}</strong></td>
                                <td dir="auto">${escapeHtml(r.description || '')}</td>
                                <td dir="auto">${Array.isArray(r.permissions) ? r.permissions.map(p => `<span class="perm-tag">${escapeHtml(p)}</span>`).join(' ') : ''}</td>
                            </tr>`).join('')}
                        </tbody>
                    </table>
                </article>`;
        } else {
            userRolesContainer.innerHTML = '';
        }
    }

    if (questionsEl) {
        questionsEl.innerHTML = (draft.questions || [])
            .map(item => `<li>${escapeHtml(item)}</li>`)
            .join('');
    }

    if (nextStepsEl) {
        nextStepsEl.innerHTML = (draft.nextSteps || draft.next_steps || [])
            .map(item => `<li>${escapeHtml(item)}</li>`)
            .join('');
    }
}

async function renderSrsMermaid(code) {
    const surface = document.getElementById('srs-mermaid-surface');
    if (!surface) return;

    const mermaidApi = globalThis.mermaid;
    if (!mermaidApi || typeof mermaidApi.render !== 'function') {
        surface.innerHTML = `<div class="srs-mermaid-error">${state.lang === 'ar' ? 'ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ù…ÙƒØªØ¨Ø© Mermaid.' : 'Failed to load Mermaid library.'}</div>`;
        return;
    }

    try {
        if (!globalThis.__tawasulMermaidInitialized) {
            mermaidApi.initialize({ startOnLoad: false, securityLevel: 'loose' });
            globalThis.__tawasulMermaidInitialized = true;
        }

        const graphId = `srs-mermaid-${Date.now()}`;
        const result = await mermaidApi.render(graphId, code);
        surface.innerHTML = result.svg || '';
    } catch (error) {
        console.error('Mermaid render error:', error);
        surface.innerHTML = `<div class="srs-mermaid-error">${state.lang === 'ar' ? 'ÙØ´Ù„ Ø±Ø³Ù… Ù…Ø®Ø·Ø· Mermaid. Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†ØµÙŠØ©.' : 'Failed to render Mermaid diagram. Text flow is shown instead.'}</div>`;
    }
}

function createActivityDiagramArticle(diagram, idx, activityTitle, textFlowLabel) {
    if (!diagram || typeof diagram !== 'object') return '';
    const title = String(diagram.title || `${activityTitle} ${idx + 1}`).trim();
    const lines = Array.isArray(diagram.activity_diagram)
        ? diagram.activity_diagram.map((line) => String(line).trim()).filter(Boolean)
        : [];
    const mermaid = typeof diagram.activity_diagram_mermaid === 'string'
        ? diagram.activity_diagram_mermaid.trim()
        : '';

    if (!lines.length && !mermaid) return '';

    const fallbackHtml = lines.length
        ? `
            <details class="srs-mermaid-fallback">
                <summary>${textFlowLabel}</summary>
                <ul class="activity-diagram-list">
                    ${lines.map((line) => `<li dir="auto">${escapeHtml(String(line).replaceAll(/\s*->\s*/g, ' â†’ '))}</li>`).join('')}
                </ul>
            </details>
        `
        : '';

    if (mermaid) {
        return `
            <article class="srs-section srs-activity-diagram">
                <div class="srs-section-header">
                    <h3>${escapeHtml(title)} (Mermaid)</h3>
                </div>
                <div class="srs-mermaid-surface" id="srs-mermaid-surface-${idx}"></div>
                ${fallbackHtml}
            </article>
        `;
    }

    return `
        <article class="srs-section srs-activity-diagram">
            <div class="srs-section-header">
                <h3>${escapeHtml(title)}</h3>
            </div>
            <ul class="activity-diagram-list">
                ${lines.map((line) => `<li dir="auto">${escapeHtml(String(line).replaceAll(/\s*->\s*/g, ' â†’ '))}</li>`).join('')}
            </ul>
        </article>
    `;
}

async function renderAllSrsMermaid(activityDiagrams) {
    if (!Array.isArray(activityDiagrams) || activityDiagrams.length === 0) return;

    const mermaidApi = globalThis.mermaid;
    if (!mermaidApi || typeof mermaidApi.render !== 'function') {
        document.querySelectorAll('[id^="srs-mermaid-surface-"]').forEach((surface) => {
            surface.innerHTML = `<div class="srs-mermaid-error">${state.lang === 'ar' ? 'ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ù…ÙƒØªØ¨Ø© Mermaid.' : 'Failed to load Mermaid library.'}</div>`;
        });
        return;
    }

    try {
        if (!globalThis.__tawasulMermaidInitialized) {
            mermaidApi.initialize({ startOnLoad: false, securityLevel: 'loose' });
            globalThis.__tawasulMermaidInitialized = true;
        }
    } catch (error) {
        console.error('Mermaid init error:', error);
    }

    for (let idx = 0; idx < activityDiagrams.length; idx += 1) {
        await renderSingleSrsMermaidByIndex(mermaidApi, activityDiagrams[idx], idx);
    }
}

async function renderSingleSrsMermaidByIndex(mermaidApi, diagram, idx) {
    const code = typeof diagram?.activity_diagram_mermaid === 'string' ? diagram.activity_diagram_mermaid.trim() : '';
    if (!code) return;

    const surface = document.getElementById(`srs-mermaid-surface-${idx}`);
    if (!surface) return;

    try {
        const graphId = `srs-mermaid-${Date.now()}-${idx}`;
        const result = await mermaidApi.render(graphId, code);
        surface.innerHTML = result.svg || '';
    } catch (error) {
        console.error('Mermaid render error:', error);
        surface.innerHTML = `<div class="srs-mermaid-error">${state.lang === 'ar' ? 'ÙØ´Ù„ Ø±Ø³Ù… Ù…Ø®Ø·Ø· Mermaid. Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†ØµÙŠØ©.' : 'Failed to render Mermaid diagram. Text flow is shown instead.'}</div>`;
    }
}

function attachSrsEditHandlers(sectionsEl) {
    sectionsEl.querySelectorAll('.srs-edit-btn').forEach(btn => {
        btn.onclick = (e) => {
            e.stopPropagation();
            const sectionIdx = Number.parseInt(btn.dataset.idx, 10);
            const article = sectionsEl.querySelector(`[data-idx="${sectionIdx}"]`);
            if (!article) return;
            const ul = article.querySelector('ul');
            if (!ul || ul.classList.contains('editing')) return;

            ul.classList.add('editing');
            const items = ul.querySelectorAll('li');
            for (const li of items) {
                li.contentEditable = 'true';
                li.classList.add('editable');
            }

            // Change button to save
            btn.innerHTML = `<i class="fas fa-check"></i>`;
            btn.title = state.lang === 'ar' ? 'Ø­ÙØ¸' : 'Save';
            btn.classList.add('saving');

            btn.onclick = () => {
                for (const li of items) {
                    li.contentEditable = 'false';
                    li.classList.remove('editable');
                }
                ul.classList.remove('editing');
                btn.innerHTML = `<i class="fas fa-pen"></i>`;
                btn.title = state.lang === 'ar' ? 'ØªØ¹Ø¯ÙŠÙ„' : 'Edit';
                btn.classList.remove('saving');
                showNotification(state.lang === 'ar' ? 'ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ù…Ø­Ù„ÙŠØ§Ù‹' : 'Changes saved locally', 'success');

                // Re-attach the edit handler properly (no recursive btn.click)
                attachSrsEditHandlers(sectionsEl);
            };
        };
    });
}

function updateExportButtonLabel(format = 'pdf') {
    const exportLabel = document.querySelector('#srs-export-btn span');
    if (!exportLabel) return;
    if (format === 'word') {
        exportLabel.textContent = i18n[state.lang].srs_export_word;
    } else if (format === 'markdown') {
        exportLabel.textContent = i18n[state.lang].srs_export_markdown;
    } else {
        exportLabel.textContent = i18n[state.lang].srs_export_pdf;
    }
}

function toMarkdownFromSrs(draftContent, draftMeta) {
    const content = draftContent || {};
    const lines = [];
    lines.push(
        `# ${state.lang === 'ar' ? 'ÙˆØ«ÙŠÙ‚Ø© Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ§Øª' : 'Software Requirements Specification'}`,
        '',
        `- ${state.lang === 'ar' ? 'Ø§Ù„Ù…Ø´Ø±ÙˆØ¹' : 'Project'}: ${draftMeta?.project_id || '-'}`,
        `- ${state.lang === 'ar' ? 'Ø§Ù„Ø¥ØµØ¯Ø§Ø±' : 'Version'}: ${draftMeta?.version || 1}`,
        `- ${state.lang === 'ar' ? 'Ø§Ù„Ø­Ø§Ù„Ø©' : 'Status'}: ${content.status || 'draft'}`,
        ''
    );

    appendSummaryMarkdown(lines, content.summary);

    const sections = Array.isArray(content.sections) ? content.sections : [];
    sections.forEach((section) => appendMarkdownSection(lines, section));

    const activityDiagram = Array.isArray(content.activity_diagram) ? content.activity_diagram : [];
    appendMarkdownListBlock(
        lines,
        state.lang === 'ar' ? 'Ù…Ø®Ø·Ø· Ø§Ù„Ù†Ø´Ø§Ø·' : 'Activity Diagram',
        activityDiagram.map((line) => String(line).replaceAll(/\s*->\s*/g, ' â†’ '))
    );

    const activityDiagrams = Array.isArray(content.activity_diagrams) ? content.activity_diagrams : [];
    if (activityDiagrams.length) {
        activityDiagrams.forEach((diagram, idx) => {
            const title = String(diagram?.title || (state.lang === 'ar' ? `ØªØ¯ÙÙ‚ Ù†Ø´Ø§Ø· ${idx + 1}` : `Activity Flow ${idx + 1}`)).trim();
            const flowLines = Array.isArray(diagram?.activity_diagram)
                ? diagram.activity_diagram.map((line) => String(line).replaceAll(/\s*->\s*/g, ' â†’ '))
                : [];
            appendMarkdownListBlock(lines, title, flowLines);

            const mermaidCode = typeof diagram?.activity_diagram_mermaid === 'string'
                ? diagram.activity_diagram_mermaid.trim()
                : '';
            if (mermaidCode) {
                lines.push(`### ${title} (Mermaid)`, '```mermaid', mermaidCode, '```', '');
            }
        });
    }

    const activityMermaid = typeof content.activity_diagram_mermaid === 'string'
        ? content.activity_diagram_mermaid.trim()
        : '';
    appendMermaidMarkdown(lines, activityMermaid);

    const questions = Array.isArray(content.questions) ? content.questions : [];
    appendMarkdownListBlock(lines, state.lang === 'ar' ? 'Ù†Ù‚Ø§Ø· ØªØ­ØªØ§Ø¬ ØªÙˆØ¶ÙŠØ­' : 'Open Questions', questions);

    // User Stories
    const userStories = Array.isArray(content.user_stories) ? content.user_stories : [];
    if (userStories.length) {
        lines.push(`## ${state.lang === 'ar' ? 'Ù‚ØµØµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆÙ…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„' : 'User Stories & Acceptance Criteria'}`);
        userStories.forEach(s => {
            const asA = state.lang === 'ar' ? 'Ø¨ÙˆØµÙÙŠ' : 'As a';
            const iWant = state.lang === 'ar' ? 'Ø£Ø±ÙŠØ¯' : 'I want to';
            const soThat = state.lang === 'ar' ? 'Ø­ØªÙ‰ Ø£ØªÙ…ÙƒÙ† Ù…Ù†' : 'so that';
            lines.push(`- **${asA}** ${s.role || ''}, **${iWant}** ${s.action || ''}, **${soThat}** ${s.goal || ''}`);
            (s.acceptance_criteria || []).forEach(ac => lines.push(`  - âœ“ ${ac}`));
        });
        lines.push('');
    }

    // User Roles
    const userRoles = Array.isArray(content.user_roles) ? content.user_roles : [];
    if (userRoles.length) {
        lines.push(`## ${state.lang === 'ar' ? 'Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†' : 'User Roles'}`);
        userRoles.forEach(r => {
            const perms = Array.isArray(r.permissions) && r.permissions.length
                ? ` â€” ${r.permissions.join(', ')}`
                : '';
            lines.push(`- **${r.role || ''}**: ${r.description || ''}${perms}`);
        });
        lines.push('');
    }

    const nextSteps = Array.isArray(content.nextSteps || content.next_steps) ? (content.nextSteps || content.next_steps) : [];
    appendMarkdownListBlock(lines, state.lang === 'ar' ? 'Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©' : 'Next Steps', nextSteps);

    lines.push('---', 'Generated intelligently by Tawasul AI');
    return lines.join('\n');
}

function appendMarkdownSection(lines, section) {
    const title = section?.title || (state.lang === 'ar' ? 'Ù‚Ø³Ù…' : 'Section');
    const confidence = section?.confidence ? ` (${section.confidence})` : '';
    const items = Array.isArray(section?.items) ? section.items : [];
    appendMarkdownListBlock(lines, `${title}${confidence}`, items);
}

function appendSummaryMarkdown(lines, summary) {
    if (!summary) return;
    lines.push(`## ${state.lang === 'ar' ? 'Ø§Ù„Ù…Ù„Ø®Øµ' : 'Summary'}`, String(summary), '');
}

function appendMermaidMarkdown(lines, mermaidCode) {
    if (!mermaidCode) return;
    lines.push('```mermaid', mermaidCode, '```', '');
}

function appendMarkdownListBlock(lines, heading, items) {
    if (!items.length) return;
    lines.push(`## ${heading}`);
    items.forEach((item) => lines.push(`- ${item}`));
    lines.push('');
}

function downloadTextBlob(filename, mimeType, text) {
    const blob = new Blob([text], { type: mimeType });
    const url = globalThis.URL.createObjectURL(blob);
    const link = document.createElement('a');
    link.href = url;
    link.download = filename;
    document.body.appendChild(link);
    link.click();
    link.remove();
    globalThis.URL.revokeObjectURL(url);
}

async function exportSrsDocument(projectId, format = 'pdf') {
    if (format === 'pdf') {
        await downloadSrsPdf(projectId);
        return;
    }

    const draftBundle = state.lastRenderedSrsDraft;
    if (!draftBundle?.content) {
        showNotification(state.lang === 'ar' ? 'Ø­Ø¯Ù‘Ø« Ø§Ù„Ù…Ø³ÙˆØ¯Ø© Ø£ÙˆÙ„Ø§Ù‹ Ø«Ù… Ø£Ø¹Ø¯ Ø§Ù„ØªØµØ¯ÙŠØ±.' : 'Refresh draft first, then export again.', 'warning');
        return;
    }

    const markdown = toMarkdownFromSrs(draftBundle.content, draftBundle.meta || {});
    if (format === 'word') {
        const mermaidCode = typeof draftBundle?.content?.activity_diagram_mermaid === 'string'
            ? draftBundle.content.activity_diagram_mermaid.trim()
            : '';
        const mermaidSvg = mermaidCode ? await renderMermaidForExport(mermaidCode) : '';
        const mermaidTitle = state.lang === 'ar' ? 'Ù…Ø®Ø·Ø· Ø§Ù„Ù†Ø´Ø§Ø· (Mermaid)' : 'Activity Diagram (Mermaid)';
        const mermaidBody = mermaidSvg ? `<div>${mermaidSvg}</div>` : `<pre>${escapeHtml(mermaidCode)}</pre>`;
        const mermaidBlock = mermaidCode
            ? `
                <h3>${escapeHtml(mermaidTitle)}</h3>
                ${mermaidBody}
            `
            : '';

        const htmlDoc = `<!doctype html><html><head><meta charset="utf-8"></head><body>${mermaidBlock}<pre>${escapeHtml(markdown)}</pre><p>Generated intelligently by Tawasul AI</p></body></html>`;
        downloadTextBlob(`srs_project_${projectId}.doc`, 'application/msword', htmlDoc);
        return;
    }
    downloadTextBlob(`srs_project_${projectId}.md`, 'text/markdown;charset=utf-8', markdown);
}

async function renderMermaidForExport(code) {
    const mermaidApi = globalThis.mermaid;
    if (!mermaidApi || typeof mermaidApi.render !== 'function') {
        return '';
    }

    try {
        if (!globalThis.__tawasulMermaidInitialized) {
            mermaidApi.initialize({ startOnLoad: false, securityLevel: 'loose' });
            globalThis.__tawasulMermaidInitialized = true;
        }
        const graphId = `srs-export-mermaid-${Date.now()}`;
        const result = await mermaidApi.render(graphId, code);
        return result?.svg || '';
    } catch (error) {
        console.error('Mermaid export render error:', error);
        return '';
    }
}

async function downloadSrsPdf(projectId) {
    try {
        const response = await fetch(`${API_BASE_URL}/projects/${projectId}/srs/export`, {
            headers: authHeaders()
        });
        await throwIfNotOk(response);
        const blob = await response.blob();
        const url = globalThis.URL.createObjectURL(blob);
        const link = document.createElement('a');
        link.href = url;
        link.download = `srs_project_${projectId}.pdf`;
        document.body.appendChild(link);
        link.click();
        link.remove();
        globalThis.URL.revokeObjectURL(url);
    } catch (error) {
        console.error('SRS Export Error:', error);
        showNotification(state.lang === 'ar' ? 'ØªØ¹Ø°Ø± ØªØµØ¯ÙŠØ± SRS' : 'Failed to export SRS', 'error');
    }
}

async function logChatMessages(projectId, userText, assistantText, sources, userMetadata = null) {
    try {
        await api.post(`/projects/${projectId}/messages`, {
            messages: [
                { role: 'user', content: userText, metadata: userMetadata || undefined },
                { role: 'assistant', content: assistantText, metadata: { sources: sources || [] } }
            ]
        });
        return true;
    } catch (error) {
        console.error('Log Messages Error:', error);
        return false;
    }
}

async function refreshLivePatchPanel(projectId, language) {
    try {
        const patchResult = await api.post(`/projects/${projectId}/messages/live-patch`, {
            language,
            last_summary: state.previousSummary || null,
            last_coverage: state.lastCoverage || null
        });
        if (!patchResult?.summary) return;

        state.previousSummary = patchResult.summary || state.previousSummary;
        state.lastCoverage = patchResult.coverage || state.lastCoverage;
        state.lastInterviewSignals = patchResult.signals || null;
        state.lastLivePatch = patchResult.live_patch || null;
        state.lastCycleTrace = patchResult.cycle_trace || null;
        state.lastTopicNavigation = patchResult.topic_navigation || null;
        state.interviewStage = patchResult.stage || state.interviewStage;

        const liveDocPanel = document.getElementById('live-doc-panel');
        if (liveDocPanel) {
            applySummaryDrawerState();
        }

        updateLiveDoc(patchResult.summary, patchResult.stage || 'discovery');
    } catch (error) {
        console.error('Live Patch Refresh Error:', error);
        showNotification(error?.message || (state.lang === 'ar' ? 'ØªØ¹Ø°Ø± ØªØ­Ø¯ÙŠØ« Ù„ÙˆØ­Ø© Ø§Ù„ØªÙ‚Ø¯Ù…' : 'Failed to refresh live patch'), 'warning');
    }
}

function getProjectNameById(projectId) {
    const project = (state.projects || []).find((item) => Number(item.id) === Number(projectId));
    return project?.name || '';
}

function updateChatProjectHeader(projectId) {
    const titleEl = document.getElementById('chat-project-title');
    if (!titleEl) return;

    const projectName = projectId ? getProjectNameById(projectId) : '';
    const fallback = state.lang === 'ar' ? 'Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: -' : 'Project: -';
    const projectLabel = state.lang === 'ar' ? `Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: ${projectName}` : `Project: ${projectName}`;
    titleEl.textContent = projectName
        ? projectLabel
        : fallback;
}

function applySummaryDrawerState() {
    const panel = document.getElementById('live-doc-panel');
    const summaryBtn = document.getElementById('summary-toggle-btn');
    if (!panel) return;

    const shouldShow = state.interviewMode && !state.summaryCollapsed;
    panel.classList.toggle('is-collapsed', !shouldShow);
    panel.style.display = shouldShow ? 'flex' : 'none';

    if (summaryBtn) {
        summaryBtn.classList.toggle('active', shouldShow);
        summaryBtn.setAttribute('aria-expanded', shouldShow ? 'true' : 'false');
    }
}

function getInterviewStarterPrompts(projectId) {
    const projectName = getProjectNameById(projectId);
    if (state.lang === 'ar') {
        return [
            `Ù…Ø§ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØ­Ù„Ù‡Ø§ ${projectName || 'Ø§Ù„Ù…Ø´Ø±ÙˆØ¹'}ØŸ`,
            `Ù…ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠÙŠÙ† ÙÙŠ ${projectName || 'Ø§Ù„Ù…Ø´Ø±ÙˆØ¹'} ÙˆØ¥ÙŠÙ‡ Ø£ÙˆÙ„ÙˆÙŠØ§ØªÙ‡Ù…ØŸ`,
            `Ù…Ø§ Ø£Ù‡Ù… 3 Ù…ØªØ·Ù„Ø¨Ø§Øª MVP ÙˆØ§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„Ø²Ù…Ù†ÙŠØ©/Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©ØŸ`
        ];
    }

    return [
        `What core problem does ${projectName || 'this project'} solve?`,
        `Who are the primary users of ${projectName || 'this project'} and what matters most to them?`,
        'What are the top 3 MVP requirements and timeline/budget constraints?'
    ];
}

function getChatWelcomeMarkup(projectId) {
    const prompts = getInterviewStarterPrompts(projectId);
    const title = state.lang === 'ar'
        ? 'Ù„Ù†Ø¨Ø¯Ø£ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª'
        : 'Letâ€™s start requirements discovery';
    const subtitle = state.lang === 'ar'
        ? 'Ø§ÙƒØªØ¨ ØªÙØ§ØµÙŠÙ„ Ù…Ø´Ø±ÙˆØ¹ÙƒØŒ ÙˆØ³Ø£Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù…Ø±ØªØ¨Ø·Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ø¨Ù†Ø§Ø¡ SRS Ø§Ø­ØªØ±Ø§ÙÙŠ.'
        : 'Share your project details and I will ask focused questions to build a professional SRS.';

    return `
        <div class="welcome-msg-pro">
            <div class="welcome-icon">
                <i class="fas fa-robot"></i>
            </div>
            <h2>${title}</h2>
            <p>${subtitle}</p>
            <div class="welcome-suggestions">
                ${prompts.map((prompt) => `<button class="suggestion-chip">${escapeHtml(prompt)}</button>`).join('')}
            </div>
        </div>
    `;
}

function bindSuggestionChips() {
    const chatInput = document.getElementById('chat-input');
    if (!chatInput) return;
    document.querySelectorAll('.suggestion-chip').forEach((chip) => {
        chip.onclick = () => {
            chatInput.value = chip.textContent;
            chatInput.dispatchEvent(new Event('input', { bubbles: true }));
            chatInput.focus();
        };
    });
}

async function loadChatHistory(projectId) {
    const messagesContainer = document.getElementById('chat-messages');
    if (!messagesContainer) return;
    updateChatProjectHeader(projectId);

    // Show loading state
    messagesContainer.innerHTML = `
        <div class="welcome-msg-pro">
            <div class="welcome-icon">
                <i class="fas fa-spinner fa-spin"></i>
            </div>
            <p>${state.lang === 'ar' ? 'Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©...' : 'Loading conversation...'}</p>
        </div>
    `;

    try {
        const messages = await api.get(`/projects/${projectId}/messages?limit=120`);

        // Clear container
        messagesContainer.innerHTML = '';

        if (!messages || messages.length === 0) {
            // Show welcome message if no history
            messagesContainer.innerHTML = getChatWelcomeMarkup(projectId);
            bindSuggestionChips();
            await refreshInterviewTelemetry(projectId);
            updateInterviewAssistBar(state.lastCoverage);
            return;
        }

        // Render each message from history
        for (const msg of messages) {
            const role = msg.role === 'user' ? 'user' : 'bot';
            const id = addChatMessage(role, msg.role === 'user' ? msg.content : '', false);

            // For assistant messages, render formatted HTML
            if (msg.role === 'assistant') {
                const textEl = document.querySelector(`#msg-${id} .msg-text`);
                if (textEl) {
                    textEl.innerHTML = formatAnswerHtml(msg.content) || escapeHtml(msg.content);
                    textEl.dir = detectTextDirection(msg.content);
                }

                // Render sources if available in metadata
                if (msg.metadata?.sources?.length > 0) {
                    const msgDiv = document.getElementById(`msg-${id}`);
                    const sourcesDiv = document.createElement('div');
                    sourcesDiv.className = 'msg-sources-pro';
                    sourcesDiv.innerHTML = `
                        <div class="sources-header">
                            <i class="fas fa-book-open"></i>
                            <span>${state.lang === 'ar' ? 'Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©' : 'Sources Used'}</span>
                        </div>
                    `;
                    const list = document.createElement('ul');
                    msg.metadata.sources.slice(0, 5).forEach(s => {
                        const li = document.createElement('li');
                        li.innerHTML = `
                            <i class="fas fa-file-alt"></i>
                            <span>${escapeHtml(s.document_name || s.name || '')}</span>
                            ${s.similarity ? `<span class="source-score">${(s.similarity * 100).toFixed(0)}%</span>` : ''}
                        `;
                        list.appendChild(li);
                    });
                    sourcesDiv.appendChild(list);
                    msgDiv.querySelector('.msg-body').appendChild(sourcesDiv);
                }
            }
        }

        // Scroll to bottom
        messagesContainer.scrollTop = messagesContainer.scrollHeight;
        if (messages.length > 0) {
            await refreshLivePatchPanel(projectId, state.lang);
        }
        await refreshInterviewTelemetry(projectId);
        updateInterviewAssistBar(state.lastCoverage);
    } catch (error) {
        console.error('Load Chat History Error:', error);
        showNotification(error?.message || (state.lang === 'ar' ? 'ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©' : 'Failed to load conversation'), 'error');
        messagesContainer.innerHTML = getChatWelcomeMarkup(projectId);
        bindSuggestionChips();
    }
}

function createProjectCard(project) {
    const card = document.createElement('div');
    card.className = 'project-card';
    const docCount = project.document_count || 0;
    const progressInfo = getProjectProgressInfo(project);
    card.innerHTML = `
        <h3>${escapeHtml(project.name)}</h3>
        <p>${escapeHtml(project.description || i18n[state.lang].project_goal_hint)}</p>
        <div class="project-progress-wrap" title="${escapeHtml(progressInfo.label)}">
            <div class="project-progress-head">
                <span>${escapeHtml(progressInfo.label)}</span>
                <strong>${Math.round(progressInfo.progress)}%</strong>
            </div>
            <div class="project-progress-track">
                <div class="project-progress-fill" style="width: ${Math.round(progressInfo.progress)}%"></div>
            </div>
        </div>
        <div class="project-card-footer">
            <span class="project-card-date"><i class="far fa-calendar"></i> ${new Date(project.created_at).toLocaleDateString(state.lang === 'ar' ? 'ar-EG' : 'en-US')}</span>
            <div class="project-card-actions">
                <span class="doc-count-badge"><i class="fas fa-file-alt"></i> ${docCount}</span>
                <button class="delete-project-btn icon-btn" style="width: 30px; height: 30px; color: var(--error);" data-id="${project.id}" title="${state.lang === 'ar' ? 'Ø­Ø°Ù Ø§Ù„Ù…Ø´Ø±ÙˆØ¹' : 'Delete Project'}">
                    <i class="fas fa-trash-alt"></i>
                </button>
            </div>
        </div>
        <div class="project-card-quick-actions">
            <button class="btn btn-primary btn-sm project-interview-btn">
                <i class="fas fa-comments"></i>
                <span>${state.lang === 'ar' ? 'Ù…Ù‚Ø§Ø¨Ù„Ø©' : 'Interview'}</span>
            </button>
            <button class="btn btn-outline btn-sm project-srs-btn">
                <i class="fas fa-file-signature"></i>
                <span>${state.lang === 'ar' ? 'Ø¹Ø±Ø¶ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª' : 'View SRS'}</span>
            </button>
        </div>
    `;

    card.onclick = (e) => {
        if (e.target.closest('.delete-project-btn')) {
            e.stopPropagation();
            handleDeleteProject(project.id, project.name);
            return;
        }

        if (e.target.closest('.project-interview-btn')) {
            state.interviewMode = true;
            state.pendingProjectSelect = project.id;
            switchView('chat');
            return;
        }
        if (e.target.closest('.project-srs-btn')) {
            state.pendingProjectSelect = project.id;
            switchView('srs');
            return;
        }
        // Default card click: open interview
        state.interviewMode = true;
        state.pendingProjectSelect = project.id;
        switchView('chat');
    };

    return card;
}

function createDocItem(doc) {
    const item = document.createElement('div');
    item.className = 'doc-item';
    let statusClass = 'status-processing';
    let statusIcon = 'fa-spinner fa-spin';
    if (doc.status === 'completed') {
        statusClass = 'status-done';
        statusIcon = 'fa-check-circle';
    } else if (doc.status === 'failed') {
        statusClass = 'status-error';
        statusIcon = 'fa-exclamation-circle';
    }
    const meta = doc.extra_metadata || {};
    const totalChunks = Number.isFinite(meta.total_chunks) ? meta.total_chunks : null;
    const processedChunks = Number.isFinite(meta.processed_chunks) ? meta.processed_chunks : null;
    const progressValue = Number.isFinite(meta.progress) ? meta.progress : null;
    const showProgress = doc.status === 'processing' && totalChunks && totalChunks > 0;
    let progressPercent;
    if (progressValue == null) {
        progressPercent = Math.round((processedChunks || 0) / totalChunks * 100);
    } else {
        progressPercent = Math.max(0, Math.min(100, progressValue));
    }

    const statusText = {
        completed: state.lang === 'ar' ? 'Ù…ÙƒØªÙ…Ù„' : 'Completed',
        failed: state.lang === 'ar' ? 'ÙØ´Ù„' : 'Failed',
        processing: state.lang === 'ar' ? 'Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©' : 'Processing'
    };

    item.innerHTML = `
        <div class="doc-info">
            <i class="fas fa-file-pdf"></i>
            <div class="doc-details">
                <span class="doc-name">${escapeHtml(doc.original_filename)}</span>
                <span class="doc-size">${(doc.file_size / 1024).toFixed(1)} KB</span>
            </div>
        </div>
        <div class="doc-status ${statusClass}">
            <i class="fas ${statusIcon}"></i>
            <span>${statusText[doc.status] || doc.status}</span>
        </div>
        ${showProgress ? `
            <div class="doc-progress">
                <div class="doc-progress-header">
                    <span>${i18n[state.lang].processing_label}</span>
                    <span>${processedChunks || 0}/${totalChunks}</span>
                </div>
                <div class="doc-progress-track">
                    <div class="doc-progress-bar" style="width: ${progressPercent}%;"></div>
                </div>
            </div>
        ` : ''}
    `;

    return item;
}

function renderDocsList(docs) {
    const docsList = document.getElementById('project-docs-list');
    if (!docsList) return;
    docsList.innerHTML = '';

    if (docs.length === 0) {
        docsList.innerHTML = `
            <div class="empty-state" style="padding: 24px 0;">
                <div class="empty-state-icon"><i class="fas fa-file-circle-plus"></i></div>
                <h3>${state.lang === 'ar' ? 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª Ø¨Ø¹Ø¯' : 'No files yet'}</h3>
                <p>${state.lang === 'ar' ? 'Ø§Ø±ÙØ¹ Ù…Ø³ØªÙ†Ø¯ Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ø£Ùˆ Ù…Ù„Ù Ù…Ø±Ø¬Ø¹ÙŠ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ' : 'Upload an interview transcript or reference document to help the AI'}</p>
            </div>
        `;
        return;
    }

    docs.forEach(doc => {
        docsList.appendChild(createDocItem(doc));
    });
}

function startDocPolling(projectId, docs) {
    if (state.docPoller) {
        clearTimeout(state.docPoller);
        state.docPoller = null;
    }

    const hasProcessing = docs.some(doc => doc.status === 'processing');
    if (!hasProcessing) return;

    const baseDelayMs = 10000;
    const maxDelayMs = 60000;
    let attempt = 0;

    const pollOnce = async () => {
        if (state.currentView !== 'chat') {
            clearTimeout(state.docPoller);
            state.docPoller = null;
            return;
        }

        try {
            const updated = await api.get(`/projects/${projectId}/documents`);
            renderDocsList(updated);
            const stillProcessing = updated.some(doc => doc.status === 'processing');
            if (!stillProcessing) {
                clearTimeout(state.docPoller);
                state.docPoller = null;
                return;
            }
            attempt += 1;
        } catch (error) {
            console.error('Docs Poll Error:', error);
            attempt += 1;
        }

        const nextDelay = Math.min(baseDelayMs * (2 ** attempt), maxDelayMs);
        state.docPoller = setTimeout(pollOnce, nextDelay);
    };

    state.docPoller = setTimeout(pollOnce, baseDelayMs);
}

const _activeToasts = [];
const _maxToasts = 3;

function showNotification(message, type = 'info') {
    const iconMap = {
        success: 'fa-check-circle',
        error: 'fa-circle-exclamation',
        warning: 'fa-triangle-exclamation',
        info: 'fa-circle-info'
    };

    // Remove oldest if at max
    while (_activeToasts.length >= _maxToasts) {
        const oldest = _activeToasts.shift();
        oldest.classList.remove('show');
        setTimeout(() => oldest.remove(), 400);
    }

    const toast = document.createElement('div');
    toast.className = `toast toast-${type}`;
    toast.innerHTML = `<i class="fas ${iconMap[type] || iconMap.info} toast-icon"></i><span>${escapeHtml(message)}</span>`;
    document.body.appendChild(toast);
    _activeToasts.push(toast);

    // Stack toasts bottom-to-top
    _repositionToasts();

    setTimeout(() => toast.classList.add('show'), 50);
    setTimeout(() => {
        toast.classList.remove('show');
        setTimeout(() => {
            toast.remove();
            const idx = _activeToasts.indexOf(toast);
            if (idx !== -1) _activeToasts.splice(idx, 1);
            _repositionToasts();
        }, 400);
    }, 3000);
}

function _repositionToasts() {
    let offset = 32;
    for (let i = _activeToasts.length - 1; i >= 0; i--) {
        _activeToasts[i].style.bottom = offset + 'px';
        offset += 60;
    }
}

function applyTranslations() {
    const t = i18n[state.lang];
    document.querySelectorAll('[data-i18n]').forEach(el => {
        const key = el.dataset.i18n;
        if (t[key]) el.textContent = t[key];
    });

    // Update placeholders
    if (document.getElementById('new-project-name')) {
        document.getElementById('new-project-name').placeholder = t.project_name_ph;
        document.getElementById('new-project-desc').placeholder = t.project_desc_ph;
    }

    // Update Lang Button
    elements.langToggle.querySelector('.lang-code').textContent = state.lang === 'ar' ? 'EN' : 'AR';

    // Update Dir
    document.documentElement.dir = state.lang === 'ar' ? 'rtl' : 'ltr';
    document.documentElement.lang = state.lang;

    // Update search placeholder
    const searchInput = document.querySelector('.search-bar input');
    if (searchInput) searchInput.placeholder = t.search_placeholder;

    const exportFormat = document.getElementById('srs-export-format');
    if (exportFormat) {
        updateExportButtonLabel(exportFormat.value || 'pdf');
    }

    // Update back button icon direction
    const backBtn = document.getElementById('back-to-projects');
    if (backBtn) {
        const icon = backBtn.querySelector('i');
        if (icon) icon.className = state.lang === 'ar' ? 'fas fa-arrow-right' : 'fas fa-arrow-left';
    }
}

function toggleTheme() {
    state.theme = state.theme === 'dark' ? 'light' : 'dark';
    document.body.classList.toggle('light-theme', state.theme === 'light');
    document.body.classList.toggle('dark-theme', state.theme === 'dark');

    const icon = elements.themeToggle.querySelector('i');
    icon.className = state.theme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';

    safeStorageSet('theme', state.theme);
}

function toggleLang() {
    state.lang = state.lang === 'ar' ? 'en' : 'ar';
    safeStorageSet('lang', state.lang);
    applyTranslations();
    switchView(state.currentView, state.selectedProject ? state.selectedProject.id : null);
}

// --- Event Handlers ---

async function switchView(viewName, params = null) {
    if (ADMIN_ONLY_VIEWS.has(viewName) && !isAdminUser()) {
        showNotification(state.lang === 'ar' ? 'Ù‡Ø°Ù‡ Ø§Ù„ØµÙØ­Ø© Ù…ØªØ§Ø­Ø© Ù„Ù„Ù…Ø³Ø¤ÙˆÙ„ ÙÙ‚Ø·' : 'This page is admin-only', 'warning');
        viewName = 'projects';
    }

    if (!views[viewName]) {
        viewName = 'projects';
    }

    if (state.docPoller) {
        clearTimeout(state.docPoller);
        state.docPoller = null;
    }
    state.currentView = viewName;

    // Update Nav
    elements.navItems.forEach(item => {
        item.classList.toggle('active', item.dataset.view === viewName);
    });

    // Render View
    if (views[viewName]) {
        await views[viewName]();
    }
}

async function handleNewProject(defaultTemplateId = '') {
    const t = i18n[state.lang];
    const langKey = state.lang === 'ar' ? 'ar' : 'en';

    elements.modalTitle.textContent = t.start_idea_title;
    elements.modalBody.innerHTML = `
        <div class="form-group">
            <label>${state.lang === 'ar' ? 'Ø§Ø³Ù… Ø§Ù„ÙÙƒØ±Ø© / Ø§Ù„Ù…Ø´Ø±ÙˆØ¹' : 'Idea / Project Name'}</label>
            <input type="text" id="new-project-name" class="form-control" placeholder="${t.idea_name_ph}">
        </div>
        <div class="form-group">
            <label>${state.lang === 'ar' ? 'Ø§Ù„ÙˆØµÙ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)' : 'Description (optional)'}</label>
            <textarea id="new-project-desc" class="form-control" placeholder="${t.project_desc_ph}"></textarea>
        </div>
        <button id="save-project-btn" class="btn btn-primary w-100 mt-4">
            <i class="fas fa-lightbulb"></i> ${t.start_idea_submit}
        </button>
    `;
    applyTranslations();

    elements.modalOverlay.classList.remove('hidden');

    const descInput = document.getElementById('new-project-desc');
    const nameInput = document.getElementById('new-project-name');

    if (defaultTemplateId && IDEA_TEMPLATES[defaultTemplateId]) {
        const info = IDEA_TEMPLATES[defaultTemplateId][langKey];
        if (!descInput.value.trim()) {
            descInput.value = info.description;
        }
        if (!nameInput.value.trim()) {
            nameInput.value = info.title;
        }
    }

    document.getElementById('save-project-btn').onclick = async () => {
        const name = nameInput.value;
        const descriptionBase = document.getElementById('new-project-desc').value;
        const templatePrompt = defaultTemplateId && IDEA_TEMPLATES[defaultTemplateId]
            ? IDEA_TEMPLATES[defaultTemplateId][langKey].prompt
            : '';
        const description = [descriptionBase, templatePrompt].filter(Boolean).join('\n\n');

        if (!name) {
            showFieldError(nameInput, i18n[state.lang].validation_project_name);
            return;
        }

        const btn = document.getElementById('save-project-btn');
        setButtonLoading(btn, true);
        try {
            const newProject = await api.post('/projects/', { name, description });
            showNotification(i18n[state.lang].success_saved, 'success');
            elements.modalOverlay.classList.add('hidden');

            // Navigate to chat with interview mode ON
            state.interviewMode = true;
            state.pendingProjectSelect = newProject.id;
            state.previousSummary = null;
            state.lastCoverage = null;
            state.lastInterviewSignals = null;
            state.lastLivePatch = null;
            state.lastCycleTrace = null;
            state.lastTopicNavigation = null;
            switchView('chat');
        } catch (error) {
            console.error('Create Project Error:', error);
        } finally {
            setButtonLoading(btn, false);
        }
    };

    // Setup field validation on project name input
    if (nameInput) {
        nameInput.addEventListener('input', () => clearFieldError(nameInput));
    }
}

async function handleDeleteProject(id, projectName = '') {
    const isAr = state.lang === 'ar';
    const safeName = String(projectName || '').trim();
    const confirmMsg = isAr
        ? `Ù‡Ù„ Ø£Ù†Øª Ù…ØªØ£ÙƒØ¯ Ù…Ù† Ø­Ø°Ù Ù…Ø´Ø±ÙˆØ¹ "${safeName || id}" ÙˆÙƒÙ„ Ù…Ø­ØªÙˆÙŠØ§ØªÙ‡ (Ø¨Ù…Ø§ ÙÙŠÙ‡Ø§ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ÙˆØ§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª)ØŸ`
        : `Are you sure you want to delete project "${safeName || id}" and all its contents?`;

    const confirmed = await showConfirmDialog(confirmMsg);
    if (!confirmed) return;

    showLoader();
    try {
        await api.delete(`/projects/${id}`);
        showNotification(isAr ? 'ØªÙ… Ø­Ø°Ù Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ù†Ø¬Ø§Ø­' : 'Project deleted successfully', 'success');
        await views.projects();
    } catch (error) {
        console.error('Delete Project Error:', error);
        showNotification(error.message || (isAr ? 'Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­Ø°Ù' : 'Error deleting project'), 'error');
    } finally {
        hideLoader();
    }
}

async function handleChatSubmit() {
    const input = document.getElementById('chat-input');
    const projectSelect = document.getElementById('chat-project-select');
    const sendBtn = document.getElementById('send-btn');

    const query = input.value.trim();
    const projectId = projectSelect.value;
    const language = detectMessageLanguage(query, state.lang);

    if (!query) return;

    const validationError = getChatSubmitValidationError(query, projectId);
    if (validationError) {
        showNotification(validationError, 'warning');
        return;
    }

    const pendingSttMetadata = state.pendingSttMeta || null;
    state.pendingSttMeta = null;

    addChatMessage('user', query);
    resetChatInputUi(input, sendBtn);

    const thinkingId = addChatMessage('bot', '', true);

    if (state.interviewMode) {
        await handleInterviewTurn({
            projectId,
            query,
            language,
            thinkingId,
            pendingSttMetadata,
        });
        return;
    }

    try {
        const streamResult = await streamProjectQuery(projectId, query, language, thinkingId);
        await finalizeQueryResult(projectId, query, language, thinkingId, streamResult, pendingSttMetadata);

    } catch (error) {
        console.warn('Stream failed, falling back to non-streaming:', error.message);
        await handleQueryFallback(projectId, query, language, thinkingId, pendingSttMetadata);
    }
}

function detectMessageLanguage(text, fallback = 'ar') {
    const value = String(text || '');
    const arabicCount = (value.match(/[\u0600-\u06FF]/g) || []).length;
    const latinCount = (value.match(/[A-Za-z]/g) || []).length;

    if (arabicCount === 0 && latinCount === 0) {
        return fallback === 'en' ? 'en' : 'ar';
    }

    return arabicCount >= latinCount ? 'ar' : 'en';
}

function getChatSubmitValidationError(query, projectId) {
    if (!query) return null;
    if (state.interviewMode && normalizeInterviewText(query) === normalizeInterviewText(state.lastUserInterviewAnswer)) {
        return i18n[state.lang].interview_duplicate_guard;
    }
    if (!projectId) {
        return state.lang === 'ar' ? 'ÙŠØ±Ø¬Ù‰ Ø§Ø®ØªÙŠØ§Ø± Ù…Ø´Ø±ÙˆØ¹ Ø£ÙˆÙ„Ø§Ù‹' : 'Select a project first';
    }
    return null;
}

function resetChatInputUi(input, sendBtn) {
    input.value = '';
    input.style.height = 'auto';
    sendBtn.disabled = true;
}

async function streamProjectQuery(projectId, query, language, thinkingId) {
    const payload = { query, language };
    const response = await fetch(`${API_BASE_URL}/projects/${projectId}/query/stream`, {
        method: 'POST',
        headers: authHeaders({ 'Content-Type': 'application/json' }),
        body: JSON.stringify(payload),
    });
    await throwIfNotOk(response);

    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    const streamState = {
        buffer: '',
        fullAnswer: '',
        sources: null,
        indicatorRemoved: false,
    };

    while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        consumeStreamChunk(streamState, decoder.decode(value, { stream: true }), thinkingId);
    }

    return {
        answer: streamState.fullAnswer,
        sources: streamState.sources,
    };
}

function consumeStreamChunk(streamState, chunk, thinkingId) {
    streamState.buffer += chunk;
    const lines = streamState.buffer.split('\n');
    streamState.buffer = lines.pop();

    for (const line of lines) {
        processStreamLine(streamState, line, thinkingId);
    }
}

function processStreamLine(streamState, line, thinkingId) {
    if (!line.startsWith('data: ')) return;
    const dataStr = line.slice(6).trim();
    if (dataStr === '[DONE]') return;

    try {
        const evt = JSON.parse(dataStr);
        applyStreamEvent(streamState, evt, thinkingId);
    } catch (error_) {
        console.warn('Malformed stream event ignored.', error_);
    }
}

function applyStreamEvent(streamState, evt, thinkingId) {
    if (evt.type === 'sources') {
        streamState.sources = evt.sources;
        return;
    }
    if (evt.type === 'token') {
        removeThinkingIndicatorOnce(streamState, thinkingId);
        streamState.fullAnswer += evt.token;
        renderStreamingAnswer(thinkingId, streamState.fullAnswer);
        return;
    }
    if (evt.type === 'error') {
        streamState.fullAnswer = evt.message || i18n[state.lang].error_generic;
    }
}

function removeThinkingIndicatorOnce(streamState, thinkingId) {
    if (streamState.indicatorRemoved) return;
    const ind = document.querySelector(`#msg-${thinkingId} .typing-indicator-pro`);
    if (ind) ind.remove();
    streamState.indicatorRemoved = true;
}

function renderStreamingAnswer(thinkingId, fullAnswer) {
    const textEl = document.querySelector(`#msg-${thinkingId} .msg-text`);
    if (textEl) {
        textEl.classList.add('streaming');
        textEl.innerHTML = formatAnswerHtml(fullAnswer) || escapeHtml(fullAnswer);
        textEl.dir = detectTextDirection(fullAnswer);
    }
    const container = document.getElementById('chat-messages');
    if (container) {
        container.scrollTop = container.scrollHeight;
    }
}

async function finalizeQueryResult(projectId, query, language, thinkingId, result, pendingSttMetadata) {
    finalizeBotMessage(thinkingId, result.answer, result.sources);
    await logChatMessages(projectId, query, result.answer, result.sources, pendingSttMetadata);
    await refreshLivePatchPanel(projectId, language);
}

async function handleQueryFallback(projectId, query, language, thinkingId, pendingSttMetadata) {
    try {
        const payload = { query, language };
        const result = await api.post(`/projects/${projectId}/query`, payload);
        removeThinkingIndicator(thinkingId);
        await finalizeQueryResult(projectId, query, language, thinkingId, { answer: result.answer, sources: result.sources }, pendingSttMetadata);
    } catch (error_) {
        removeThinkingIndicator(thinkingId);
        console.warn('Fallback non-streaming query failed:', error_);
        finalizeBotMessage(thinkingId, i18n[state.lang].error_generic, null);
    }
}

function removeThinkingIndicator(thinkingId) {
    const ind = document.querySelector(`#msg-${thinkingId} .typing-indicator-pro`);
    if (ind) ind.remove();
}

async function handleInterviewTurn({ projectId, query, language, thinkingId, pendingSttMetadata }) {
    const timeoutMs = 30000;
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), timeoutMs);

    try {
        state.lastUserInterviewAnswer = query;
        const userMessageMetadata = {};
        if (state.pendingInterviewSelectionMeta && typeof state.pendingInterviewSelectionMeta === 'object') {
            Object.assign(userMessageMetadata, state.pendingInterviewSelectionMeta);
        }
        if (pendingSttMetadata && typeof pendingSttMetadata === 'object') {
            Object.assign(userMessageMetadata, pendingSttMetadata);
        }
        state.pendingInterviewSelectionMeta = null;
        await api.post(`/projects/${projectId}/messages`, {
            messages: [{ role: 'user', content: query, metadata: Object.keys(userMessageMetadata).length ? userMessageMetadata : undefined }]
        });

        const interviewPayload = { language };
        // Do NOT send summary or coverage anymore; backend is source of truth

        const next = await api.post(
            `/projects/${projectId}/interview/next`,
            interviewPayload,
            false,
            { signal: controller.signal },
        );
        await refreshInterviewTelemetry(projectId);

        const questionText = next.question || (state.lang === 'ar'
            ? 'Ù‡Ù„ ÙŠÙ…ÙƒÙ† ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ø¨Ù…Ø«Ø§Ù„ØŸ'
            : 'Could you clarify the last point with an example?');

        let finalQuestionText = questionText;
        if (normalizeInterviewText(questionText) === normalizeInterviewText(state.lastAssistantQuestion)) {
            finalQuestionText = state.lang === 'ar'
                ? 'ØªÙ…Ø§Ù…ØŒ Ø®Ù„Ù‘ÙŠÙ†Ø§ Ù†ÙˆØ¶Ù‘Ø­ Ø§Ù„Ù†Ù‚Ø·Ø© Ø¯ÙŠ Ø¨Ø³Ø±Ø¹Ø©: Ø¥ÙŠÙ‡ Ø§Ù„ØªÙØµÙŠÙ„Ø© Ø§Ù„Ø£Ù‡Ù… Ø§Ù„Ù„ÙŠ ØªØ­Ø¨ Ù†Ø¹ØªÙ…Ø¯Ù‡Ø§ Ø§Ù„Ø¢Ù† ÙƒÙ‚Ø±Ø§Ø± ÙˆØ§Ø¶Ø­ØŸ'
                : 'Understood â€” letâ€™s clarify this quickly: what is the single most important detail we should lock now as a clear decision?';
        }
        state.lastAssistantQuestion = finalQuestionText;
        finalizeBotMessage(thinkingId, finalQuestionText, null);

        if (next.coverage) {
            state.lastCoverage = next.coverage;
        }
        state.lastInterviewSignals = next.signals || null;
        state.lastLivePatch = next.live_patch || null;
        state.lastCycleTrace = next.cycle_trace || null;
        state.lastTopicNavigation = next.topic_navigation || null;
        state.interviewStage = next.stage || state.interviewStage;
        updateInterviewProgress(next.coverage, next.done, next.topic_navigation);
        updateInterviewAssistBar(next.coverage);
        if (next.summary) {
            updateLiveDoc(next.summary, next.stage);
        }

        const warningText = Array.isArray(next?.live_patch?.warnings) ? next.live_patch.warnings[0] : '';
        if (warningText) {
            showNotification(warningText, 'warning');
        }

        await saveInterviewDraft(Number.parseInt(projectId, 10));

        await api.post(`/projects/${projectId}/messages`, {
            messages: [{ role: 'assistant', content: finalQuestionText, metadata: { stage: next.stage || '' } }]
        });

        if (next.done) {
            showNotification(i18n[state.lang].interview_completed, 'success');
            await openInterviewReviewModal(Number.parseInt(projectId, 10), language, next);
        }
    } catch (error) {
        console.error('Interview Error:', error);
        const aborted = error?.name === 'AbortError';
        const timeoutMsg = state.lang === 'ar' ? 'Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø·Ù„Ø¨. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.' : 'Request timed out. Please try again.';
        const failedMsg = state.lang === 'ar' ? 'ØªØ¹Ø°Ø± Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ø§Ù„Ø¢Ù†' : 'Interview failed. Try again.';
        const msg = aborted ? timeoutMsg : failedMsg;
        finalizeBotMessage(thinkingId, msg, null);
        showNotification(msg, 'error');
    } finally {
        clearTimeout(timeoutId);
    }
}

async function openInterviewReviewModal(projectId, language, interviewResult) {
    const t = i18n[state.lang];
    const summary = interviewResult?.summary || state.previousSummary || {};
    const coverage = interviewResult?.coverage || state.lastCoverage || {};

    elements.modalTitle.textContent = state.lang === 'ar' ? 'Ù…Ø±Ø§Ø¬Ø¹Ø© Ù†Ù‡Ø§Ø¦ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„' : 'Final review before submit';
    elements.modalBody.innerHTML = `
        <div class="interview-review-box">
            <p class="chat-disclaimer">${escapeHtml(t.interview_privacy)}</p>
            ${INTERVIEW_AREAS.map((area) => {
        const label = i18n[state.lang][`stage_${area}`] || area;
        const items = Array.isArray(summary[area]) ? summary[area] : [];
        const percent = Math.round(Number(coverage[area] || 0));
        return `
                    <div class="form-group">
                        <label>${escapeHtml(label)} (${percent}%)</label>
                        <textarea class="form-control interview-review-area" data-area="${area}" rows="4" placeholder="${state.lang === 'ar' ? 'ÙƒÙ„ Ø³Ø·Ø± = Ù†Ù‚Ø·Ø© Ù…ØªØ·Ù„Ø¨' : 'Each line = one requirement item'}">${escapeHtml(items.join('\n'))}</textarea>
                    </div>
                `;
    }).join('')}
            <p class="chat-disclaimer">${escapeHtml(t.interview_next_step)}</p>
            <button id="interview-final-submit" class="btn btn-primary w-100 mt-4">
                <i class="fas fa-check-circle"></i>
                <span>${state.lang === 'ar' ? 'Ø¥Ø±Ø³Ø§Ù„ ÙˆØ¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©' : 'Submit and finish interview'}</span>
            </button>
        </div>
    `;
    elements.modalOverlay.classList.remove('hidden');

    const submitBtn = document.getElementById('interview-final-submit');
    if (!submitBtn) return;

    submitBtn.onclick = async () => {
        setButtonLoading(submitBtn, true);
        try {
            const editedSummary = {};
            document.querySelectorAll('.interview-review-area').forEach((input) => {
                const area = input.dataset.area;
                const lines = String(input.value || '')
                    .split('\n')
                    .map((line) => line.trim())
                    .filter(Boolean);
                editedSummary[area] = lines;
            });

            state.previousSummary = editedSummary;
            await saveInterviewDraft(projectId);

            await api.post(`/projects/${projectId}/messages`, {
                messages: [
                    {
                        role: 'assistant',
                        content: state.lang === 'ar' ? 'ØªÙ…Øª Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ§Ø¹ØªÙ…Ø§Ø¯Ù‡ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….' : 'Final interview summary reviewed and approved by user.',
                        metadata: { summary: editedSummary, stage: state.interviewStage || 'features' }
                    }
                ]
            });

            state.srsRefreshing = true;
            await api.post(`/projects/${projectId}/srs/refresh`, { language });
            state.srsRefreshing = false;

            await clearInterviewDraft(projectId);
            elements.modalOverlay.classList.add('hidden');
            state.selectedProject = { id: projectId };
            showNotification(state.lang === 'ar' ? 'ØªÙ… Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­.' : 'Submitted successfully.', 'success');
            setTimeout(() => switchView('srs'), 700);
        } catch (error) {
            state.srsRefreshing = false;
            console.error('Final interview submit failed:', error);
            showNotification(i18n[state.lang].error_generic, 'error');
        } finally {
            setButtonLoading(submitBtn, false);
        }
    };
}

/**
 * Finalize a bot message after streaming completes:
 * render final formatted text, attach sources and copy button.
 */
function finalizeBotMessage(id, text, sources) {
    const msgDiv = document.getElementById(`msg-${id}`);
    if (!msgDiv) return;

    const textEl = msgDiv.querySelector('.msg-text');
    textEl.classList.remove('streaming');
    textEl.innerHTML = formatAnswerHtml(text) || escapeHtml(text);
    textEl.dir = detectTextDirection(text);

    if (sources && sources.length > 0) {
        const sourcesDiv = document.createElement('div');
        sourcesDiv.className = 'msg-sources-pro';
        sourcesDiv.innerHTML = `
            <div class="sources-header">
                <i class="fas fa-book-open"></i>
                <span>${state.lang === 'ar' ? 'Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©' : 'Sources Used'}</span>
            </div>
        `;
        const list = document.createElement('ul');
        sources.slice(0, 5).forEach(s => {
            const li = document.createElement('li');
            const docName = s.document_name || s.name || '';
            const score = typeof s.similarity === 'number' ? `<span class="source-score">${(s.similarity * 100).toFixed(0)}%</span>` : '';
            li.innerHTML = `
                <i class="fas fa-file-alt"></i>
                <span>${escapeHtml(docName)}</span>
                ${score}
            `;
            list.appendChild(li);
        });
        sourcesDiv.appendChild(list);
        msgDiv.querySelector('.msg-body').appendChild(sourcesDiv);
    }

    // Copy button
    const actionsDiv = document.createElement('div');
    actionsDiv.className = 'msg-actions';
    actionsDiv.innerHTML = `
        <button class="msg-action-btn copy-msg-btn" title="${i18n[state.lang].copy_btn}">
            <i class="fas fa-copy"></i> ${i18n[state.lang].copy_btn}
        </button>
    `;
    actionsDiv.querySelector('.copy-msg-btn').onclick = () => {
        const plainText = textEl.innerText || textEl.textContent;
        navigator.clipboard.writeText(plainText).then(() => {
            const btn = actionsDiv.querySelector('.copy-msg-btn');
            btn.classList.add('copied');
            btn.innerHTML = `<i class="fas fa-check"></i> ${i18n[state.lang].copied_btn}`;
            setTimeout(() => {
                btn.classList.remove('copied');
                btn.innerHTML = `<i class="fas fa-copy"></i> ${i18n[state.lang].copy_btn}`;
            }, 2000);
        });
    };
    msgDiv.querySelector('.msg-body').appendChild(actionsDiv);

    const container = document.getElementById('chat-messages');
    container.scrollTop = container.scrollHeight;
}

function addChatMessage(role, text, isThinking = false) {
    const messagesContainer = document.getElementById('chat-messages');
    const welcome = messagesContainer.querySelector('.welcome-msg-pro');
    if (welcome) welcome.remove();

    const id = `m${Date.now()}-${++_msgIdCounter}`;
    const msgDiv = document.createElement('div');
    msgDiv.className = `chat-msg-pro ${role}-msg-pro`;
    msgDiv.id = `msg-${id}`;

    const isUser = role === 'user';
    const userName = state.lang === 'ar' ? 'Ø£Ù†Øª' : 'You';
    const authorName = isUser ? userName : 'Tawasul';
    const thinkingStageLabel = state.lang === 'ar' ? 'ÙŠØ­Ù„Ù„ Ø¥Ø¬Ø§Ø¨ØªÙƒ Â· ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰' : 'Analyzing your answer Â· focusing on';
    const stageValue = i18n[state.lang]['stage_' + state.interviewStage] || state.interviewStage;
    const thinkingStageHtml = state.interviewMode && state.interviewStage
        ? `<div class="thinking-stage-label"><i class="fas fa-brain"></i> ${thinkingStageLabel} <strong>${stageValue}</strong></div>`
        : '';

    // Detect text direction
    const textDir = detectTextDirection(text);

    msgDiv.innerHTML = `
        <div class="msg-inner">
            <div class="msg-avatar-pro">
                ${isUser ? 'U' : '<i class="fas fa-robot"></i>'}
            </div>
            <div class="msg-body">
                <div class="msg-author">${authorName}</div>
                <div class="msg-text" dir="${textDir}">${isUser ? escapeHtml(text) : ''}</div>
                ${isThinking ? `<div class="typing-indicator-pro"><span></span><span></span><span></span></div>${thinkingStageHtml}` : ''}
            </div>
        </div>
    `;

    if (!isUser && !isThinking) {
        const msgText = msgDiv.querySelector('.msg-text');
        msgText.innerHTML = formatAnswerHtml(text) || escapeHtml(text);
        msgText.dir = textDir;
    }

    messagesContainer.appendChild(msgDiv);
    messagesContainer.scrollTop = messagesContainer.scrollHeight;
    return id;
}

function escapeHtml(value) {
    if (value == null) return '';
    return String(value)
        .replaceAll('&', '&amp;')
        .replaceAll('<', '&lt;')
        .replaceAll('>', '&gt;')
        .replaceAll('"', '&quot;')
        .replaceAll("'", '&#39;');
}

function detectTextDirection(text) {
    if (!text) return 'auto';
    // Check for Arabic/Hebrew/Persian characters
    const rtlChars = /[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF\u0590-\u05FF]/;
    const firstChars = text.trim().substring(0, 50);
    return rtlChars.test(firstChars) ? 'rtl' : 'ltr';
}

function deepCloneValue(value) {
    if (value == null || typeof value !== 'object') {
        return value;
    }
    if (Array.isArray(value)) {
        return value.map(item => deepCloneValue(item));
    }
    const cloned = {};
    Object.keys(value).forEach((key) => {
        cloned[key] = deepCloneValue(value[key]);
    });
    return cloned;
}

function formatAnswerHtml(text) {
    if (!text) return '';

    let cleaned = String(text).replaceAll('\r\n', '\n').trim();
    cleaned = cleaned.replaceAll(/\s*(Source|Sources|Ø§Ù„Ù…ØµØ¯Ø±|Ø§Ù„Ù…ØµØ§Ø¯Ø±)\s*:.*/gi, '').trim();
    cleaned = cleaned.replaceAll(/\s+\*\s+/g, '\n* ');
    cleaned = cleaned.replaceAll(/\s+-\s+/g, '\n- ');
    if (!cleaned.includes('\n') && cleaned.length > 220) {
        cleaned = cleaned.replaceAll(/([.!ØŸ?])\s+(?=[^\n])/g, '$1\n');
    }

    const lines = cleaned.split('\n').map(line => line.trim()).filter(Boolean);
    if (lines.length === 0) return '';

    const parts = [];
    let bulletBuffer = [];
    let numberedBuffer = [];

    const isHeadingLine = (line) => {
        if (!line) return false;
        if (/^#{1,4}\s+/.test(line)) return true;
        if (/^(Ø§Ù„Ø¹Ù†ÙˆØ§Ù†|Ù…Ù„Ø®Øµ|Summary|Overview|Key Points|Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©)\s*[:ï¼š]?$/i.test(line)) return true;
        return /^\*\*[^*]{3,}\*\*\s*:?$/.test(line);
    };

    const isQuestionLine = (line) => /[ØŸ?]\s*$/.test(line);
    const isNoteLine = (line) => /^(note|important|tip|warning|Ù…Ù„Ø§Ø­Ø¸Ø©|ØªÙ†Ø¨ÙŠÙ‡|Ù…Ù‡Ù…)\s*[:-]/i.test(line);

    const flushBullets = () => {
        if (bulletBuffer.length === 0) return;
        const items = bulletBuffer
            .map(item => `<li>${formatInlineMarkdown(item)}</li>`)
            .join('');
        parts.push(`<ul class="answer-list-plain">${items}</ul>`);
        bulletBuffer = [];
    };

    const flushNumbered = () => {
        if (numberedBuffer.length === 0) return;
        const items = numberedBuffer
            .map(item => `<li>${formatInlineMarkdown(item)}</li>`)
            .join('');
        parts.push(`<ol class="answer-ordered-list-plain">${items}</ol>`);
        numberedBuffer = [];
    };

    lines.forEach(line => {
        if (/^\d+[).-]\s+/.test(line)) {
            flushBullets();
            numberedBuffer.push(line.replace(/^\d+[).-]\s+/, ''));
            return;
        }

        if (/^[*-]\s+/.test(line)) {
            flushNumbered();
            bulletBuffer.push(line.replace(/^[*-]\s+/, ''));
            return;
        }

        flushBullets();
        flushNumbered();

        if (isHeadingLine(line)) {
            const headingText = line.replace(/^#{1,4}\s+/, '').replace(/^\*\*(.+)\*\*\s*:?$/, '$1');
            parts.push(`<h4 class="answer-heading-line">${formatInlineMarkdown(headingText)}</h4>`);
            return;
        }

        if (isNoteLine(line)) {
            parts.push(`<p class="answer-note-line">${formatInlineMarkdown(line)}</p>`);
            return;
        }

        if (isQuestionLine(line)) {
            parts.push(`<p class="answer-question-line">${formatInlineMarkdown(line)}</p>`);
            return;
        }

        parts.push(`<p class="answer-line">${formatInlineMarkdown(line)}</p>`);
    });

    flushBullets();
    flushNumbered();
    return parts.join('');
}

function formatInlineMarkdown(value) {
    const escaped = escapeHtml(value);
    return escaped
        .replaceAll(/\*\*(.+?)\*\*/g, '<strong>$1</strong>')
        .replaceAll(/`([^`]+)`/g, '<code>$1</code>');
}

let _resizeRafId = null;
function autoResizeTextarea(textarea) {
    if (_resizeRafId) cancelAnimationFrame(_resizeRafId);
    _resizeRafId = requestAnimationFrame(() => {
        textarea.style.height = 'auto';
        textarea.style.height = Math.min(textarea.scrollHeight, 200) + 'px';
        _resizeRafId = null;
    });
}

// --- Animated Counter ---
function animateCounter(elementId, target) {
    const el = document.getElementById(elementId);
    if (!el) return;
    const duration = 600;
    const start = performance.now();
    const from = 0;

    function update(now) {
        const elapsed = now - start;
        const progress = Math.min(elapsed / duration, 1);
        // ease-out cubic
        const eased = 1 - Math.pow(1 - progress, 3);
        el.textContent = Math.round(from + (target - from) * eased).toLocaleString();
        if (progress < 1) requestAnimationFrame(update);
    }

    requestAnimationFrame(update);
}

// --- Empty State ---
function createEmptyState(icon, titleKey, descKey) {
    const t = i18n[state.lang];
    return `
        <div class="empty-state">
            <div class="empty-state-icon"><i class="fas ${icon}"></i></div>
            <h3>${t[titleKey] || ''}</h3>
            <p>${t[descKey] || ''}</p>
        </div>
    `;
}

// --- Search ---
function handleSearch(query) {
    const q = query.toLowerCase().trim();
    if (!q) {
        // Restore current view
        switchView(state.currentView, state.selectedProject ? state.selectedProject.id : null);
        return;
    }
    const filtered = state.projects.filter(p =>
        p.name?.toLowerCase().includes(q) ||
        p.description?.toLowerCase().includes(q)
    );
    // Render inline results in current view container
    const list = document.getElementById('all-projects-list') || document.getElementById('recent-projects-list-dashboard');
    if (list) {
        list.innerHTML = '';
        if (filtered.length === 0) {
            list.innerHTML = createEmptyState('fa-search', 'empty_projects', 'empty_projects_desc');
        } else {
            filtered.forEach(p => list.appendChild(createProjectCard(p)));
        }
    }
}

// --- Mobile Sidebar ---
function openMobileSidebar() {
    document.querySelector('.sidebar').classList.add('open');
    const overlay = document.getElementById('sidebar-overlay');
    overlay.style.display = 'block';
    requestAnimationFrame(() => overlay.classList.add('active'));
    const hamburger = document.getElementById('mobile-hamburger');
    if (hamburger) hamburger.setAttribute('aria-expanded', 'true');
}

function closeMobileSidebar() {
    document.querySelector('.sidebar').classList.remove('open');
    const overlay = document.getElementById('sidebar-overlay');
    overlay.classList.remove('active');
    setTimeout(() => { overlay.style.display = 'none'; }, 300);
    const hamburger = document.getElementById('mobile-hamburger');
    if (hamburger) hamburger.setAttribute('aria-expanded', 'false');
}

function setupUploadZone(projectId) {
    const zone = document.getElementById('upload-zone');
    const input = document.getElementById('file-input');

    zone.onclick = () => input.click();

    zone.ondragover = (e) => {
        e.preventDefault();
        zone.classList.add('dragover');
    };

    zone.ondragleave = () => zone.classList.remove('dragover');

    zone.ondrop = (e) => {
        e.preventDefault();
        zone.classList.remove('dragover');
        handleFiles(e.dataTransfer.files, projectId);
    };

    input.onchange = () => handleFiles(input.files, projectId);
}

async function handleFiles(files, projectId) {
    const unsupportedTypeText = state.lang === 'ar'
        ? 'Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…. Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©: PDF, TXT, DOCX'
        : 'Unsupported file type. Supported: PDF, TXT, DOCX';
    let uploadedAny = false;
    for (const file of files) {
        if (!isSupportedUploadFile(file)) {
            showNotification(unsupportedTypeText, 'warning');
            continue;
        }
        const uploadSucceeded = await uploadSingleFile(projectId, file);
        uploadedAny = uploadedAny || uploadSucceeded;
    }

    if (uploadedAny) {
        await refreshDocumentsAfterUpload(projectId);
    }
}

function isSupportedUploadFile(file) {
    const lowerName = file.name.toLowerCase();
    return lowerName.endsWith('.pdf') || lowerName.endsWith('.txt') || lowerName.endsWith('.docx');
}

async function uploadSingleFile(projectId, file) {
    const lowerName = file.name.toLowerCase();
    showNotification(`${state.lang === 'ar' ? 'Ø¬Ø§Ø±ÙŠ Ø±ÙØ¹' : 'Uploading'} ${file.name}...`, 'info');

    try {
        const presign = await api.post(`/projects/${projectId}/documents/presign`, {
            filename: file.name,
            file_size: file.size,
            content_type: file.type || 'application/octet-stream',
        });

        const uploadResp = await fetch(presign.upload_url, {
            method: 'PUT',
            headers: {
                'Content-Type': presign.content_type || file.type || 'application/octet-stream',
            },
            body: file,
        });
        if (!uploadResp.ok) {
            throw new Error(state.lang === 'ar' ? 'ÙØ´Ù„ Ø§Ù„Ø±ÙØ¹ Ø¥Ù„Ù‰ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ' : 'Failed uploading to object storage');
        }

        await api.post(`/projects/${projectId}/documents/complete`, {
            unique_filename: presign.unique_filename,
            original_filename: file.name,
            file_key: presign.file_key,
            file_size: file.size,
            file_type: lowerName.split('.').pop() || '',
        });
        showNotification(`${state.lang === 'ar' ? 'ØªÙ… Ø±ÙØ¹' : 'Uploaded'} ${file.name}`, 'success');
        return true;
    } catch (error) {
        console.error('Upload Error:', error);
        return false;
    }
}

async function refreshDocumentsAfterUpload(projectId) {
    try {
        const docs = await api.get(`/projects/${projectId}/documents`);
        renderDocsList(docs);
        startDocPolling(projectId, docs);
    } catch (error) {
        console.error('Documents refresh after upload failed:', error);
        const refreshError = state.lang === 'ar'
            ? 'ØªÙ… Ø§Ù„Ø±ÙØ¹ ÙˆÙ„ÙƒÙ† ØªØ¹Ø°Ø± ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù„ÙØ§Øª'
            : 'Upload succeeded but document list refresh failed';
        showNotification(error?.message || refreshError, 'warning');
    }
}

// --- Upload Modal (from chat) ---

function openUploadModal(projectId) {
    const t = i18n[state.lang];
    elements.modalTitle.textContent = t.upload_reference_docs || (state.lang === 'ar' ? 'Ø±ÙØ¹ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø±Ø¬Ø¹ÙŠØ©' : 'Upload Reference Documents');
    elements.modalBody.innerHTML = `
        <p class="config-desc">${state.lang === 'ar'
            ? 'Ø§Ø±ÙØ¹ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø±Ø¬Ø¹ÙŠØ© Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ ÙÙ‡Ù… Ù…Ø´Ø±ÙˆØ¹Ùƒ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„. Ù‡Ø°Ù‡ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©.'
            : 'Upload reference documents to help the AI better understand your project. This step is optional.'}</p>
        <div class="upload-zone" id="modal-upload-zone">
            <i class="fas fa-cloud-upload-alt"></i>
            <p>${t.upload_desc}</p>
            <span class="hint">${state.lang === 'ar' ? 'ÙŠØ¯Ø¹Ù… PDF, TXT, DOCX' : 'Supports PDF, TXT, DOCX'}</span>
            <input type="file" id="modal-file-input" multiple hidden accept=".pdf,.txt,.docx">
        </div>
    `;

    elements.modalOverlay.classList.remove('hidden');

    const zone = document.getElementById('modal-upload-zone');
    const input = document.getElementById('modal-file-input');

    zone.onclick = () => input.click();

    zone.ondragover = (e) => {
        e.preventDefault();
        zone.classList.add('dragover');
    };

    zone.ondragleave = () => zone.classList.remove('dragover');

    zone.ondrop = (e) => {
        e.preventDefault();
        zone.classList.remove('dragover');
        handleFiles(e.dataTransfer.files, projectId);
    };

    input.onchange = () => handleFiles(input.files, projectId);
}

// --- Booking Modal ---

function openBookingModal() {
    const t = i18n[state.lang];
    elements.modalTitle.textContent = t.book_meeting_title;
    elements.modalBody.innerHTML = `
        <div class="booking-form">
            <div class="form-group">
                <label>${t.book_name} *</label>
                <input type="text" id="book-name" class="form-control" required>
            </div>
            <div class="form-group">
                <label>${t.book_email} *</label>
                <input type="email" id="book-email" class="form-control" required>
            </div>
            <div class="form-row">
                <div class="form-group">
                    <label>${t.book_date} *</label>
                    <input type="date" id="book-date" class="form-control" min="${new Date().toISOString().split('T')[0]}" required>
                </div>
                <div class="form-group">
                    <label>${t.book_time} *</label>
                    <select id="book-time" class="form-control">
                        <option value="09:00">09:00 AM</option>
                        <option value="10:00">10:00 AM</option>
                        <option value="11:00">11:00 AM</option>
                        <option value="12:00">12:00 PM</option>
                        <option value="13:00">01:00 PM</option>
                        <option value="14:00">02:00 PM</option>
                        <option value="15:00">03:00 PM</option>
                        <option value="16:00">04:00 PM</option>
                        <option value="17:00">05:00 PM</option>
                    </select>
                </div>
            </div>
            <div class="form-group">
                <label>${t.book_notes}</label>
                <textarea id="book-notes" class="form-control" rows="3"></textarea>
            </div>
            <button id="book-submit-btn" class="btn btn-primary w-100 mt-4">
                <i class="fas fa-calendar-check"></i> ${t.book_submit}
            </button>
        </div>
    `;

    elements.modalOverlay.classList.remove('hidden');

    document.getElementById('book-submit-btn').onclick = async () => {
        const name = document.getElementById('book-name').value.trim();
        const email = document.getElementById('book-email').value.trim();
        const date = document.getElementById('book-date').value;
        const time = document.getElementById('book-time').value;
        const notes = document.getElementById('book-notes').value.trim();

        if (!name || !email || !date) {
            showNotification(t.book_fill_required, 'warning');
            return;
        }

        const submitBtn = document.getElementById('book-submit-btn');
        submitBtn.disabled = true;

        try {
            const projectId = state.selectedProject ? state.selectedProject.id : null;
            if (projectId) {
                await api.post(`/projects/${projectId}/handoff`, {
                    client_name: name,
                    client_email: email,
                    preferred_date: date,
                    preferred_time: time,
                    notes
                });
            }
            showNotification(t.book_success, 'success');
            elements.modalOverlay.classList.add('hidden');
        } catch (error) {
            console.error('Booking Error:', error);
        } finally {
            submitBtn.disabled = false;
        }
    };
}

// --- Voice Recording (STT) ---

async function startRecording() {
    if (navigator.mediaDevices?.getUserMedia == null) {
        showNotification(i18n[state.lang].mic_no_support, 'warning');
        return;
    }

    try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        const mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
        const chunks = [];

        mediaRecorder.ondataavailable = (e) => {
            if (e.data.size > 0) chunks.push(e.data);
        };

        mediaRecorder.onstop = async () => {
            stream.getTracks().forEach(track => track.stop());
            const blob = new Blob(chunks, { type: 'audio/webm' });
            await transcribeAudio(blob);
        };

        state.mediaRecorder = mediaRecorder;
        state.isRecording = true;
        mediaRecorder.start();
        updateMicButton(true);
        showNotification(i18n[state.lang].mic_recording, 'info');
    } catch (error) {
        console.error('Mic access error:', error);
        showNotification(i18n[state.lang].mic_error, 'error');
    }
}

function stopRecording() {
    if (state.mediaRecorder && state.isRecording) {
        state.mediaRecorder.stop();
        state.isRecording = false;
        updateMicButton(false);
    }
}

async function transcribeAudio(blob) {
    const micBtn = document.getElementById('mic-btn');
    if (micBtn) {
        micBtn.disabled = true;
        micBtn.innerHTML = '<i class="fas fa-spinner fa-spin"></i>';
    }

    try {
        const language = state.lang === 'ar' ? 'ar' : 'en';

        const formData = new FormData();
        formData.append('file', blob, 'recording.webm');
        formData.append('language', language);

        const response = await fetch(`${API_BASE_URL}/stt/transcribe`, {
            method: 'POST',
            headers: authHeaders(),
            body: formData
        });

        await throwIfNotOk(response, 'Transcription failed');

        const result = await response.json();
        if (result.success && result.text) {
            await applyTranscriptResult(result);
        }
    } catch (error) {
        console.error('STT Error:', error);
        showNotification(error.message, 'error');
    } finally {
        if (micBtn) {
            micBtn.disabled = false;
            micBtn.innerHTML = '<i class="fas fa-microphone"></i>';
        }
    }
}

function getUnavailableConfidenceText() {
    return state.lang === 'ar' ? 'ØºÙŠØ± Ù…ØªØ§Ø­' : 'N/A';
}

function buildTranscriptConfirmationText(resultText, confidencePct, warningSuffix) {
    if (state.lang === 'ar') {
        return `Ù…Ø±Ø§Ø¬Ø¹Ø© Ù†Øµ Ø§Ù„ØªÙØ±ÙŠØº Ù‚Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯Ù‡:\n\n${resultText}\n\nØ¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©: ${confidencePct}${warningSuffix}\n\nÙ‡Ù„ ØªØ¤ÙƒØ¯ Ù‡Ø°Ø§ Ø§Ù„Ù†ØµØŸ`;
    }
    return `Please review the transcript before using it:\n\n${resultText}\n\nConfidence: ${confidencePct}${warningSuffix}\n\nDo you confirm this text?`;
}

function applyTranscriptToInput(text) {
    const chatInput = document.getElementById('chat-input');
    if (!chatInput) return;
    chatInput.value += (chatInput.value ? ' ' : '') + text;
    chatInput.oninput();
    chatInput.focus();
}

function notifyTranscriptReviewState(requiresConfirmation, warningList) {
    if (!(requiresConfirmation || warningList.length > 0)) return;
    showNotification(
        state.lang === 'ar'
            ? 'ØªÙ… Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ù†Øµ Ø¨Ø¹Ø¯ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¨Ø´Ø±ÙŠØ©. ÙŠÙØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©.'
            : 'Transcript confirmed by human review. Please re-check sensitive terms.',
        'info'
    );
}

async function applyTranscriptResult(result) {
    const confidence = Number.isFinite(result.confidence) ? result.confidence : null;
    const warningList = Array.isArray(result.quality_warnings) ? result.quality_warnings : [];
    const confidencePct = confidence == null ? getUnavailableConfidenceText() : `${Math.round(confidence * 100)}%`;
    const warningSuffix = warningList.length ? `\n\n${warningList.slice(0, 2).join('\n')}` : '';
    const confirmText = buildTranscriptConfirmationText(result.text, confidencePct, warningSuffix);
    const confirmed = globalThis.confirm(confirmText);

    if (!confirmed) {
        showNotification(state.lang === 'ar' ? 'ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„ØªÙØ±ÙŠØº Ø§Ù„ØµÙˆØªÙŠ' : 'Transcript insertion cancelled', 'warning');
        return;
    }

    state.pendingSttMeta = {
        source: 'stt',
        transcript_confirmed: true,
        stt_confidence: confidence,
        quality_warnings: warningList,
    };

    applyTranscriptToInput(result.text);
    notifyTranscriptReviewState(result.requires_confirmation, warningList);
}

function updateMicButton(recording) {
    const micBtn = document.getElementById('mic-btn');
    if (!micBtn) return;
    if (recording) {
        micBtn.classList.add('recording');
        micBtn.innerHTML = '<i class="fas fa-stop"></i>';
    } else {
        micBtn.classList.remove('recording');
        micBtn.innerHTML = '<i class="fas fa-microphone"></i>';
    }
}

// --- Interview Progress ---

function updateInterviewProgress(coverage, done, topicNavigation = null) {
    const progressBar = document.getElementById('interview-progress');
    const liveDocPanel = document.getElementById('live-doc-panel');
    const assistBar = document.getElementById('interview-assist-bar');

    if (!state.interviewMode || done) {
        if (progressBar) progressBar.style.display = 'none';
        if (liveDocPanel) liveDocPanel.style.display = 'none';
        if (assistBar) assistBar.style.display = 'none';
        return;
    }

    if (progressBar) {
        progressBar.style.display = 'block';
        const areas = INTERVIEW_AREAS;
        const stageLabels = {
            discovery: i18n[state.lang].stage_discovery,
            scope: i18n[state.lang].stage_scope,
            users: i18n[state.lang].stage_users,
            features: i18n[state.lang].stage_features,
            constraints: i18n[state.lang].stage_constraints
        };
        const activeArea = getActiveCoverageArea(coverage, areas);

        areas.forEach((area, index) => {
            const pct = coverage ? (coverage[area] || 0) : 0;
            updateCoverageProgressItem(progressBar, area, index, pct, stageLabels, activeArea);
        });
    }

    if (liveDocPanel) {
        applySummaryDrawerState();
    }

    if (assistBar) {
        assistBar.style.display = 'block';
    }
}

function getActiveCoverageArea(coverage, areas) {
    if (!coverage) return null;
    let minCoverage = Infinity;
    let activeArea = null;
    for (const area of areas) {
        const pct = coverage[area] || 0;
        if (pct < 70 && pct < minCoverage) {
            minCoverage = pct;
            activeArea = area;
        }
    }
    return activeArea;
}

function updateCoverageProgressItem(progressBar, area, index, pct, stageLabels, activeArea) {
    const fillEl = document.getElementById(`coverage-${area}`);
    const pctEl = document.getElementById(`coverage-pct-${area}`);
    const itemEl = progressBar.querySelector(`[data-area="${area}"]`);
    const roundedPct = Math.round(pct);

    if (fillEl) {
        fillEl.textContent = String(index + 1);
        fillEl.dataset.pct = `${roundedPct}%`;
        fillEl.setAttribute('aria-label', `${stageLabels[area] || area} ${roundedPct}%`);
    }
    if (pctEl) pctEl.textContent = `${roundedPct}%`;

    if (!itemEl) return;
    itemEl.classList.remove('active-area', 'complete-area');
    if (pct >= 70) {
        itemEl.classList.add('complete-area');
    } else if (area === activeArea) {
        itemEl.classList.add('active-area');
    }
    itemEl.title = `${stageLabels[area] || area} ${roundedPct}%`;
}

function updateLiveDoc(summary, stage) {
    const content = document.getElementById('live-doc-content');
    if (!content || !summary) return;

    const areaIcons = {
        discovery: 'fa-compass',
        scope: 'fa-bullseye',
        users: 'fa-users',
        features: 'fa-puzzle-piece',
        constraints: 'fa-shield-halved'
    };

    const stageLabels = {
        discovery: i18n[state.lang].stage_discovery,
        scope: i18n[state.lang].stage_scope,
        users: i18n[state.lang].stage_users,
        features: i18n[state.lang].stage_features,
        constraints: i18n[state.lang].stage_constraints
    };

    if (isStructuredSummary(summary)) {
        renderStructuredLiveDoc(content, summary, areaIcons, stageLabels);
        return;
    }

    renderLegacyLiveDoc(content, summary, stageLabels, stage);
}

function isStructuredSummary(summary) {
    return typeof summary === 'object' && !Array.isArray(summary);
}

function renderStructuredLiveDoc(content, summary, areaIcons, stageLabels) {
    const model = buildLiveDocRenderModel(summary);
    const htmlParts = [];

    const alertsHtml = buildLiveDocAlertsHtml(model.alerts);
    if (alertsHtml) htmlParts.push(alertsHtml);

    const planHtml = buildLiveDocPlanHtml(model.plan);
    if (planHtml) htmlParts.push(planHtml);

    const traceHtml = buildLiveDocTraceHtml(model.trace);
    if (traceHtml) htmlParts.push(traceHtml);

    const removedHtml = buildLiveDocRemovedHtml(model.removedEvents);
    if (removedHtml) htmlParts.push(removedHtml);

    htmlParts.push(buildLiveDocSectionsHtml(summary, model, areaIcons, stageLabels));

    content.innerHTML = htmlParts.join('') || `
        <div class="live-doc-empty">
            <i class="fas fa-pencil-alt"></i>
            <p>${i18n[state.lang].live_doc_empty}</p>
        </div>
    `;

    state.previousSummary = deepCloneValue(summary);
    scrollToLatestLiveDocItem(content);
}

function buildLiveDocRenderModel(summary) {
    const patch = state.lastLivePatch || null;
    const patchEvents = Array.isArray(patch?.events) ? patch.events : [];
    return {
        prevSummary: state.previousSummary || {},
        focusArea: patch?.focus_area || null,
        patchMap: new Map((patch?.changed_areas || []).map((item) => [item.area, item])),
        eventMap: new Map(patchEvents.map((event) => [String(event?.field_path || ''), String(event?.op || '')])),
        removedEvents: patchEvents.filter((event) => String(event?.op || '') === 'removed'),
        alerts: Array.isArray(patch?.alerts) ? patch.alerts : [],
        plan: patch?.next_plan || null,
        trace: state.lastCycleTrace || null,
        summary,
    };
}

function buildLiveDocAlertsHtml(alerts) {
    if (!alerts.length) return '';
    return `
        <div class="live-doc-alerts">
            ${alerts.map((alert) => {
        const severity = String(alert?.severity || 'info');
        const title = escapeHtml(String(alert?.title || 'Alert'));
        const msg = escapeHtml(String(alert?.message || ''));
        return `
                    <div class="live-doc-alert live-doc-alert-${severity}">
                        <div class="live-doc-alert-title">${title}</div>
                        <div class="live-doc-alert-text">${msg}</div>
                    </div>
                `;
    }).join('')}
        </div>
    `;
}

function buildLiveDocPlanHtml(plan) {
    if (!(plan?.target_stage || plan?.question_style)) return '';
    const stageKey = plan?.target_stage ? `stage_${plan.target_stage}` : '';
    const stageLabel = stageKey ? (i18n[state.lang][stageKey] || plan.target_stage) : '';
    const styleText = escapeHtml(String(plan?.question_style || ''));
    const hintText = escapeHtml(String(plan?.prompt_hint || ''));
    return `
        <div class="live-doc-plan">
            <div class="live-doc-plan-row"><strong>${state.lang === 'ar' ? 'Ø§Ù„ØªØ±ÙƒÙŠØ² Ø§Ù„Ù‚Ø§Ø¯Ù…:' : 'Next focus:'}</strong> ${escapeHtml(stageLabel)}</div>
            <div class="live-doc-plan-row"><strong>${state.lang === 'ar' ? 'Ù†Ù…Ø· Ø§Ù„Ø³Ø¤Ø§Ù„:' : 'Question style:'}</strong> ${styleText}</div>
            ${hintText ? `<div class="live-doc-plan-row">${hintText}</div>` : ''}
        </div>
    `;
}

function buildLiveDocTraceHtml(trace) {
    if (!(trace?.steps && Array.isArray(trace.steps))) return '';
    const scoreCoverage = Number(trace?.score?.coverage || 0);
    const scoreConfidence = Number(trace?.score?.confidence || 0);
    const riskLevel = escapeHtml(String(trace?.score?.risk_level || 'low'));
    return `
        <div class="live-doc-trace">
            <div class="live-doc-trace-head">
                <strong>${state.lang === 'ar' ? 'Ø¯ÙˆØ±Ø© Ø§Ù„ØªÙÙƒÙŠØ±' : 'Cognitive loop'}</strong>
                <span>${state.lang === 'ar' ? 'ØªØºØ·ÙŠØ©' : 'Coverage'}: ${Math.round(scoreCoverage)}%</span>
                <span>${state.lang === 'ar' ? 'Ø«Ù‚Ø©' : 'Confidence'}: ${Math.round(scoreConfidence * 100)}%</span>
                <span>${state.lang === 'ar' ? 'Ù…Ø®Ø§Ø·Ø±Ø©' : 'Risk'}: ${riskLevel}</span>
            </div>
            <div class="live-doc-trace-steps">
                ${trace.steps.map((step, idx) => {
        const name = escapeHtml(String(step?.name || `step-${idx + 1}`));
        const text = escapeHtml(String(step?.summary || ''));
        return `
                        <div class="live-doc-trace-step">
                            <div class="live-doc-trace-step-name">${idx + 1}. ${name}</div>
                            <div class="live-doc-trace-step-text">${text}</div>
                        </div>
                    `;
    }).join('')}
            </div>
        </div>
    `;
}

function buildLiveDocRemovedHtml(removedEvents) {
    if (!removedEvents.length) return '';
    return `
        <div class="live-doc-plan">
            <div class="live-doc-plan-row"><strong>${state.lang === 'ar' ? 'Ø­Ù‚ÙˆÙ„ ØªÙ…Øª Ø¥Ø²Ø§Ù„ØªÙ‡Ø§:' : 'Removed fields:'}</strong></div>
            ${removedEvents.slice(0, 5).map((event) => {
        const path = escapeHtml(String(event?.field_path || ''));
        const value = escapeHtml(String(event?.value || ''));
        return `<div class="live-doc-plan-row live-doc-removed-item">${path}: ${value}</div>`;
    }).join('')}
        </div>
    `;
}

function buildLiveDocSectionsHtml(summary, model, areaIcons, stageLabels) {
    let html = '';
    for (const [area, items] of Object.entries(summary)) {
        if (!Array.isArray(items) || items.length === 0) continue;
        html += buildLiveDocAreaHtml(area, items, model, areaIcons, stageLabels);
    }
    return html;
}

function buildLiveDocAreaHtml(area, items, model, areaIcons, stageLabels) {
    const prevItems = model.prevSummary[area] || [];
    const icon = areaIcons[area] || 'fa-circle-dot';
    const label = stageLabels[area] || area;
    const isFocus = model.focusArea && model.focusArea === area;
    const areaPatch = model.patchMap.get(area);
    const delta = Number(areaPatch?.coverage_delta || 0);

    return `
        <div class="live-doc-srs-section ${isFocus ? 'live-doc-focus-area' : ''}">
            <div class="live-doc-section-header">
                <i class="fas ${icon}"></i>
                <span>${escapeHtml(label)}</span>
                <span class="live-doc-count-badge">${items.length}</span>
                ${delta > 0 ? `<span class="live-doc-delta-badge">+${Math.round(delta)}%</span>` : ''}
            </div>
            <ul class="live-doc-items">
                ${items.map((item, idx) => buildLiveDocItemHtml(area, idx, item, prevItems, model.eventMap)).join('')}
            </ul>
        </div>
    `;
}

function buildLiveDocItemHtml(area, idx, item, prevItems, eventMap) {
    // Items from the interview service are {id, value} objects; fall back to plain string.
    const text = (typeof item === 'object' && item !== null)
        ? String(item.value || '')
        : String(item ?? '');
    const itemId = (typeof item === 'object' && item !== null) ? String(item.id || '') : '';
    const isNew = !prevItems.some((p) => {
        if (typeof p === 'object' && p !== null) {
            return (itemId && p.id === itemId) || p.value === text;
        }
        return String(p) === text;
    });
    const fieldPath = `${area}.${idx}`;
    const op = eventMap.get(fieldPath);
    const updatedClass = op === 'updated' ? 'live-doc-updated-item' : '';
    const addedClass = op === 'added' ? 'live-doc-new-item' : '';
    return `<li class="${isNew ? 'live-doc-new-item' : ''} ${addedClass} ${updatedClass}">${escapeHtml(text)}</li>`;
}

function scrollToLatestLiveDocItem(content) {
    const newItems = content.querySelectorAll('.live-doc-new-item');
    if (newItems.length > 0) {
        newItems[newItems.length - 1].scrollIntoView({ behavior: 'smooth', block: 'nearest' });
    }
}

function renderLegacyLiveDoc(content, summary, stageLabels, stage) {
    const stageLabel = stageLabels[stage] || stage;
    content.innerHTML = `
        <div class="live-doc-section">
            <div class="live-doc-stage-badge">
                <i class="fas fa-circle-dot"></i> ${escapeHtml(stageLabel)}
            </div>
            <div class="live-doc-text" dir="${detectTextDirection(String(summary))}">${formatAnswerHtml(String(summary)) || escapeHtml(String(summary))}</div>
        </div>
    `;
}

// --- Authentication ---

async function autoLogin() {
    try {
        const res = await fetch(`${API_BASE_URL}/auth/login`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ email: 'admin@tawasul.com', password: 'admin123' })
        });
        await throwIfNotOk(res, 'Login failed');
        if (res.ok) {
            const data = await res.json();
            setAuthState(data.token, data.user);
            showApp();
            return;
        }
    } catch (e) {
        console.warn('Auto-login unavailable; showing auth screen.', e);
    }
    showAuthScreen();
}

function showAuthScreen() {
    document.getElementById('auth-screen').style.display = 'flex';
    document.getElementById('app-container').style.display = 'none';

    // Tab switching
    document.querySelectorAll('.auth-tab').forEach(tab => {
        tab.onclick = () => {
            document.querySelectorAll('.auth-tab').forEach(t => t.classList.remove('active'));
            tab.classList.add('active');
            const isLogin = tab.dataset.tab === 'login';
            document.getElementById('login-form').style.display = isLogin ? 'block' : 'none';
            document.getElementById('register-form').style.display = isLogin ? 'none' : 'block';
            document.getElementById('auth-error').style.display = 'none';
        };
    });

    // Setup field validation for login
    const loginEmailInput = document.getElementById('login-email');
    const loginPasswordInput = document.getElementById('login-password');
    if (loginEmailInput) {
        setupFieldValidation(loginEmailInput, (v) => {
            if (!v.trim()) return i18n[state.lang].validation_required;
            return null;
        });
    }
    if (loginPasswordInput) {
        setupFieldValidation(loginPasswordInput, (v) => {
            if (!v) return i18n[state.lang].validation_required;
            return null;
        });
    }

    // Setup field validation for register
    const regNameInput = document.getElementById('register-name');
    const regEmailInput = document.getElementById('register-email');
    const regPasswordInput = document.getElementById('register-password');
    if (regNameInput) {
        setupFieldValidation(regNameInput, (v) => {
            if (!v.trim()) return i18n[state.lang].validation_required;
            return null;
        });
    }
    if (regEmailInput) {
        setupFieldValidation(regEmailInput, (v) => {
            if (!v.trim()) return i18n[state.lang].validation_required;
            if (!isValidEmail(v)) return i18n[state.lang].validation_email_invalid;
            return null;
        });
    }
    if (regPasswordInput) {
        setupFieldValidation(regPasswordInput, (v) => {
            if (!v) return i18n[state.lang].validation_required;
            if (v.length < 6) return i18n[state.lang].validation_password_min;
            return null;
        });
    }

    // Login
    document.getElementById('login-form').onsubmit = async (e) => {
        e.preventDefault();
        const email = document.getElementById('login-email').value.trim();
        const password = document.getElementById('login-password').value;
        const errorEl = document.getElementById('auth-error');
        errorEl.style.display = 'none';

        // Validate fields
        if (!email) {
            showFieldError(document.getElementById('login-email'), i18n[state.lang].validation_required);
            return;
        }
        if (!password) {
            showFieldError(document.getElementById('login-password'), i18n[state.lang].validation_required);
            return;
        }

        const submitBtn = e.target.querySelector('button[type="submit"]');
        setButtonLoading(submitBtn, true);
        try {
            const res = await fetch(`${API_BASE_URL}/auth/login`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ email, password })
            });
            await throwIfNotOk(res, 'Login failed');
            const data = await res.json();
            setAuthState(data.token, data.user);
            showApp();
        } catch (err) {
            errorEl.textContent = err.message;
            errorEl.style.display = 'block';
        } finally {
            setButtonLoading(submitBtn, false);
        }
    };

    // Register
    document.getElementById('register-form').onsubmit = async (e) => {
        e.preventDefault();
        const name = document.getElementById('register-name').value.trim();
        const email = document.getElementById('register-email').value.trim();
        const password = document.getElementById('register-password').value;
        const errorEl = document.getElementById('auth-error');
        errorEl.style.display = 'none';

        // Validate fields
        if (!name) {
            showFieldError(document.getElementById('register-name'), i18n[state.lang].validation_required);
            return;
        }
        if (!email || !isValidEmail(email)) {
            showFieldError(document.getElementById('register-email'), i18n[state.lang].validation_email_invalid);
            return;
        }
        if (!password || password.length < 6) {
            showFieldError(document.getElementById('register-password'), i18n[state.lang].validation_password_min);
            return;
        }

        const submitBtn = e.target.querySelector('button[type="submit"]');
        setButtonLoading(submitBtn, true);
        try {
            const res = await fetch(`${API_BASE_URL}/auth/register`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ name, email, password })
            });
            await throwIfNotOk(res, 'Registration failed');
            const data = await res.json();
            setAuthState(data.token, data.user);
            showApp();
        } catch (err) {
            errorEl.textContent = err.message;
            errorEl.style.display = 'block';
        } finally {
            setButtonLoading(submitBtn, false);
        }
    };
}

function setAuthState(token, user) {
    state.authToken = token;
    state.currentUser = user;
    safeStorageSet('authToken', token);
    safeStorageSet('currentUser', JSON.stringify(user));
}

function clearAuthState() {
    state.authToken = null;
    state.currentUser = null;
    safeStorageRemove('authToken');
    safeStorageRemove('currentUser');
}

function showApp() {
    document.getElementById('auth-screen').style.display = 'none';
    document.getElementById('app-container').style.display = 'flex';

    // Update sidebar user info
    const avatarEl = document.getElementById('user-avatar');
    const nameEl = document.getElementById('user-name');
    if (state.currentUser) {
        if (avatarEl) avatarEl.textContent = state.currentUser.name.charAt(0).toUpperCase();
        if (nameEl) nameEl.textContent = state.currentUser.name;
    }

    applyRoleBasedNavigation();

    // Logout handler
    const logoutBtn = document.getElementById('logout-btn');
    if (logoutBtn) {
        logoutBtn.onclick = () => {
            clearAuthState();
            showAuthScreen();
        };
    }
}

// --- Initialization ---

document.addEventListener('DOMContentLoaded', () => {
    // Check authentication
    if (state.authToken && state.currentUser) {
        showApp();
        // Validate token with a lightweight API call
        api.get('/stats/').catch(() => {
            // Token invalid/expired - redirect to login
            clearAuthState();
            autoLogin();
        });
    } else {
        autoLogin();
    }

    // Nav Clicks
    elements.navItems.forEach(item => {
        item.onclick = () => {
            if (!item.dataset.view) return;
            closeMobileSidebar();
            switchView(item.dataset.view);
        };
    });

    // New Project Click
    elements.newProjectBtn.onclick = handleNewProject;

    // Close Modal
    elements.closeModalBtn.onclick = () => elements.modalOverlay.classList.add('hidden');
    elements.modalOverlay.onclick = (e) => {
        if (e.target === elements.modalOverlay) elements.modalOverlay.classList.add('hidden');
    };

    // Theme & Lang
    elements.themeToggle.onclick = toggleTheme;
    elements.langToggle.onclick = toggleLang;

    // Search Bar
    const searchInput = document.querySelector('.search-bar input');
    let searchTimeout;
    if (searchInput) {
        searchInput.oninput = () => {
            clearTimeout(searchTimeout);
            searchTimeout = setTimeout(() => handleSearch(searchInput.value), 300);
        };
        searchInput.onkeydown = (e) => {
            if (e.key === 'Escape') {
                searchInput.value = '';
                handleSearch('');
                searchInput.blur();
            }
        };
    }

    // Mobile Hamburger
    const hamburger = document.getElementById('mobile-hamburger');
    const sidebarOverlay = document.getElementById('sidebar-overlay');
    if (hamburger) hamburger.onclick = openMobileSidebar;
    if (sidebarOverlay) sidebarOverlay.onclick = closeMobileSidebar;

    if (elements.sidebarToggleBtn) {
        elements.sidebarToggleBtn.onclick = toggleSidebarCollapsed;
    }

    // Keyboard Shortcut: Ctrl+K to focus search
    document.addEventListener('keydown', (e) => {
        if ((e.ctrlKey || e.metaKey) && e.key === 'k') {
            e.preventDefault();
            if (searchInput) searchInput.focus();
        }
    });

    // Global Escape handler: close modal â†’ close mobile sidebar
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape') {
            if (!elements.modalOverlay.classList.contains('hidden')) {
                elements.modalOverlay.classList.add('hidden');
                return;
            }
            const confirmOverlay = document.getElementById('confirm-overlay');
            if (confirmOverlay && !confirmOverlay.classList.contains('hidden')) {
                return; // handled by showConfirmDialog's own handler
            }
            if (document.querySelector('.sidebar.open')) {
                closeMobileSidebar();
            }
        }
    });

    // Sidebar keyboard navigation: Enter/Space triggers switchView
    elements.navItems.forEach(item => {
        item.onkeydown = (e) => {
            if (!item.dataset.view) return;
            if (e.key === 'Enter' || e.key === ' ') {
                e.preventDefault();
                closeMobileSidebar();
                switchView(item.dataset.view);
            }
        };
    });

    // Offline detection
    const offlineBanner = document.getElementById('offline-banner');
    if (offlineBanner) {
        const updateOnlineStatus = () => {
            if (navigator.onLine) {
                offlineBanner.classList.remove('visible');
                return;
            }
            offlineBanner.classList.add('visible');
        };
        globalThis.addEventListener('online', updateOnlineStatus);
        globalThis.addEventListener('offline', updateOnlineStatus);
        updateOnlineStatus();
    }

    // Init State
    if (state.theme === 'light') {
        document.body.classList.add('light-theme');
        document.body.classList.remove('dark-theme');
        elements.themeToggle.querySelector('i').className = 'fas fa-moon';
    } else {
        document.body.classList.add('dark-theme');
        document.body.classList.remove('light-theme');
        elements.themeToggle.querySelector('i').className = 'fas fa-sun';
    }

    applySidebarCollapsedState();

    // Initial View
    applyTranslations();
    switchView('projects');
});

################################################################################
# FILE: frontend\index.html
################################################################################

<!DOCTYPE html>
<html lang="ar" dir="rtl">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tawasul | Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø°ÙƒÙŠØ©</title>
    <link rel="icon" href="favicon.svg" type="image/svg+xml">
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+Arabic:wght@300;400;500;600;700&family=Space+Grotesk:wght@300;400;500;600;700&display=swap"
        rel="stylesheet">
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <!-- Custom Styles -->
    <link rel="stylesheet" href="style.css?v=20260223-3">
</head>

<body class="light-theme">
    <!-- Offline Banner -->
    <div id="offline-banner" class="offline-banner">
        <i class="fas fa-wifi"></i>
        <span data-i18n="offline_banner">You are offline</span>
    </div>

    <!-- Auth Screen -->
    <div id="auth-screen" class="auth-screen">
        <div class="auth-card">
            <div class="auth-logo">
                <i class="fas fa-brain-circuit"></i>
                <span>Tawasul</span>
            </div>
            <p class="auth-subtitle" id="auth-subtitle">Ø³Ø¬Ù‘Ù„ Ø¯Ø®ÙˆÙ„Ùƒ Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø©</p>

            <div id="auth-tabs" class="auth-tabs">
                <button class="auth-tab active" data-tab="login">ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„</button>
                <button class="auth-tab" data-tab="register">Ø­Ø³Ø§Ø¨ Ø¬Ø¯ÙŠØ¯</button>
            </div>

            <!-- Login Form -->
            <form id="login-form" class="auth-form" autocomplete="on">
                <div class="form-group">
                    <label data-i18n="book_email">Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ</label>
                    <input type="email" id="login-email" class="form-control" required autocomplete="email">
                </div>
                <div class="form-group">
                    <label>ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±</label>
                    <input type="password" id="login-password" class="form-control" required
                        autocomplete="current-password">
                </div>
                <button type="submit" class="btn btn-primary w-100">
                    <i class="fas fa-sign-in-alt"></i> ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„
                </button>
            </form>

            <!-- Register Form -->
            <form id="register-form" class="auth-form" style="display:none;" autocomplete="on">
                <div class="form-group">
                    <label data-i18n="book_name">Ø§Ù„Ø§Ø³Ù…</label>
                    <input type="text" id="register-name" class="form-control" required autocomplete="name">
                </div>
                <div class="form-group">
                    <label data-i18n="book_email">Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ</label>
                    <input type="email" id="register-email" class="form-control" required autocomplete="email">
                </div>
                <div class="form-group">
                    <label>ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±</label>
                    <input type="password" id="register-password" class="form-control" minlength="6" required
                        autocomplete="new-password">
                </div>
                <button type="submit" class="btn btn-primary w-100">
                    <i class="fas fa-user-plus"></i> Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø³Ø§Ø¨
                </button>
            </form>

            <p id="auth-error" class="auth-error" style="display:none;"></p>
        </div>
    </div>

    <div class="app-container" id="app-container" style="display:none;">
        <!-- Sidebar -->
        <aside class="sidebar">
            <div class="sidebar-header">
                <div class="logo">
                    <i class="fas fa-brain-circuit"></i>
                    <span>Tawasul</span>
                </div>
                <button id="sidebar-toggle-btn" class="icon-btn sidebar-toggle-btn" title="Toggle sidebar"
                    aria-label="Toggle sidebar">
                    <i class="fas fa-angles-right"></i>
                </button>
            </div>

            <nav class="sidebar-nav" role="navigation" aria-label="Main navigation">
                <ul>
                    <li class="active" data-view="projects" tabindex="0" role="button">
                        <i class="fas fa-folder-tree"></i>
                        <span data-i18n="nav_projects">Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹</span>
                    </li>
                    <li data-view="chat" tabindex="0" role="button">
                        <i class="fas fa-comment-dots"></i>
                        <span data-i18n="nav_chat">AI Interview</span>
                    </li>
                    <li data-view="srs" tabindex="0" role="button">
                        <i class="fas fa-file-signature"></i>
                        <span data-i18n="nav_srs">SRS Documents</span>
                    </li>
                    <li class="sidebar-divider" aria-hidden="true"></li>
                    <li data-view="settings" tabindex="0" role="button">
                        <i class="fas fa-sliders"></i>
                        <span data-i18n="nav_settings">Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª</span>
                    </li>
                </ul>
            </nav>

            <div class="sidebar-footer">
                <div class="user-info">
                    <div class="avatar" id="user-avatar">A</div>
                    <div class="details">
                        <p class="name" id="user-name">Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…</p>
                        <p class="status">Ù…ØªØµÙ„</p>
                    </div>
                </div>
                <button id="logout-btn" class="icon-btn" title="ØªØ³Ø¬ÙŠÙ„ Ø®Ø±ÙˆØ¬" aria-label="Logout">
                    <i class="fas fa-sign-out-alt"></i>
                </button>
                <button id="lang-toggle" class="icon-btn" title="Switch Language" aria-label="Switch language">
                    <span class="lang-code">EN</span>
                </button>
                <button id="theme-toggle" class="icon-btn" title="Toggle theme" aria-label="Toggle theme">
                    <i class="fas fa-moon"></i>
                </button>
            </div>
        </aside>

        <!-- Sidebar Overlay (mobile) -->
        <div class="sidebar-overlay" id="sidebar-overlay"></div>

        <!-- Main Content -->
        <main class="main-content" role="main">
            <header class="top-bar">
                <button class="mobile-hamburger" id="mobile-hamburger" aria-label="Toggle menu" aria-expanded="false">
                    <i class="fas fa-bars"></i>
                </button>
                <div class="search-bar">
                    <i class="fas fa-search"></i>
                    <input type="text" placeholder="Ø§Ø¨Ø­Ø« Ø¹Ù† Ù…Ø´Ø±ÙˆØ¹ Ø£Ùˆ Ù…Ø³ØªÙ†Ø¯...">
                </div>
                <div class="actions">
                    <button id="new-project-btn" class="btn btn-primary">
                        <i class="fas fa-lightbulb"></i>
                        <span data-i18n="start_idea_btn">Ø§Ø¨Ø¯Ø£ ÙÙƒØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©</span>
                    </button>
                </div>
            </header>

            <div id="view-container" class="view-container">
                <!-- Views will be injected here -->
                <div class="loader-container">
                    <div class="loader"></div>
                </div>
            </div>
        </main>
    </div>

    <!-- Modals -->
    <div id="modal-overlay" class="modal-overlay hidden" role="dialog" aria-modal="true" aria-labelledby="modal-title">
        <div class="modal">
            <div class="modal-header">
                <h3 id="modal-title">Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù†Ø§ÙØ°Ø©</h3>
                <button class="close-modal" title="Close" aria-label="Close dialog"><i
                        class="fas fa-times"></i></button>
            </div>
            <div id="modal-body" class="modal-body">
                <!-- Modal content -->
            </div>
        </div>
    </div>

    <!-- Confirm Dialog -->
    <div id="confirm-overlay" class="modal-overlay hidden" role="dialog" aria-modal="true">
        <div class="modal confirm-dialog-content">
            <p id="confirm-message"></p>
            <div class="confirm-dialog-actions">
                <button id="confirm-cancel-btn" class="btn btn-secondary"></button>
                <button id="confirm-ok-btn" class="btn btn-danger"></button>
            </div>
        </div>
    </div>

    <!-- Templates -->
    <template id="dashboard-template">
        <div class="dashboard-view">
            <h1 class="view-title" data-i18n="welcome_title">Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Tawasul</h1>
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="icon"><i class="fas fa-folder"></i></div>
                    <div class="info">
                        <span class="label" data-i18n="stat_projects">Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹</span>
                        <span class="value" id="stat-projects-dashboard">0</span>
                    </div>
                </div>
                <div class="stat-card">
                    <div class="icon"><i class="fas fa-file-alt"></i></div>
                    <div class="info">
                        <span class="label" data-i18n="stat_docs">Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª</span>
                        <span class="value" id="stat-docs-dashboard">0</span>
                    </div>
                </div>
            </div>

            <section class="recent-projects">
                <div class="section-header">
                    <h2 data-i18n="recent_projects">Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø£Ø®ÙŠØ±Ø©</h2>
                    <a href="#" class="link" data-i18n="view_all">Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙ„</a>
                </div>
                <div class="projects-grid" id="recent-projects-list-dashboard">
                    <!-- Projects will be injected here -->
                </div>
            </section>
        </div>
    </template>

    <template id="projects-template">
        <div class="projects-view">
            <div class="stats-grid compact-stats">
                <div class="stat-card">
                    <div class="icon"><i class="fas fa-folder"></i></div>
                    <div class="info">
                        <span class="label" data-i18n="stat_projects">Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹</span>
                        <span class="value" id="stat-projects">0</span>
                    </div>
                </div>
                <div class="stat-card">
                    <div class="icon"><i class="fas fa-file-alt"></i></div>
                    <div class="info">
                        <span class="label" data-i18n="stat_docs">Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª</span>
                        <span class="value" id="stat-docs">0</span>
                    </div>
                </div>
            </div>
            <div class="section-header">
                <h1 class="view-title" data-i18n="your_projects">Ù…Ø´Ø§Ø±ÙŠØ¹Ùƒ</h1>
                <button id="projects-new-project-btn" class="btn btn-primary">
                    <i class="fas fa-plus"></i>
                    <span data-i18n="start_idea_btn">Ø§Ø¨Ø¯Ø£ ÙÙƒØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©</span>
                </button>
            </div>
            <div class="projects-grid" id="all-projects-list">
                <!-- Projects will be injected here -->
            </div>
        </div>
    </template>

    <template id="chat-template">
        <div class="chat-view-pro">
            <!-- Chat Header -->
            <div class="chat-header-pro">
                <div class="chat-header-content">
                    <div class="chat-header-left">
                        <div id="chat-project-title" class="chat-project-pill">Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: -</div>
                    </div>

                    <div class="interview-progress" id="interview-progress" style="display:none;">
                        <div class="interview-coverage-grid">
                            <div class="coverage-item" data-area="discovery" data-step="1" title="Discovery 0%">
                                <span class="coverage-dot" id="coverage-discovery">1</span>
                            </div>
                            <div class="coverage-item" data-area="scope" data-step="2" title="Scope 0%">
                                <span class="coverage-dot" id="coverage-scope">2</span>
                            </div>
                            <div class="coverage-item" data-area="users" data-step="3" title="Users 0%">
                                <span class="coverage-dot" id="coverage-users">3</span>
                            </div>
                            <div class="coverage-item" data-area="features" data-step="4" title="Features 0%">
                                <span class="coverage-dot" id="coverage-features">4</span>
                            </div>
                            <div class="coverage-item" data-area="constraints" data-step="5" title="Constraints 0%">
                                <span class="coverage-dot" id="coverage-constraints">5</span>
                            </div>
                        </div>
                        <div class="coverage-pcts" aria-hidden="true">
                            <span id="coverage-pct-discovery">0%</span>
                            <span id="coverage-pct-scope">0%</span>
                            <span id="coverage-pct-users">0%</span>
                            <span id="coverage-pct-features">0%</span>
                            <span id="coverage-pct-constraints">0%</span>
                        </div>
                    </div>

                    <div class="chat-controls chat-controls-hidden">
                        <select id="chat-project-select" class="chat-select" style="display: none;">
                            <option value="" data-i18n="select_project_ph">Ø§Ø®ØªØ± Ù…Ø´Ø±ÙˆØ¹Ø§Ù‹...</option>
                        </select>
                        <select id="chat-lang" class="chat-select chat-select-small" style="display: none;">
                            <option value="ar">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</option>
                            <option value="en">English</option>
                        </select>
                        <label class="chat-toggle" style="display: none;">
                            <input type="checkbox" id="chat-interview-toggle" checked>
                            <span class="chat-toggle-slider"></span>
                            <span class="chat-toggle-label" data-i18n="interview_mode">ÙˆØ¶Ø¹ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©</span>
                        </label>
                        <button id="chat-files-btn" class="chat-upload-ref-btn" title="Project Files"
                            style="display: none;">
                            <i class="fas fa-folder-open"></i>
                            <span class="chat-upload-ref-label" data-i18n="project_files_btn">Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹</span>
                        </button>
                    </div>
                    <div class="chat-header-actions">
                        <button id="summary-toggle-btn" class="chat-clear-btn top-icon-btn" title="Ù…Ù„Ø®Øµ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª"
                            aria-label="Toggle requirements summary" data-tooltip="Ù…Ù„Ø®Øµ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª">
                            <i class="fas fa-table-columns"></i>
                            <span>Ù…Ù„Ø®Øµ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª</span>
                        </button>
                        <button id="interview-resume-btn" class="chat-clear-btn top-icon-btn" title="Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø¢Ø®Ø± Ù…Ù‚Ø§Ø¨Ù„Ø©"
                            aria-label="Resume last interview" data-tooltip="Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø¢Ø®Ø± Ù…Ù‚Ø§Ø¨Ù„Ø©" disabled>
                            <i class="fas fa-rotate-left"></i>
                            <span>Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø¢Ø®Ø± Ù…Ù‚Ø§Ø¨Ù„Ø©</span>
                        </button>
                        <button id="interview-save-later-btn" class="chat-clear-btn top-icon-btn"
                            title="Ø§Ø­ÙØ¸ ÙˆÙƒÙ…Ù„ Ø¨Ø¹Ø¯ÙŠÙ†" aria-label="Save and continue later" data-tooltip="Ø§Ø­ÙØ¸ ÙˆÙƒÙ…Ù„ Ø¨Ø¹Ø¯ÙŠÙ†">
                            <i class="fas fa-floppy-disk"></i>
                            <span>Ø§Ø­ÙØ¸ ÙˆÙƒÙ…Ù„ Ø¨Ø¹Ø¯ÙŠÙ†</span>
                        </button>
                        <button id="interview-review-btn" class="chat-clear-btn top-icon-btn" title="Ù…Ø±Ø§Ø¬Ø¹Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„"
                            aria-label="Review before submit" data-tooltip="Ù…Ø±Ø§Ø¬Ø¹Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„" disabled>
                            <i class="fas fa-list-check"></i>
                            <span>Ù…Ø±Ø§Ø¬Ø¹Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„</span>
                        </button>
                        <button id="clear-chat-btn" class="chat-clear-btn top-icon-btn" title="Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"
                            aria-label="Clear chat" data-tooltip="Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©">
                            <i class="fas fa-trash-alt"></i>
                        </button>
                    </div>
                </div>
            </div>

            <!-- Project Files Drawer -->
            <div class="files-drawer" id="files-drawer" style="display:none;">
                <div class="files-drawer-header">
                    <h4><i class="fas fa-folder-open"></i> <span data-i18n="project_files_btn">Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹</span></h4>
                    <button id="files-drawer-close" class="live-doc-close" title="Close">
                        <i class="fas fa-times"></i>
                    </button>
                </div>
                <div class="files-drawer-body">
                    <div class="upload-zone" id="upload-zone">
                        <i class="fas fa-cloud-upload-alt"></i>
                        <p data-i18n="upload_desc">Ø§Ø³Ø­Ø¨ Ø§Ù„Ù…Ù„ÙØ§Øª Ù‡Ù†Ø§ Ø£Ùˆ Ø§Ø¶ØºØ· Ù„Ù„Ø§Ø®ØªÙŠØ§Ø±</p>
                        <span class="hint" data-i18n="upload_hint">ÙŠØ¯Ø¹Ù… PDF, TXT, DOCX</span>
                        <input type="file" id="file-input" multiple hidden accept=".pdf,.txt,.docx">
                    </div>
                    <div class="documents-list" id="project-docs-list"></div>
                </div>
            </div>

            <!-- Messages Area -->

            <div class="chat-body-wrapper">
                <div class="chat-messages-pro" id="chat-messages">
                    <div class="welcome-msg-pro">
                        <div class="welcome-icon">
                            <i class="fas fa-robot"></i>
                        </div>
                        <h2>Ù„Ù†Ø¨Ø¯Ø£ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª</h2>
                        <p>Ø§ÙƒØªØ¨ ØªÙØ§ØµÙŠÙ„ Ù…Ø´Ø±ÙˆØ¹Ùƒ ÙˆØ³Ø£Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù…Ø±ØªØ¨Ø·Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ø¨Ù†Ø§Ø¡ SRS Ø§Ø­ØªØ±Ø§ÙÙŠ.</p>
                        <div class="welcome-suggestions">
                            <button class="suggestion-chip">Ù…Ø§ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØ­Ù„Ù‡Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ØŸ</button>
                            <button class="suggestion-chip">Ù…ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠÙŠÙ† ÙˆØ¥ÙŠÙ‡ Ø£ÙˆÙ„ÙˆÙŠØ§ØªÙ‡Ù…ØŸ</button>
                            <button class="suggestion-chip">Ù…Ø§ Ø£Ù‡Ù… Ù…ØªØ·Ù„Ø¨Ø§Øª MVP ÙˆØ§Ù„Ù‚ÙŠÙˆØ¯ØŸ</button>
                        </div>
                    </div>
                </div>

                <!-- Live Document Panel (Co-pilot) -->
                <aside class="live-doc-panel" id="live-doc-panel" style="display:none;">
                    <div class="live-doc-header">
                        <h3><i class="fas fa-file-lines"></i> <span data-i18n="live_doc_title">Ù…Ù„Ø®Øµ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª</span>
                        </h3>
                        <button id="live-doc-close" class="live-doc-close" title="Close" aria-label="Close panel">
                            <i class="fas fa-times"></i>
                        </button>
                    </div>
                    <div class="live-doc-content" id="live-doc-content">
                        <div class="live-doc-empty">
                            <i class="fas fa-pencil-alt"></i>
                            <p data-i18n="live_doc_empty">Ø§Ø¨Ø¯Ø£ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© ÙˆØ³ÙŠØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù„Ø®Øµ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹...</p>
                        </div>
                    </div>
                </aside>
            </div>

            <div class="chat-input-pro">
                <div class="chat-input-wrapper-pro">
                    <textarea id="chat-input" placeholder="Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§..." rows="1"></textarea>
                    <button id="mic-btn" class="mic-btn-pro" title="ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ" aria-label="Voice recording">
                        <i class="fas fa-microphone"></i>
                    </button>
                    <button id="send-btn" class="send-btn-pro" disabled>
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </div>
            </div>

        </div>
    </template>

    <template id="srs-template">
        <div class="srs-view">
            <div class="srs-header">
                <div>
                    <h1 class="view-title" data-i18n="srs_title">Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª (SRS)</h1>
                    <p class="view-subtitle" data-i18n="srs_subtitle">Ù†Ø³Ø®Ø© Ø£ÙˆÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ù…Ø¹ Ù†Ù‚Ø§Ø· ØªØ­ØªØ§Ø¬ ØªÙˆØ¶ÙŠØ­.</p>
                </div>
                <div class="srs-actions">
                    <select id="srs-project-select" class="srs-select">
                        <option value="" data-i18n="select_project_ph">Ø§Ø®ØªØ± Ù…Ø´Ø±ÙˆØ¹Ø§Ù‹...</option>
                    </select>
                    <select id="srs-export-format" class="srs-select srs-export-format">
                        <option value="pdf">PDF</option>
                        <option value="word">Word</option>
                        <option value="markdown">Markdown</option>
                    </select>
                    <button id="srs-refresh-btn" class="btn btn-secondary">
                        <i class="fas fa-rotate"></i>
                        <span data-i18n="srs_refresh">ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø³ÙˆØ¯Ø©</span>
                    </button>
                    <button id="srs-export-btn" class="btn btn-primary" disabled>
                        <i class="fas fa-download"></i>
                        <span data-i18n="srs_export">ØªØµØ¯ÙŠØ± SRS</span>
                    </button>
                </div>
            </div>

            <div class="srs-grid">
                <section class="srs-draft">
                    <div class="srs-draft-header">
                        <div class="status-pill" id="srs-status">Ù…Ø³ÙˆØ¯Ø© Ø£ÙˆÙ„ÙŠØ©</div>
                        <div class="srs-meta" id="srs-updated">Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: Ø§Ù„ÙŠÙˆÙ…</div>
                    </div>
                    <div class="srs-paper-canvas">
                        <div id="srs-sections" class="srs-sections"></div>
                    </div>
                </section>

                <aside class="srs-summary">
                    <div class="summary-card">
                        <h3 data-i18n="srs_summary_title">Ù…Ù„Ø®Øµ Ø³Ø±ÙŠØ¹</h3>
                        <p id="srs-summary-text"></p>
                        <div class="summary-metrics" id="srs-metrics"></div>
                    </div>
                    <div class="summary-card">
                        <h3 data-i18n="srs_open_questions">Ù†Ù‚Ø§Ø· ØªØ­ØªØ§Ø¬ ØªÙˆØ¶ÙŠØ­</h3>
                        <ul id="srs-open-questions" class="summary-list"></ul>
                    </div>
                    <div class="summary-card highlight">
                        <h3 data-i18n="srs_next_steps">Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©</h3>
                        <ul id="srs-next-steps" class="summary-list"></ul>
                        <button id="srs-confirm-btn" class="btn btn-primary w-100 mt-4">
                            <i class="fas fa-check-circle"></i>
                            <span data-i18n="srs_confirm">Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª</span>
                        </button>
                        <button id="srs-book-btn" class="btn btn-ghost w-100 mt-4">
                            <i class="fas fa-calendar-check"></i>
                            <span data-i18n="srs_book_meeting">Ø§Ø­Ø¬Ø² Ø¬Ù„Ø³Ø© Ù…Ø¹ Ø§Ù„ÙØ±ÙŠÙ‚</span>
                        </button>
                    </div>
                </aside>
            </div>
        </div>
    </template>

    <template id="ai-config-template">
        <div class="config-view">
            <h1 class="view-title" data-i18n="ai_config_title">Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ</h1>
            <div class="config-card">
                <p class="config-desc" data-i18n="ai_settings_desc">Ø§Ø®ØªØ± Ù…Ø²ÙˆØ¯ Ø§Ù„ØªÙˆÙ„ÙŠØ¯.</p>
                <div class="config-section">
                    <label for="ai-gen-provider" class="config-label" data-i18n="gen_provider_label">Ù…Ø²ÙˆØ¯
                        Ø§Ù„ØªÙˆÙ„ÙŠØ¯</label>
                    <select id="ai-gen-provider" class="form-control"></select>
                </div>
                <button id="save-ai-config-btn" class="btn btn-primary mt-4">
                    <i class="fas fa-save"></i>
                    <span data-i18n="save_settings">Ø­ÙØ¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª</span>
                </button>
            </div>
        </div>
    </template>

    <template id="settings-template">
        <div class="settings-view">
            <h1 class="view-title" data-i18n="settings_title">Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª</h1>
            <div class="config-card">
                <div class="config-section">
                    <h3 data-i18n="settings_tab_ai">Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ</h3>
                    <p class="config-desc" data-i18n="ai_settings_desc">Ø§Ø®ØªØ± Ù…Ø²ÙˆØ¯ Ø§Ù„ØªÙˆÙ„ÙŠØ¯.</p>
                    <select id="settings-ai-gen-provider" class="form-control"></select>
                    <button id="settings-save-ai-config-btn" class="btn btn-primary mt-4">
                        <i class="fas fa-save"></i>
                        <span data-i18n="save_settings">Ø­ÙØ¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª</span>
                    </button>
                </div>
                <div class="config-section">
                    <h3 data-i18n="bot_active_project">Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ù†Ø´Ø·</h3>
                    <p class="config-desc" data-i18n="bot_active_project_desc">Ø§Ø®ØªØ± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø°ÙŠ Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¨ÙˆØª Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                        Ù…Ù†Ù‡.</p>
                    <select id="bot-active-project" class="form-control">
                        <option value="">Ø§Ø®ØªØ± Ù…Ø´Ø±ÙˆØ¹Ø§Ù‹...</option>
                    </select>
                    <button id="save-bot-config-btn" class="btn btn-primary mt-4">
                        <i class="fas fa-save"></i>
                        <span data-i18n="save_settings">Ø­ÙØ¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª</span>
                    </button>
                </div>
                <div class="config-section mt-4">
                    <h3 data-i18n="bot_profile">Ù…Ù„Ù Ø§Ù„Ø¨ÙˆØª</h3>
                    <p class="config-desc" data-i18n="bot_profile_desc">ØªØ­Ø¯ÙŠØ« Ø§Ø³Ù… Ø§Ù„Ø¨ÙˆØª Ø¹Ù„Ù‰ ØªÙ„ÙŠØ¬Ø±Ø§Ù….</p>
                    <div class="form-group">
                        <label for="bot-name-input" data-i18n="bot_name">Ø§Ø³Ù… Ø§Ù„Ø¨ÙˆØª</label>
                        <input type="text" id="bot-name-input" class="form-control" placeholder="Tawasul Bot">
                    </div>
                    <button id="update-bot-profile-btn" class="btn btn-primary mt-4">
                        <i class="fas fa-user-edit"></i>
                        <span data-i18n="update_profile">ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠ</span>
                    </button>
                </div>
                <div class="config-section mt-4" style="border-top: 2px solid var(--border); padding-top: 1.5rem;">
                    <h3><i class="fab fa-telegram" style="color:#0088cc;margin-inline-end:8px;"></i> Connect Telegram
                        Bot</h3>
                    <p class="config-desc">
                        Link your Telegram bot by entering the credentials below.
                        Get a bot token from <a href="https://t.me/BotFather" target="_blank"
                            style="color:var(--primary);font-weight:600;">@BotFather</a> on Telegram.
                    </p>
                    <div class="form-group">
                        <label for="tg-bot-token">Bot Token <span
                                style="color:var(--danger);font-weight:bold;">*</span></label>
                        <div style="position:relative;">
                            <input type="password" id="tg-bot-token" class="form-control"
                                placeholder="123456789:ABCdefGHIjklMNOpqrSTUvwxYZ" style="padding-inline-end:3rem;">
                            <button type="button" id="tg-token-toggle" class="icon-btn"
                                style="position:absolute;top:50%;inset-inline-end:0.5rem;transform:translateY(-50%);font-size:0.85rem;"
                                title="Show/Hide">
                                <i class="fas fa-eye"></i>
                            </button>
                        </div>
                        <small style="color:var(--text-secondary);">Paste the full token from @BotFather (e.g.
                            <code>123456789:ABCdef...</code>)</small>
                    </div>
                    <div class="form-group">
                        <label for="tg-admin-id">Admin Telegram ID</label>
                        <input type="text" id="tg-admin-id" class="form-control" placeholder="@username or numeric ID">
                        <small style="color:var(--text-secondary);">Your Telegram username or numeric ID (get from <a
                                href="https://t.me/userinfobot" target="_blank"
                                style="color:var(--primary);">@userinfobot</a>)</small>
                    </div>
                    <div style="display:grid;grid-template-columns:1fr 1fr;gap:1rem;">
                        <div class="form-group">
                            <label for="tg-api-email">Bot API Email</label>
                            <input type="email" id="tg-api-email" class="form-control" placeholder="admin@tawasul.com">
                        </div>
                        <div class="form-group">
                            <label for="tg-api-password">Bot API Password</label>
                            <input type="password" id="tg-api-password" class="form-control"
                                placeholder="Leave empty to keep current">
                        </div>
                    </div>
                    <div class="form-group">
                        <label for="tg-api-url">API Base URL</label>
                        <input type="text" id="tg-api-url" class="form-control" placeholder="http://localhost:8500">
                        <small style="color:var(--text-secondary);">Backend server URL the bot connects to</small>
                    </div>
                    <div id="tg-config-status"
                        style="display:none;padding:0.75rem 1rem;border-radius:8px;margin-bottom:1rem;font-size:0.9rem;">
                    </div>
                    <button id="save-tg-config-btn" class="btn btn-primary mt-4">
                        <i class="fab fa-telegram"></i>
                        <span>Save Telegram Config</span>
                    </button>
                    <p style="margin-top:1rem;font-size:0.85rem;color:var(--text-secondary);">
                        <i class="fas fa-info-circle"></i>
                        After saving, <strong>restart the Telegram bot</strong> process for changes to take effect.
                    </p>
                </div>
            </div>
        </div>
    </template>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script src="app.js?v=20260225-2"></script>
</body>

</html>

################################################################################
# FILE: frontend\style.css
################################################################################

:root {
    /* Color Palette - Calm Mint Light */
    --bg-main: #eef8f5;
    --bg-sidebar: #ffffff;
    --bg-card: #ffffff;
    --bg-card-hover: #e4f3ef;
    --accent-primary: #2db296;
    --accent-secondary: #5fc8b2;
    --text-main: #16322d;
    --text-muted: #5f7972;
    --border-color: #cfe5dd;
    --glass-bg: rgba(255, 255, 255, 0.7);
    --glass-border: rgba(207, 229, 221, 0.9);
    --success: #16a34a;
    --error: #dc2626;
    --warning: #14b8a6;
    --shadow-soft: 0 14px 30px rgba(31, 24, 18, 0.08);

    /* Transitions */
    --transition-fast: 0.2s ease;
    --transition-slow: 0.4s cubic-bezier(0.4, 0, 0.2, 1);
}

/* Dark Mode Overrides */
body.dark-theme {
    --bg-main: #0b0d12;
    --bg-sidebar: #12141c;
    --bg-card: rgba(255, 255, 255, 0.04);
    --bg-card-hover: rgba(255, 255, 255, 0.08);
    --text-main: #f7f5f2;
    --text-muted: #a3a6ad;
    --border-color: rgba(255, 255, 255, 0.08);
    --glass-bg: rgba(255, 255, 255, 0.04);
    --glass-border: rgba(255, 255, 255, 0.08);
}

body.dark-theme .main-content {
    background: radial-gradient(circle at top left, rgba(45, 178, 150, 0.16), transparent 55%);
}

body.dark-theme .stat-card,
body.dark-theme .project-card {
    box-shadow: 0 8px 20px rgba(0, 0, 0, 0.4);
}

body.dark-theme .suggestion-card {
    border-color: rgba(255, 255, 255, 0.15);
    background: rgba(255, 255, 255, 0.06);
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.25);
}

body.dark-theme .suggestion-card:hover:not(.disabled) {
    border-color: var(--accent-primary);
    background: rgba(45, 178, 150, 0.12);
    box-shadow: 0 4px 14px rgba(45, 178, 150, 0.18);
}

body.dark-theme .suggestion-card.selected {
    border-color: var(--accent-primary);
    background: rgba(45, 178, 150, 0.15);
}

body.dark-theme .suggestion-card-radio {
    border-color: rgba(255, 255, 255, 0.25);
}

* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: 'Space Grotesk', 'IBM Plex Sans Arabic', sans-serif;
    background-color: var(--bg-main);
    color: var(--text-main);
    overflow: hidden;
    line-height: 1.6;
}

/* App Layout */
.app-container {
    display: flex;
    height: 100vh;
    width: 100vw;
}

/* Sidebar */
.sidebar {
    width: 280px;
    background-color: var(--bg-sidebar);
    border-left: 1px solid var(--border-color);
    display: flex;
    flex-direction: column;
    padding: 24px;
    z-index: 100;
    transition: width var(--transition-slow), padding var(--transition-slow), border-color var(--transition-fast);
}

.sidebar-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
}

.sidebar-header .logo {
    display: flex;
    align-items: center;
    gap: 12px;
    font-size: 24px;
    font-weight: 700;
    color: var(--accent-primary);
    margin-bottom: 40px;
}

.sidebar-toggle-btn {
    width: 36px;
    height: 36px;
    display: inline-flex;
    align-items: center;
    justify-content: center;
}

.app-container.sidebar-collapsed .sidebar {
    width: 56px;
    padding: 12px 8px;
    overflow: hidden;
}

.app-container.sidebar-collapsed .sidebar-header {
    justify-content: center;
}

.app-container.sidebar-collapsed .sidebar-header .logo,
.app-container.sidebar-collapsed .sidebar-nav,
.app-container.sidebar-collapsed .sidebar-footer {
    display: none;
}

.app-container.sidebar-collapsed .sidebar-toggle-btn i {
    transform: rotate(180deg);
}

.sidebar-nav ul {
    list-style: none;
}
.sidebar-divider {
    height: 1px;
    margin: 10px 12px 14px;
    background: var(--border-color);
    border-radius: 999px;
    pointer-events: none;
}

.sidebar-nav li {
    display: flex;
    align-items: center;
    gap: 12px;
    padding: 14px 18px;
    border-radius: 12px;
    margin-bottom: 8px;
    cursor: pointer;
    color: var(--text-muted);
    transition: var(--transition-fast);
}

.sidebar-nav li:hover {
    background-color: var(--bg-card-hover);
    color: var(--text-main);
}

.sidebar-nav li.active {
    background-color: var(--accent-primary);
    color: white;
    box-shadow: 0 4px 12px rgba(45, 178, 150, 0.3);
}

.sidebar-nav li.sidebar-divider,
.sidebar-nav li.sidebar-divider:hover,
.sidebar-nav li.sidebar-divider.active {
    display: block;
    padding: 0;
    margin: 10px 12px 14px;
    height: 1px;
    background: var(--border-color);
    border-radius: 999px;
    box-shadow: none;
    cursor: default;
}

.sidebar-footer {
    margin-top: auto;
    display: flex;
    align-items: center;
    justify-content: space-between;
    padding-top: 24px;
    border-top: 1px solid var(--border-color);
}

.user-info {
    display: flex;
    align-items: center;
    gap: 12px;
}

.avatar {
    width: 40px;
    height: 40px;
    background: linear-gradient(135deg, var(--accent-primary), var(--accent-secondary));
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    font-weight: 600;
}

.user-info .name {
    font-size: 14px;
    font-weight: 600;
}

.user-info .status {
    font-size: 12px;
    color: var(--success);
}

/* Main Content */
.main-content {
    flex: 1;
    display: flex;
    flex-direction: column;
    overflow: hidden;
    position: relative;
    background: radial-gradient(circle at 12% 8%, rgba(45, 178, 150, 0.12), transparent 45%),
        radial-gradient(circle at 88% 18%, rgba(31, 157, 143, 0.12), transparent 50%),
        linear-gradient(135deg, rgba(255, 255, 255, 0.5), transparent 40%);
}

.top-bar {
    height: 80px;
    padding: 0 40px;
    display: none;
    align-items: center;
    justify-content: space-between;
    border-bottom: 1px solid var(--border-color);
}

.search-bar {
    display: flex;
    align-items: center;
    background-color: var(--bg-card);
    padding: 10px 20px;
    border-radius: 30px;
    width: 400px;
    border: 1px solid var(--border-color);
}

.search-bar input {
    background: none;
    border: none;
    color: var(--text-main);
    margin-right: 12px;
    width: 100%;
    outline: none;
}

.view-container {
    flex: 1;
    padding: 40px;
    overflow-y: auto;
}

.view-container.chat-mode {
    padding: 0;
}

/* Dashboard */
.view-title {
    font-size: 32px;
    font-weight: 700;
    margin-bottom: 32px;
}

.stats-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
    gap: 24px;
    margin-bottom: 48px;
}
.compact-stats {
    margin-bottom: 24px;
}

.compact-stats .stat-card {
    padding: 18px;
    border-radius: 16px;
}

.compact-stats .stat-card .value {
    font-size: 22px;
}

.stat-card {
    background: var(--glass-bg);
    backdrop-filter: blur(10px);
    border: 1px solid var(--glass-border);
    padding: 24px;
    border-radius: 20px;
    display: flex;
    align-items: center;
    gap: 20px;
    transition: var(--transition-slow);
}

.stat-card:hover {
    transform: translateY(-5px);
    border-color: var(--accent-primary);
}

.stat-card .icon {
    width: 56px;
    height: 56px;
    background: rgba(45, 178, 150, 0.12);
    border-radius: 16px;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 24px;
    color: var(--accent-primary);
}

.stat-card .label {
    display: block;
    color: var(--text-muted);
    font-size: 14px;
}

.stat-card .value {
    font-size: 28px;
    font-weight: 700;
}

/* Projects Grid */
.projects-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
    gap: 24px;
}

.project-card {
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    border-radius: 20px;
    padding: 24px;
    cursor: pointer;
    transition: var(--transition-fast);
    position: relative;
    transition: width var(--transition-slow), padding var(--transition-slow), border-color var(--transition-fast);
}

.project-card:hover {
    background: var(--bg-card-hover);
    border-color: var(--accent-primary);
}

.project-card h3 {
    font-size: 20px;
    margin-bottom: 12px;
}

.project-card p {
    color: var(--text-muted);
    font-size: 14px;
    margin-bottom: 20px;
    display: -webkit-box;
    -webkit-line-clamp: 2;
    -webkit-box-orient: vertical;
    margin-bottom: 40px;
}

.project-meta {
    display: flex;
    justify-content: space-between;
    font-size: 12px;
    color: var(--text-muted);
    min-height: 44px;
}

.project-progress-wrap {
    margin-top: 12px;
    padding: 10px;
    border-radius: 12px;
    border: 1px solid var(--border-color);
    background: var(--glass-bg);
}

.project-progress-head {
    display: flex;
    align-items: center;
    justify-content: space-between;
    gap: 8px;
    margin-bottom: 8px;
    font-size: 12px;
    color: var(--text-muted);
}

.project-progress-head strong {
    color: var(--text-main);
}

.project-progress-track {
    width: 100%;
    height: 6px;
    border-radius: 999px;
    background: var(--border-color);
    overflow: hidden;
}

.project-progress-fill {
    height: 100%;
    width: 0;
    background: linear-gradient(90deg, var(--accent-primary), var(--accent-secondary));
}

/* Chat View - Professional ChatGPT-like Design */
.chat-view-pro {
    display: flex;
    flex-direction: column;
    height: 100%;
    background: var(--bg-main);
    border-radius: 0;
    overflow: hidden;
}

/* Chat Header */
.chat-header-pro {
    padding: 16px 24px;
    background: var(--bg-sidebar);
    border-bottom: 1px solid var(--border-color);
}

.chat-header-content {
    max-width: 100%;
    margin: 0;
    display: flex;
    align-items: center;
    justify-content: space-between;
    gap: 16px;
    flex-wrap: nowrap;
}

.chat-header-left {
    display: flex;
    align-items: center;
    min-width: 0;
}

.chat-project-pill {
    display: inline-flex;
    align-items: center;
    max-width: 420px;
    min-width: 120px;
    height: 34px;
    padding: 0 12px;
    border-radius: 999px;
    border: 1px solid var(--border-color);
    background: var(--bg-card);
    color: var(--text-main);
    font-size: 13px;
    font-weight: 600;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
}

.chat-header-actions {
    display: flex;
    align-items: center;
    gap: 8px;
    margin-inline-start: auto;
}

.chat-controls-hidden {
    display: none !important;
}

.top-icon-btn {
    position: relative;
}

.top-icon-btn span {
    display: none;
}

.top-icon-btn::after {
    content: attr(data-tooltip);
    position: absolute;
    bottom: calc(100% + 8px);
    right: 0;
    background: var(--bg-card);
    color: var(--text-main);
    border: 1px solid var(--border-color);
    border-radius: 8px;
    padding: 6px 10px;
    font-size: 12px;
    line-height: 1;
    white-space: nowrap;
    opacity: 0;
    transform: translateY(2px);
    pointer-events: none;
    transition: opacity 0.15s ease, transform 0.15s ease;
    z-index: 10;
}

.top-icon-btn:hover::after {
    opacity: 1;
    transform: translateY(0);
}

.top-icon-btn:disabled {
    opacity: 0.45;
    cursor: not-allowed;
}

#summary-toggle-btn.active {
    border-color: var(--accent-primary);
    color: var(--accent-primary);
    background: rgba(45, 178, 150, 0.08);
}

.chat-controls {
    display: flex;
    gap: 12px;
    flex: 1;
    flex-wrap: wrap;
}

.chat-select {
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    border-radius: 10px;
    padding: 10px 16px;
    color: var(--text-main);
    font-size: 14px;
    outline: none;
    cursor: pointer;
    transition: var(--transition-fast);
    min-width: 180px;
}

.chat-select:hover,
.chat-select:focus {
    border-color: var(--accent-primary);
}

.chat-select-small {
    min-width: 120px;
}

.chat-toggle {
    display: inline-flex;
    align-items: center;
    gap: 10px;
    padding: 8px 12px;
    border-radius: 999px;
    border: 1px solid var(--border-color);
    background: var(--bg-card);
    color: var(--text-main);
    font-size: 13px;
    cursor: pointer;
    transition: var(--transition-fast);
}

.chat-toggle:hover {
    border-color: var(--accent-primary);
}

.chat-toggle input {
    display: none;
}

.chat-toggle-slider {
    width: 34px;
    height: 18px;
    background: var(--border-color);
    border-radius: 999px;
    position: relative;
    transition: var(--transition-fast);
}

.chat-toggle-slider::after {
    content: "";
    margin-bottom: 24px;
    top: 2px;
    left: 2px;
    width: 14px;
    height: 14px;
    border-radius: 50%;
    background: #fff;
    transition: var(--transition-fast);
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
}

.chat-toggle input:checked + .chat-toggle-slider {
    background: var(--accent-primary);
}

.chat-toggle input:checked + .chat-toggle-slider::after {
    transform: translateX(16px);
}

.chat-toggle-label {
    white-space: nowrap;
}

.chat-clear-btn {
    background: transparent;
    border: 1px solid var(--border-color);
    border-radius: 10px;
    padding: 10px 14px;
    color: var(--text-muted);
    cursor: pointer;
    transition: var(--transition-fast);
}

.chat-clear-btn:hover {
    background: rgba(239, 68, 68, 0.1);
    border-color: var(--error);
    color: var(--error);
}

/* Messages Area */
.chat-messages-pro {
    flex: 1;
    overflow-y: auto;
    padding: 10px 0;
}

/* Welcome Message */
.welcome-msg-pro {
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    text-align: center;
    padding: 36px 20px;
    max-width: 760px;
    margin: 0 auto;
}

.welcome-icon {
    width: 80px;
    height: 80px;
    background: linear-gradient(135deg, var(--accent-primary), var(--accent-secondary));
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    margin-bottom: 24px;
    box-shadow: 0 8px 32px rgba(45, 178, 150, 0.3);
}

.welcome-icon i {
    font-size: 36px;
    color: white;
}

.welcome-msg-pro h2 {
    font-size: 28px;
    font-weight: 700;
    margin-bottom: 12px;
    color: var(--text-main);
}

.welcome-msg-pro p {
    color: var(--text-muted);
    font-size: 16px;
    margin-bottom: 32px;
}

.welcome-suggestions {
    display: flex;
    flex-wrap: wrap;
    gap: 12px;
    justify-content: center;
}

.suggestion-chip {
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    border-radius: 20px;
    padding: 10px 18px;
    color: var(--text-main);
    font-size: 14px;
    cursor: pointer;
    transition: var(--transition-fast);
}

.suggestion-chip:hover {
    background: var(--bg-card-hover);
    border-color: var(--accent-primary);
    color: var(--accent-primary);
}

/* Interview suggested answers: legacy classes (kept for backward compat) */
.interview-answer-options {
    display: flex;
    flex-direction: column;
    gap: 8px;
}

/* Chat Input */
.chat-input-pro {
    padding: 20px 24px 24px;
    background: linear-gradient(to top, var(--bg-main) 70%, transparent);
}

.chat-input-wrapper-pro {
    max-width: 100%;
    margin: 0 auto;
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    border-radius: 24px;
    padding: 8px 8px 8px 20px;
    display: flex;
    align-items: flex-end;
    gap: 12px;
    transition: var(--transition-fast);
}

.chat-input-wrapper-pro:focus-within {
    border-color: var(--accent-primary);
    box-shadow: 0 0 0 3px rgba(45, 178, 150, 0.1);
}

.chat-input-wrapper-pro textarea {
    flex: 1;
    background: none;
    border: none;
    color: var(--text-main);
    resize: none;
    outline: none;
    font-family: inherit;
    font-size: 16px;
    line-height: 1.5;
    max-height: 200px;
    padding: 8px 0;
}

.send-btn-pro {
    width: 40px;
    height: 40px;
    background: var(--accent-primary);
    border: none;
    border-radius: 50%;
    color: white;
    cursor: pointer;
    transition: var(--transition-fast);
    display: flex;
    align-items: center;
    justify-content: center;
    flex-shrink: 0;
}

.send-btn-pro:disabled {
    background: var(--border-color);
    cursor: not-allowed;
}

.send-btn-pro:not(:disabled):hover {
    background: var(--accent-secondary);
    transform: scale(1.05);
}

.chat-disclaimer {
    text-align: center;
    font-size: 12px;
    color: var(--text-muted);
    margin-top: 12px;
    max-width: 100%;
    margin-left: auto;
    margin-right: auto;
}

/* Chat Messages - Professional Style */
.chat-msg-pro {
    padding: 24px 0;
    animation: fadeIn 0.3s ease;
}

.chat-msg-pro.user-msg-pro {
    background: transparent;
}

.chat-msg-pro.bot-msg-pro {
    background: var(--bg-card);
    border-top: 1px solid var(--border-color);
    border-bottom: 1px solid var(--border-color);
}

.msg-inner {
    max-width: 100%;
    margin: 0 auto;
    padding: 0 24px;
    display: flex;
    gap: 20px;
    align-items: flex-start;
}

.bot-msg-pro .msg-inner {
    flex-direction: row;
}

.user-msg-pro .msg-inner {
    flex-direction: row-reverse;
}

[dir="ltr"] .bot-msg-pro .msg-inner {
    flex-direction: row;
}

[dir="ltr"] .user-msg-pro .msg-inner {
    flex-direction: row-reverse;
}

.msg-avatar-pro {
    width: 36px;
    height: 36px;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    font-weight: 600;
    font-size: 14px;
    flex-shrink: 0;
}

.user-msg-pro .msg-avatar-pro {
    background: linear-gradient(135deg, var(--accent-primary), var(--accent-secondary));
    color: white;
}

.bot-msg-pro .msg-avatar-pro {
    background: var(--bg-card-hover);
    color: var(--accent-primary);
    border: 1px solid var(--border-color);
}

.msg-body {
    flex: 1;
    min-width: 0;
}

.bot-msg-pro .msg-body {
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    border-radius: 12px;
    padding: 12px 14px;
}

body.dark-theme .bot-msg-pro .msg-body {
    background: rgba(255, 255, 255, 0.04);
    border-color: rgba(255, 255, 255, 0.12);
}

.msg-author {
    font-weight: 600;
    font-size: 14px;
    margin-bottom: 8px;
    color: var(--text-main);
}

.msg-text {
    font-size: 15px;
    line-height: 1.8;
    color: var(--text-main);
    word-wrap: break-word;
    overflow-wrap: break-word;
    display: flex;
    flex-direction: column;
    gap: 6px;
}

.msg-text[dir="rtl"] {
    text-align: right;
}

.msg-text[dir="ltr"] {
    text-align: left;
}

.user-msg-pro .msg-author {
    text-align: end;
}

.bot-msg-pro .msg-author {
    text-align: start;
}

.msg-text .answer-line {
    margin: 0;
    line-height: 1.8;
}

.msg-text .answer-heading-line {
    margin: 2px 0 4px;
    font-size: 15px;
    font-weight: 700;
    color: var(--accent-primary);
}

.msg-text .answer-question-line {
    margin: 2px 0;
    font-weight: 600;
    color: var(--text-main);
}

.msg-text .answer-note-line {
    margin: 2px 0;
    color: var(--text-muted);
    font-style: italic;
}

.msg-text .answer-list-plain,
.msg-text .answer-ordered-list-plain {
    margin: 2px 0 6px;
    padding-inline-start: 20px;
    display: flex;
    flex-direction: column;
    gap: 4px;
}

.msg-text .answer-list-plain li,
.msg-text .answer-ordered-list-plain li {
    line-height: 1.8;
}

/* Legacy styles kept intentionally omitted for cleaner ChatGPT-like output */

.msg-text strong {
    color: var(--accent-primary);
    font-weight: 700;
}

.msg-text code {
    background: rgba(45, 178, 150, 0.12);
    color: var(--accent-primary);
    border: 1px solid rgba(45, 178, 150, 0.24);
    border-radius: 6px;
    padding: 1px 6px;
    font-size: 12px;
}

.msg-sources-pro {
    margin-top: 20px;
    padding: 16px;
    background: var(--bg-main);
    border-radius: 12px;
    border: 1px solid var(--border-color);
}

.msg-sources-pro .sources-header {
    display: flex;
    align-items: center;
    gap: 8px;
    margin-bottom: 12px;
    color: var(--text-muted);
    font-size: 13px;
    font-weight: 600;
}

.msg-sources-pro .sources-header i {
    color: var(--accent-primary);
}

.msg-sources-pro ul {
    list-style: none;
    margin: 0;
    padding: 0;
    display: flex;
    flex-wrap: wrap;
    gap: 8px;
}

.msg-sources-pro li {
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    border-radius: 8px;
    padding: 8px 12px;
    font-size: 12px;
    color: var(--text-muted);
    display: flex;
    align-items: center;
    gap: 6px;
    transition: var(--transition-fast);
}

.msg-sources-pro li:hover {
    border-color: var(--accent-primary);
    color: var(--accent-primary);
}

.msg-sources-pro li i {
    color: var(--accent-primary);
    font-size: 10px;
}

.msg-sources-pro .source-score {
    background: rgba(45, 178, 150, 0.12);
    color: var(--accent-primary);
    padding: 2px 6px;
    border-radius: 4px;
    font-size: 11px;
    font-weight: 600;
}

/* Typing Indicator - Professional */
.typing-indicator-pro {
    display: flex;
    gap: 6px;
    padding: 8px 0;
}

.typing-indicator-pro span {
    width: 8px;
    height: 8px;
    background: var(--accent-primary);
    border-radius: 50%;
    animation: typingBounce 1.4s infinite ease-in-out both;
}

.typing-indicator-pro span:nth-child(1) {
    animation-delay: -0.32s;
}

.typing-indicator-pro span:nth-child(2) {
    animation-delay: -0.16s;
}

@keyframes typingBounce {
    0%, 80%, 100% {
        transform: scale(0.6);
        opacity: 0.5;
    }
    40% {
        transform: scale(1);
        opacity: 1;
    }
}

.suggested-answer-card {
    background: var(--bg-color);
    border: 1px solid var(--border-color);
    border-radius: 8px;
    padding: 16px;
    margin: 12px 0;
    box-shadow: 0 2px 4px rgba(0,0,0,0.05);
}

.suggested-answer-card .answer-list {
    margin: 0;
    padding-inline-start: 20px;
}

.suggested-answer-card .answer-list li {
    margin-bottom: 8px;
}

.suggested-answer-card .answer-list li:last-child {
    margin-bottom: 0;
}

/* Document Items */
.doc-item {
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    padding: 16px;
    border-radius: 16px;
    display: flex;
    align-items: center;
    justify-content: space-between;
    margin-bottom: 12px;
    transition: var(--transition-fast);
}

.doc-item:hover {
    border-color: var(--accent-primary);
    background: var(--bg-card-hover);
}

.doc-info {
    display: flex;
    align-items: center;
    gap: 16px;
}

.doc-info i {
    font-size: 24px;
    color: var(--error);
}

.doc-details {
    display: flex;
    flex-direction: column;
}

.doc-name {
    font-weight: 600;
    font-size: 14px;
}

.doc-size {
    font-size: 12px;
    color: var(--text-muted);
}

.doc-status {
    display: flex;
    align-items: center;
    gap: 8px;
    font-size: 12px;
    padding: 4px 12px;
    border-radius: 20px;
}

.status-done {
    background: rgba(16, 185, 129, 0.1);
    color: var(--success);
}

.status-processing {
    background: rgba(245, 158, 11, 0.1);
    color: var(--warning);
}

.status-error {
    background: rgba(239, 68, 68, 0.1);
    color: var(--error);
}

/* Project Detail Layout */
.project-content-grid {
    display: grid;
    grid-template-columns: 1fr 1.5fr;
    gap: 32px;
}

/* Interview CTA in project detail */
.interview-cta-section {
    grid-column: 1 / -1;
    margin-bottom: 8px;
    text-align: center;
}

.interview-cta-section .btn {
    font-size: 1.1em;
    padding: 14px 32px;
}

.interview-cta-hint {
    margin-top: 8px;
    color: var(--text-secondary);
    font-size: 0.9em;
}

.upload-optional-hint {
    color: var(--text-secondary);
    font-size: 0.85em;
    margin-bottom: 8px;
}

.upload-zone {
    border: 2px dashed var(--border-color);
    border-radius: 20px;
    padding: 40px;
    text-align: center;
    cursor: pointer;
    transition: var(--transition-fast);
}

.upload-zone:hover,
.upload-zone.dragover {
    border-color: var(--accent-primary);
    background: rgba(45, 178, 150, 0.06);
}

.upload-zone i {
    font-size: 48px;
    color: var(--accent-primary);
    margin-bottom: 16px;
}

/* Buttons */
.btn {
    padding: 10px 24px;
    border-radius: 12px;
    border: none;
    font-weight: 600;
    cursor: pointer;
    display: flex;
    align-items: center;
    gap: 10px;
    transition: var(--transition-fast);
}

.btn-primary {
    background: var(--accent-primary);
    color: white;
}

.btn-primary:hover {
    background: var(--accent-secondary);
    box-shadow: 0 4px 15px rgba(45, 178, 150, 0.4);
}

/* Modal */
.modal-overlay {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: rgba(0, 0, 0, 0.8);
    backdrop-filter: blur(5px);
    display: flex;
    align-items: center;
    justify-content: center;
    z-index: 1000;
    transition: var(--transition-fast);
}

.modal {
    background: var(--bg-sidebar);
    width: 500px;
    border-radius: 24px;
    padding: 32px;
    border: 1px solid var(--border-color);
}

.hidden {
    display: none;
    opacity: 0;
    pointer-events: none;
}

/* Loader */
.loader-container {
    display: flex;
    justify-content: center;
    align-items: center;
    height: 100%;
}

.loader {
    width: 48px;
    height: 48px;
    border: 5px solid var(--bg-card);
    border-bottom-color: var(--accent-primary);
    border-radius: 50%;
    animation: rotation 1s linear infinite;
}

@keyframes rotation {
    0% {
        transform: rotate(0deg);
    }

    100% {
        transform: rotate(360deg);
    }
}

/* Custom Scrollbar */
::-webkit-scrollbar {
    width: 6px;
}

::-webkit-scrollbar-track {
    background: transparent;
}

::-webkit-scrollbar-thumb {
    background: var(--border-color);
    border-radius: 10px;
}

::-webkit-scrollbar-thumb:hover {
    background: var(--text-muted);
}

/* Config Views */
.config-card {
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    border-radius: 20px;
    padding: 24px;
    max-width: 720px;
    animation: fadeIn 0.4s ease;
}

.config-section {
    display: flex;
    flex-direction: column;
    gap: 8px;
}

.config-label {
    font-size: 14px;
    font-weight: 600;
    color: var(--text-main);
}

.config-desc {
    color: var(--text-muted);
    margin-bottom: 16px;
}

/* Forms */
.form-control {
    width: 100%;
    background: var(--bg-main);
    border: 1px solid var(--border-color);
    padding: 12px 16px;
    border-radius: 10px;
    color: var(--text-main);
    outline: none;
}

.form-control:focus {
    border-color: var(--accent-primary);
}

/* â”€â”€ Icon Buttons â”€â”€ */
.icon-btn {
    width: 40px;
    height: 40px;
    border-radius: 12px;
    border: 1px solid var(--border-color);
    background: transparent;
    color: var(--text-muted);
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 15px;
    transition: var(--transition-fast);
}

.icon-btn:hover {
    background: var(--bg-card-hover);
    color: var(--text-main);
    border-color: var(--accent-primary);
}

.icon-btn .lang-code {
    font-size: 13px;
    font-weight: 700;
    letter-spacing: 0.5px;
}

/* â”€â”€ Form Groups â”€â”€ */
.form-group {
    display: flex;
    flex-direction: column;
    gap: 6px;
    margin-bottom: 16px;
}

.form-group label {
    font-size: 14px;
    font-weight: 600;
    color: var(--text-main);
}

.form-group textarea.form-control {
    min-height: 80px;
    resize: vertical;
}

/* â”€â”€ Toggle Switches (for checkboxes) â”€â”€ */
.toggle-wrap {
    display: flex;
    align-items: center;
    justify-content: space-between;
}

.toggle-switch {
    position: relative;
    width: 48px;
    height: 26px;
    flex-shrink: 0;
}

.toggle-switch input {
    opacity: 0;
    width: 0;
    height: 0;
}

.toggle-slider {
    position: absolute;
    inset: 0;
    background: var(--border-color);
    border-radius: 999px;
    cursor: pointer;
    transition: var(--transition-fast);
}

.toggle-slider::before {
    content: '';
    position: absolute;
    width: 20px;
    height: 20px;
    border-radius: 50%;
    background: var(--text-muted);
    top: 3px;
    inset-inline-start: 3px;
    transition: var(--transition-fast);
}

.toggle-switch input:checked + .toggle-slider {
    background: var(--accent-primary);
}

.toggle-switch input:checked + .toggle-slider::before {
    background: white;
    inset-inline-start: calc(100% - 23px);
}

/* â”€â”€ Delete Buttons â”€â”€ */
.delete-project-btn,
.delete-doc-btn {
    background: transparent;
    border: 1px solid transparent;
    border-radius: 8px;
    padding: 6px 10px;
    color: var(--text-muted);
    cursor: pointer;
    font-size: 14px;
    transition: var(--transition-fast);
}

.delete-project-btn:hover,
.delete-doc-btn:hover {
    background: rgba(239, 68, 68, 0.1);
    color: var(--error);
    border-color: var(--error);
}

/* â”€â”€ Modal Enhancements â”€â”€ */
.modal-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    margin-bottom: 24px;
    padding-bottom: 16px;
    border-bottom: 1px solid var(--border-color);
}

.modal-header h3 {
    font-size: 20px;
    font-weight: 700;
}

.close-modal {
    width: 36px;
    height: 36px;
    border-radius: 10px;
    border: none;
    background: transparent;
    color: var(--text-muted);
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 16px;
    transition: var(--transition-fast);
}

.close-modal:hover {
    background: rgba(239, 68, 68, 0.1);
    color: var(--error);
}

.modal-overlay:not(.hidden) {
    animation: fadeIn 0.2s ease;
}

.modal {
    animation: modalSlideIn 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

@keyframes modalSlideIn {
    from { opacity: 0; transform: scale(0.95) translateY(10px); }
    to   { opacity: 1; transform: scale(1) translateY(0); }
}

/* â”€â”€ Animations â”€â”€ */
@keyframes fadeIn {
    from {
        opacity: 0;
        transform: translateY(10px);
    }

    to {
        opacity: 1;
        transform: translateY(0);
    }
}

@keyframes countUp {
    from { opacity: 0; transform: translateY(8px); }
    to   { opacity: 1; transform: translateY(0); }
}

/* â”€â”€ Utilities â”€â”€ */
.w-100 {
    width: 100%;
}

.mt-4 {
    margin-top: 16px;
}

.empty-msg {
    text-align: center;
    color: var(--text-muted);
    padding: 40px;
}

/* Copy Button for Messages */
.msg-actions {
    display: flex;
    gap: 8px;
    margin-top: 12px;
    opacity: 0;
    transition: var(--transition-fast);
}

.chat-msg-pro:hover .msg-actions {
    opacity: 1;
}

.msg-action-btn {
    background: transparent;
    border: 1px solid var(--border-color);
    border-radius: 8px;
    padding: 6px 12px;
    color: var(--text-muted);
    font-size: 12px;
    cursor: pointer;
    display: flex;
    align-items: center;
    gap: 6px;
    transition: var(--transition-fast);
    font-family: inherit;
}

.msg-action-btn:hover {
    background: var(--bg-card-hover);
    color: var(--text-main);
    border-color: var(--accent-primary);
}

.msg-action-btn.copied {
    color: var(--success);
    border-color: var(--success);
}

/* Streaming cursor blink */
.msg-text.streaming::after {
    content: 'â–';
    display: inline;
    color: var(--accent-primary);
    animation: cursorBlink 0.8s step-end infinite;
    margin-inline-start: 2px;
}

@keyframes cursorBlink {
    0%, 100% { opacity: 1; }
    50%      { opacity: 0; }
}

.doc-progress {
    margin-top: 12px;
    width: 100%;
}

.doc-progress-header {
    display: flex;
    justify-content: space-between;
    font-size: 12px;
    color: var(--text-muted);
    margin-bottom: 6px;
    gap: 10px;
}

.doc-progress-track {
    width: 100%;
    height: 6px;
    border-radius: 999px;
    background: rgba(255, 255, 255, 0.08);
    overflow: hidden;
}

.doc-progress-bar {
    height: 100%;
    width: 0;
    background: linear-gradient(90deg, var(--accent-primary), var(--accent-secondary));
    border-radius: 999px;
    transition: width 0.2s ease;
}

/* â”€â”€ Mobile Hamburger â”€â”€ */
.mobile-hamburger {
    display: none;
    width: 44px;
    height: 44px;
    border: none;
    background: transparent;
    color: var(--text-main);
    font-size: 22px;
    cursor: pointer;
    border-radius: 12px;
    align-items: center;
    justify-content: center;
    transition: var(--transition-fast);
}

.mobile-hamburger:hover {
    background: var(--bg-card-hover);
}

.sidebar-overlay {
    display: none;
    position: fixed;
    inset: 0;
    background: rgba(0,0,0,0.5);
    z-index: 99;
    opacity: 0;
    transition: opacity 0.3s ease;
}

.sidebar-overlay.active {
    opacity: 1;
}

/* â”€â”€ Skeleton Loader â”€â”€ */
.skeleton {
    background: linear-gradient(90deg, var(--bg-card) 25%, var(--bg-card-hover) 50%, var(--bg-card) 75%);
    background-size: 200% 100%;
    animation: shimmer 1.5s ease-in-out infinite;
    border-radius: 12px;
}

.skeleton-card {
    height: 140px;
    border-radius: 20px;
}

.skeleton-stat {
    height: 90px;
    border-radius: 20px;
}

@keyframes shimmer {
    0%   { background-position: 200% 0; }
    100% { background-position: -200% 0; }
}

/* â”€â”€ Empty State â”€â”€ */
.empty-state {
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    padding: 60px 24px;
    text-align: center;
    animation: fadeIn 0.4s ease;
}

.empty-state-icon {
    width: 80px;
    height: 80px;
    border-radius: 50%;
    background: rgba(45, 178, 150, 0.1);
    display: flex;
    align-items: center;
    justify-content: center;
    margin-bottom: 20px;
    font-size: 32px;
    color: var(--accent-primary);
}

.empty-state h3 {
    font-size: 18px;
    font-weight: 600;
    margin-bottom: 8px;
    color: var(--text-main);
}

.empty-state p {
    color: var(--text-muted);
    font-size: 14px;
    max-width: 320px;
}

/* â”€â”€ Stat Value Animation â”€â”€ */
.stat-card .value {
    font-size: 28px;
    font-weight: 700;
    animation: countUp 0.5s ease;
}

/* â”€â”€ Enhanced Toast â”€â”€ */
.toast {
    position: fixed;
    bottom: 32px;
    left: 50%;
    transform: translateX(-50%) translateY(100px);
    padding: 14px 24px;
    border-radius: 14px;
    background: var(--bg-sidebar);
    border: 1px solid var(--border-color);
    color: var(--text-main);
    z-index: 2000;
    transition: var(--transition-slow);
    box-shadow: 0 10px 30px rgba(0, 0, 0, 0.5);
    display: flex;
    align-items: center;
    gap: 10px;
    font-size: 14px;
    max-width: 480px;
}

.toast.show {
    transform: translateX(-50%) translateY(0);
}

.toast-icon {
    font-size: 18px;
    flex-shrink: 0;
}

.toast-success { border-left: 4px solid var(--success); }
.toast-success .toast-icon { color: var(--success); }

.toast-error { border-left: 4px solid var(--error); }
.toast-error .toast-icon { color: var(--error); }

.toast-warning { border-left: 4px solid var(--warning); }
.toast-warning .toast-icon { color: var(--warning); }

.toast-info { border-left: 4px solid var(--accent-primary); }
.toast-info .toast-icon { color: var(--accent-primary); }

/* â”€â”€ Search Bar Focus â”€â”€ */
.search-bar:focus-within {
    border-color: var(--accent-primary);
    box-shadow: 0 0 0 3px rgba(45, 178, 150, 0.1);
}

.search-bar input::placeholder {
    color: var(--text-muted);
}

/* â”€â”€ Section Header Link â”€â”€ */
.section-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    margin-bottom: 24px;
}

.section-header .link {
    color: var(--accent-primary);
    text-decoration: none;
    font-size: 14px;
    font-weight: 600;
    transition: var(--transition-fast);
    padding: 6px 12px;
    border-radius: 8px;
}

.section-header .link:hover {
    background: rgba(45, 178, 150, 0.12);
}

/* â”€â”€ Back Button â”€â”€ */
.back-btn {
    width: 40px;
    height: 40px;
    border-radius: 12px;
    border: 1px solid var(--border-color);
    background: transparent;
    color: var(--text-main);
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 16px;
    transition: var(--transition-fast);
    margin-inline-end: 16px;
}

.back-btn:hover {
    background: var(--bg-card-hover);
    border-color: var(--accent-primary);
}

.project-header {
    display: flex;
    align-items: center;
    margin-bottom: 32px;
}

/* â”€â”€ Project Card Doc Count Badge â”€â”€ */
.project-card-footer {
    display: flex;
    align-items: center;
    justify-content: space-between;
    margin-top: 16px;
    padding-top: 16px;
    border-top: 1px solid var(--border-color);
}

.project-card-date {
    display: flex;
    align-items: center;
    gap: 6px;
    font-size: 12px;
    color: var(--text-muted);
}

.project-card-actions {
    display: flex;
    align-items: center;
    gap: 8px;
}

.doc-count-badge {
    display: inline-flex;
    align-items: center;
    gap: 4px;
    font-size: 12px;
    color: var(--accent-primary);
    background: rgba(45, 178, 150, 0.12);
    padding: 4px 10px;
    border-radius: 20px;
    font-weight: 600;
}

/* â”€â”€ AI Config Grouped Sections â”€â”€ */
.config-group {
    margin-bottom: 24px;
    padding: 20px;
    background: var(--bg-main);
    border-radius: 16px;
    border: 1px solid var(--border-color);
}

.config-group-title {
    font-size: 15px;
    font-weight: 700;
    color: var(--accent-primary);
    margin-bottom: 16px;
    display: flex;
    align-items: center;
    gap: 8px;
}

.config-group-title i {
    font-size: 14px;
}

@media (max-width: 1100px) {
    .srs-grid {
        grid-template-columns: 1fr;
    }
}

/* â”€â”€ Responsive â”€â”€ */
@media (max-width: 768px) {
    .sidebar-toggle-btn {
        display: none;
    }

    .mobile-hamburger {
        display: flex;
    }

    .sidebar {
        position: fixed;
        inset-block: 0;
        inset-inline-start: 0;
        transform: translateX(-100%);
        transition: transform 0.3s cubic-bezier(0.4, 0, 0.2, 1);
        z-index: 100;
    }

    [dir="rtl"] .sidebar {
        transform: translateX(100%);
    }

    .sidebar.open {
        transform: translateX(0) !important;
    }

    .sidebar-overlay.active {
        display: block;
    }

    .top-bar {
        padding: 0 16px;
        height: 64px;
    }

    .search-bar {
        width: auto;
        flex: 1;
        margin: 0 12px;
    }

    .view-container {
        padding: 20px 16px;
    }

    .view-container.chat-mode {
        padding: 0;
    }

    .view-title {
        font-size: 24px;
    }

    .project-content-grid {
        grid-template-columns: 1fr;
    }

    .chat-header-content {
        flex-direction: row;
        flex-wrap: wrap;
        gap: 8px;
    }

    .chat-controls {
        width: 100%;
    }

    .chat-select {
        flex: 1;
        min-width: auto;
    }

    .msg-inner {
        padding: 0 16px;
    }

    .welcome-msg-pro {
        padding: 40px 16px;
    }

    .welcome-suggestions {
        flex-direction: column;
    }

    .suggestion-chip {
        width: 100%;
    }

    .config-card {
        max-width: 100%;
    }
}

/* --- SRS Review --- */
.view-subtitle {
    margin-top: -18px;
    color: var(--text-muted);
    max-width: 640px;
}

.srs-view {
    display: flex;
    flex-direction: column;
    gap: 24px;
    animation: viewFade 0.5s ease;
}

.srs-header {
    display: flex;
    align-items: flex-start;
    justify-content: space-between;
    gap: 24px;
    flex-wrap: wrap;
}

.srs-actions {
    display: flex;
    align-items: center;
    gap: 12px;
    flex-wrap: wrap;
}
.srs-export-format {
    min-width: 150px;
}

.srs-select {
    min-width: 220px;
    padding: 10px 14px;
    border-radius: 14px;
    border: 1px solid var(--border-color);
    background: var(--bg-card);
    color: var(--text-main);
}

.btn.btn-secondary {
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    color: var(--text-main);
}

.btn.btn-ghost {
    background: transparent;
    border: 1px dashed var(--border-color);
    color: var(--text-main);
}

.srs-grid {
    display: grid;
    grid-template-columns: minmax(0, 2.1fr) minmax(260px, 1fr);
    gap: 24px;
}

.srs-draft {
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    border-radius: 24px;
    padding: 28px;
    box-shadow: var(--shadow-soft);
}

.srs-paper-canvas {
    background: linear-gradient(180deg, rgba(255, 255, 255, 0.95), rgba(255, 255, 255, 0.85));
    border: 1px solid var(--glass-border);
    border-radius: 14px;
    padding: 22px;
    box-shadow: 0 10px 24px rgba(22, 50, 45, 0.08);
}

body.dark-theme .srs-paper-canvas {
    background: rgba(255, 255, 255, 0.04);
    box-shadow: 0 10px 24px rgba(0, 0, 0, 0.35);
}

.srs-draft-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    margin-bottom: 18px;
    gap: 16px;
    flex-wrap: wrap;
}

.status-pill {
    background: rgba(45, 178, 150, 0.15);
    color: var(--accent-primary);
    padding: 6px 14px;
    border-radius: 999px;
    font-weight: 600;
}

.srs-meta {
    color: var(--text-muted);
    font-size: 13px;
}

.srs-sections {
    display: grid;
    gap: 16px;
}

.srs-section {
    border: 1px solid var(--border-color);
    border-radius: 18px;
    padding: 18px 20px;
    background: linear-gradient(135deg, rgba(255, 255, 255, 0.7), rgba(255, 255, 255, 0.3));
}

.srs-section-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    margin-bottom: 12px;
}

.srs-section-header h3 {
    font-size: 18px;
    font-weight: 600;
}

.srs-section-actions {
    display: flex;
    align-items: center;
    gap: 8px;
}

.srs-edit-btn {
    width: 30px;
    height: 30px;
    border: 1px solid var(--border-color);
    background: transparent;
    color: var(--text-muted);
    border-radius: 8px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 12px;
    transition: var(--transition-fast);
    opacity: 0;
}

.srs-section:hover .srs-edit-btn {
    opacity: 1;
}

.srs-edit-btn:hover {
    border-color: var(--accent-primary);
    color: var(--accent-primary);
    background: rgba(45, 178, 150, 0.08);
}

.srs-edit-btn.saving {
    opacity: 1;
    border-color: var(--success);
    color: var(--success);
    background: rgba(22, 163, 74, 0.08);
}

.srs-section ul li.editable {
    border: 1px solid var(--accent-primary);
    border-radius: 8px;
    padding: 8px 12px;
    margin-bottom: 4px;
    outline: none;
    background: rgba(45, 178, 150, 0.04);
    cursor: text;
    transition: var(--transition-fast);
}

.srs-section ul li.editable:focus {
    border-color: var(--accent-secondary);
    box-shadow: 0 0 0 2px rgba(31, 157, 143, 0.15);
}

.confidence-badge {
    padding: 4px 12px;
    border-radius: 999px;
    background: rgba(31, 157, 143, 0.15);
    color: var(--accent-secondary);
    font-size: 12px;
    font-weight: 600;
}

.srs-section ul {
    padding-inline-start: 18px;
    color: var(--text-muted);
}

.srs-activity-diagram {
    border-style: dashed;
}

.activity-diagram-list {
    list-style: none;
    padding: 0;
    margin: 0;
    display: grid;
    gap: 8px;
}

.activity-diagram-list li {
    padding: 10px 12px;
    border: 1px solid var(--border-color);
    border-radius: 10px;
    background: var(--bg-card);
    color: var(--text-main);
    font-size: 13px;
}

.srs-mermaid-surface {
    border: 1px solid var(--border-color);
    border-radius: 12px;
    padding: 12px;
    background: var(--bg-card);
    overflow-x: auto;
}

.srs-mermaid-surface svg {
    max-width: 100%;
    height: auto;
}

.srs-mermaid-fallback {
    margin-top: 10px;
}

.srs-mermaid-fallback summary {
    cursor: pointer;
    color: var(--text-muted);
    font-size: 12px;
}

.srs-mermaid-error {
    color: var(--error);
    font-size: 13px;
}

.srs-summary {
    display: grid;
    gap: 18px;
}

.summary-card {
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    border-radius: 20px;
    padding: 20px;
    box-shadow: var(--shadow-soft);
}

.summary-card.highlight {
    border-color: rgba(45, 178, 150, 0.4);
    background: linear-gradient(140deg, rgba(45, 178, 150, 0.12), rgba(255, 255, 255, 0.2));
}

.summary-metrics {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    margin-top: 16px;
}

.metric-chip {
    background: var(--bg-card-hover);
    padding: 8px 12px;
    border-radius: 12px;
    display: flex;
    flex-direction: column;
    gap: 2px;
    min-width: 90px;
}

.metric-label {
    font-size: 11px;
    color: var(--text-muted);
}

.metric-value {
    font-size: 14px;
    font-weight: 600;
}

.summary-list {
    margin-top: 12px;
    padding-inline-start: 18px;
    color: var(--text-muted);
    display: grid;
    gap: 6px;
}

@keyframes viewFade {
    from {
        opacity: 0;
        transform: translateY(8px);
    }
    to {
        opacity: 1;
        transform: translateY(0);
    }
}

/* â”€â”€ Microphone Button â”€â”€ */
.mic-btn-pro {
    width: 40px;
    height: 40px;
    background: transparent;
    border: 1px solid var(--border-color);
    border-radius: 50%;
    color: var(--text-muted);
    cursor: pointer;
    transition: var(--transition-fast);
    display: flex;
    align-items: center;
    justify-content: center;
    flex-shrink: 0;
}

.mic-btn-pro:hover {
    border-color: var(--accent-primary);
    color: var(--accent-primary);
    background: rgba(45, 178, 150, 0.08);
}

.mic-btn-pro.recording {
    background: var(--error);
    border-color: var(--error);
    color: white;
    animation: micPulse 1.5s ease-in-out infinite;
}

.mic-btn-pro:disabled {
    opacity: 0.5;
    cursor: not-allowed;
}

@keyframes micPulse {
    0%, 100% { box-shadow: 0 0 0 0 rgba(220, 38, 38, 0.4); }
    50% { box-shadow: 0 0 0 10px rgba(220, 38, 38, 0); }
}

/* â”€â”€ Interview Progress Bar â”€â”€ */
.interview-progress {
    display: flex;
    align-items: center;
    gap: 8px;
    padding: 0;
    border: 0;
    background: transparent;
    position: static;
    min-width: 0;
}

/* â”€â”€ Split-Screen Chat Body â”€â”€ */
.chat-body-wrapper {
    flex: 1;
    display: flex;
    overflow: hidden;
    min-height: 0;
}

.chat-body-wrapper .chat-messages-pro {
    flex: 1;
    min-width: 0;
}

/* â”€â”€ Live Document Panel (Co-pilot) â”€â”€ */
.live-doc-panel {
    width: 340px;
    flex-shrink: 0;
    display: flex;
    flex-direction: column;
    background: var(--bg-card);
    border-inline-start: 1px solid var(--border-color);
    animation: liveDocSlideIn 0.3s ease;
    overflow: hidden;
}

.chat-header-actions #summary-toggle-btn i {
    color: var(--accent-primary);
}

.live-doc-panel.is-collapsed {
    display: none !important;
}

@keyframes liveDocSlideIn {
    from {
        opacity: 0;
        transform: translateX(20px);
    }
    to {
        opacity: 1;
        transform: translateX(0);
    }
}

.live-doc-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    padding: 16px 20px;
    border-bottom: 1px solid var(--border-color);
    background: var(--bg-sidebar);
}

.live-doc-header h3 {
    font-size: 14px;
    font-weight: 600;
    color: var(--accent-primary);
    display: flex;
    align-items: center;
    gap: 8px;
}

.live-doc-close {
    width: 28px;
    height: 28px;
    border: none;
    background: transparent;
    color: var(--text-muted);
    cursor: pointer;
    border-radius: 6px;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: var(--transition-fast);
}

.live-doc-close:hover {
    background: rgba(239, 68, 68, 0.1);
    color: var(--error);
}

.live-doc-content {
    flex: 1;
    overflow-y: auto;
    padding: 20px;
}

.live-doc-empty {
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    text-align: center;
    padding: 40px 16px;
    color: var(--text-muted);
    gap: 12px;
}

.live-doc-empty i {
    font-size: 32px;
    color: var(--border-color);
}

.live-doc-empty p {
    font-size: 13px;
}

.live-doc-section {
    animation: fadeIn 0.3s ease;
}

.live-doc-stage-badge {
    display: inline-flex;
    align-items: center;
    gap: 6px;
    background: rgba(45, 178, 150, 0.12);
    color: var(--accent-primary);
    padding: 4px 12px;
    border-radius: 20px;
    font-size: 12px;
    font-weight: 600;
    margin-bottom: 16px;
}

.live-doc-text {
    font-size: 14px;
    line-height: 1.8;
    color: var(--text-main);
}

.msg-text,
.live-doc-items li,
.srs-section li,
#srs-summary-text {
    unicode-bidi: plaintext;
}

.live-doc-text .answer-paragraph {
    margin-bottom: 10px;
}

.live-doc-text .answer-list {
    margin: 8px 0;
}

.interview-telemetry-card {
    margin-top: 10px;
    border: 1px solid var(--border-color);
    border-radius: 10px;
    background: var(--bg-card);
    padding: 8px 10px;
}

.interview-telemetry-title {
    font-size: 12px;
    font-weight: 700;
    color: var(--text-main);
    margin-bottom: 8px;
}

.interview-telemetry-grid {
    display: grid;
    grid-template-columns: repeat(2, minmax(0, 1fr));
    gap: 6px;
}

.interview-telemetry-item {
    display: flex;
    align-items: center;
    justify-content: space-between;
    gap: 8px;
    border: 1px solid var(--border-color);
    border-radius: 8px;
    padding: 6px 8px;
    font-size: 11px;
    color: var(--text-muted);
}

.interview-telemetry-item strong {
    color: var(--text-main);
    font-size: 12px;
}

@media (max-width: 768px) {
    .live-doc-panel {
        position: fixed;
        inset: 0;
        width: 100%;
        z-index: 200;
        border: none;
    }
}

/* â”€â”€ Booking Form â”€â”€ */
.booking-form .form-row {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 16px;
}

/* â”€â”€ Confirmed Button â”€â”€ */
.btn-confirmed {
    background: var(--success) !important;
    color: white !important;
    cursor: default !important;
    opacity: 0.85;
}

/* â”€â”€ Auth Screen â”€â”€ */
.auth-screen {
    display: flex;
    align-items: center;
    justify-content: center;
    position: fixed;
    inset: 0;
    background: var(--bg-main);
    z-index: 5000;
}

.auth-card {
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    border-radius: 24px;
    padding: 40px;
    width: 420px;
    max-width: 90vw;
    box-shadow: var(--shadow-soft);
    animation: modalSlideIn 0.4s ease;
}

.auth-logo {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 12px;
    font-size: 28px;
    font-weight: 700;
    color: var(--accent-primary);
    margin-bottom: 8px;
}

.auth-logo i {
    font-size: 32px;
}

.auth-subtitle {
    text-align: center;
    color: var(--text-muted);
    font-size: 14px;
    margin-bottom: 28px;
}

.auth-tabs {
    display: flex;
    gap: 4px;
    background: var(--bg-main);
    border-radius: 12px;
    padding: 4px;
    margin-bottom: 24px;
}

.auth-tab {
    flex: 1;
    padding: 10px;
    border: none;
    background: transparent;
    border-radius: 10px;
    cursor: pointer;
    font-size: 14px;
    font-weight: 600;
    color: var(--text-muted);
    transition: var(--transition-fast);
    font-family: inherit;
}

.auth-tab.active {
    background: var(--accent-primary);
    color: white;
    box-shadow: 0 2px 8px rgba(45, 178, 150, 0.3);
}

.auth-tab:not(.active):hover {
    color: var(--text-main);
}

.auth-form .form-group {
    margin-bottom: 16px;
}

.auth-form .btn {
    margin-top: 8px;
    justify-content: center;
    font-size: 15px;
    padding: 12px;
}

.auth-error {
    text-align: center;
    color: var(--error);
    font-size: 13px;
    margin-top: 16px;
    padding: 10px;
    background: rgba(220, 38, 38, 0.08);
    border-radius: 10px;
}

@media (max-width: 1100px) {
    .srs-grid {
        grid-template-columns: 1fr;
    }
}

/* â”€â”€ Coverage Progress Indicators â”€â”€ */
.interview-coverage-grid {
    margin: 0;
    display: flex;
    align-items: center;
    gap: 8px;
    flex-wrap: nowrap;
}

.coverage-item {
    display: flex;
    align-items: center;
    justify-content: center;
    padding: 0;
    border-radius: 999px;
    background: transparent;
    border: 0;
    transition: var(--transition-fast);
}

.coverage-dot {
    width: 24px;
    height: 24px;
    border-radius: 50%;
    border: 1px solid var(--border-color);
    background: var(--bg-card);
    color: var(--text-muted);
    font-size: 11px;
    font-weight: 700;
    display: inline-flex;
    align-items: center;
    justify-content: center;
    transition: var(--transition-fast);
}

.coverage-label {
    display: none;
}

.coverage-kpi {
    display: none;
}

.coverage-pcts {
    display: none;
}

/* Active area - glow */
.coverage-item.active-area {
    background: transparent;
}

.coverage-item.active-area .coverage-dot {
    border-color: var(--accent-primary);
    color: var(--accent-primary);
    box-shadow: 0 0 0 2px rgba(45, 178, 150, 0.16);
}

/* Complete area - green */
.coverage-item.complete-area {
    background: transparent;
}

.coverage-item.complete-area .coverage-dot {
    border-color: var(--success);
    background: rgba(22, 163, 74, 0.12);
    color: var(--success);
}

/* â”€â”€ Upload Ref Button (Chat Header) â”€â”€ */
.chat-upload-ref-btn {
    display: inline-flex;
    align-items: center;
    gap: 8px;
    padding: 8px 14px;
    border-radius: 999px;
    border: 1px solid var(--border-color);
    background: var(--bg-card);
    color: var(--text-muted);
    font-size: 13px;
    cursor: pointer;
    transition: var(--transition-fast);
    font-family: inherit;
}

.chat-upload-ref-btn:hover {
    border-color: var(--accent-primary);
    color: var(--accent-primary);
    background: rgba(45, 178, 150, 0.06);
}

.chat-upload-ref-btn i {
    font-size: 14px;
}

/* â”€â”€ Structured Live Doc Sections â”€â”€ */
.live-doc-srs-section {
    margin-bottom: 20px;
    animation: fadeIn 0.3s ease;
}

.live-doc-alerts {
    display: flex;
    flex-direction: column;
    gap: 8px;
    margin-bottom: 14px;
}

.live-doc-alert {
    border: 1px solid var(--border-color);
    border-radius: 10px;
    padding: 8px 10px;
    background: var(--bg-card);
}

.live-doc-alert-title {
    font-size: 12px;
    font-weight: 700;
    margin-bottom: 4px;
}

.live-doc-alert-text {
    font-size: 12px;
    color: var(--text-muted);
    line-height: 1.5;
}

.live-doc-alert-high {
    border-color: rgba(220, 38, 38, 0.45);
    background: rgba(220, 38, 38, 0.08);
}

.live-doc-alert-high .live-doc-alert-title {
    color: var(--error);
}

.live-doc-alert-medium {
    border-color: rgba(245, 158, 11, 0.45);
    background: rgba(245, 158, 11, 0.08);
}

.live-doc-alert-medium .live-doc-alert-title {
    color: var(--warning);
}

.live-doc-plan {
    margin-bottom: 14px;
    border: 1px dashed var(--border-color);
    border-radius: 10px;
    padding: 8px 10px;
    background: rgba(31, 157, 143, 0.06);
}

.live-doc-plan-row {
    font-size: 12px;
    line-height: 1.55;
    color: var(--text-main);
}

.live-doc-trace {
    margin-bottom: 14px;
    border: 1px solid var(--border-color);
    border-radius: 10px;
    padding: 8px 10px;
    background: rgba(45, 178, 150, 0.05);
}

.live-doc-trace-head {
    display: flex;
    flex-wrap: wrap;
    gap: 8px;
    font-size: 11px;
    color: var(--text-muted);
    margin-bottom: 8px;
}

.live-doc-trace-steps {
    display: flex;
    flex-direction: column;
    gap: 6px;
}

.live-doc-trace-step {
    border: 1px dashed var(--border-color);
    border-radius: 8px;
    padding: 6px 8px;
    background: var(--bg-card);
}

.live-doc-trace-step-name {
    font-size: 11px;
    font-weight: 700;
    color: var(--accent-primary);
    margin-bottom: 2px;
    text-transform: capitalize;
}

.live-doc-trace-step-text {
    font-size: 12px;
    color: var(--text-main);
    line-height: 1.5;
}

.live-doc-focus-area {
    border: 1px solid rgba(45, 178, 150, 0.35);
    border-radius: 12px;
    padding: 10px;
    background: rgba(45, 178, 150, 0.06);
}

.live-doc-section-header {
    display: flex;
    align-items: center;
    gap: 8px;
    margin-bottom: 10px;
    color: var(--accent-primary);
    font-size: 13px;
    font-weight: 700;
}

.live-doc-section-header i {
    font-size: 12px;
}

.live-doc-count-badge {
    background: rgba(45, 178, 150, 0.12);
    color: var(--accent-primary);
    padding: 2px 8px;
    border-radius: 999px;
    font-size: 11px;
    font-weight: 600;
    margin-inline-start: auto;
}

.live-doc-delta-badge {
    background: rgba(22, 163, 74, 0.14);
    color: var(--success);
    padding: 2px 8px;
    border-radius: 999px;
    font-size: 11px;
    font-weight: 700;
}

.live-doc-items {
    list-style: none;
    padding: 0;
    margin: 0;
    display: flex;
    flex-direction: column;
    gap: 6px;
}

.live-doc-items li {
    position: relative;
    padding: 6px 12px 6px 24px;
    font-size: 13px;
    line-height: 1.6;
    color: var(--text-main);
    border-radius: 8px;
    transition: background 0.3s ease;
}

[dir="rtl"] .live-doc-items li {
    padding: 6px 24px 6px 12px;
}

.live-doc-items li::before {
    content: '';
    position: absolute;
    inset-inline-start: 8px;
    top: 13px;
    width: 5px;
    height: 5px;
    background: var(--text-muted);
    border-radius: 50%;
}

/* New item highlight + pulse animation */
.live-doc-new-item {
    background: rgba(45, 178, 150, 0.12);
    animation: newItemPulse 2s ease forwards;
}

.live-doc-new-item::before {
    background: var(--accent-primary) !important;
}

.live-doc-updated-item {
    background: rgba(245, 158, 11, 0.14);
}

.live-doc-updated-item::before {
    background: var(--warning) !important;
}

.live-doc-removed-item {
    color: var(--error);
    font-size: 12px;
}

.coverage-kpi {
    display: block;
    margin-top: 6px;
    font-size: 10px;
    color: var(--text-muted);
    font-weight: 600;
    letter-spacing: 0.2px;
}

@keyframes newItemPulse {
    0% {
        background: rgba(45, 178, 150, 0.2);
    }
    100% {
        background: transparent;
    }
}

/* â”€â”€ Phase 2: Project Card Quick Actions â”€â”€ */
.project-card-quick-actions {

    .template-card {
        cursor: default;
    }

    .template-card .template-use-btn {
        margin-top: 8px;
        width: 100%;
    }
    display: flex;
    gap: 8px;
    margin-top: 12px;
    padding-top: 12px;
    border-top: 1px solid rgba(255, 255, 255, 0.06);
}

.project-card-quick-actions .btn {
    flex: 1;
    font-size: 13px;
    padding: 7px 12px;
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 6px;
}

.btn-outline {
    background: transparent;
    border: 1px solid var(--accent-primary);
    color: var(--accent-primary);
    border-radius: 8px;
    cursor: pointer;
    transition: background 0.2s, color 0.2s;
}

.btn-outline:hover {
    background: var(--accent-primary);
    color: var(--bg-primary);
}

/* â”€â”€ Phase 2: Project Files Drawer â”€â”€ */
.files-drawer {
    background: var(--bg-secondary);
    border-bottom: 1px solid rgba(255, 255, 255, 0.07);
    max-height: 360px;
    overflow-y: auto;
    animation: slideDown 0.2s ease;
}

@keyframes slideDown {
    from { max-height: 0; opacity: 0; }
    to { max-height: 360px; opacity: 1; }
}

.files-drawer-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    padding: 10px 16px;
    border-bottom: 1px solid rgba(255, 255, 255, 0.06);
}

.files-drawer-header h4 {
    margin: 0;
    font-size: 14px;
    font-weight: 600;
    color: var(--text-main);
    display: flex;
    align-items: center;
    gap: 8px;
}

.files-drawer-body {
    padding: 12px 16px;
    display: flex;
    flex-direction: column;
    gap: 12px;
}

.files-drawer .upload-zone {
    padding: 14px;
    min-height: 80px;
}

/* â”€â”€ Phase 2: Settings Page Tabs â”€â”€ */
.settings-view {
    max-width: 680px;
    margin: 0 auto;
    padding: 32px 16px;
}

.settings-tabs {
    display: flex;
    gap: 4px;
    margin-bottom: 24px;
    background: var(--bg-secondary);
    padding: 4px;
    border-radius: 10px;
    width: fit-content;
}

.settings-tab-btn {
    background: none;
    border: none;
    padding: 8px 20px;
    border-radius: 7px;
    font-size: 14px;
    font-weight: 500;
    color: var(--text-secondary);
    cursor: pointer;
    transition: background 0.2s, color 0.2s;
}

.settings-tab-btn.active {
    background: var(--accent-primary);
    color: var(--bg-primary);
}

.settings-tab-panel {
    display: none;
}

.settings-tab-panel.active {
    display: block;
}

@media (max-width: 768px) {
    .interview-coverage-grid {
        gap: 6px;
    }

    .coverage-item {
        padding: 0;
    }

    .coverage-dot {
        width: 22px;
        height: 22px;
        font-size: 10px;
    }

    .chat-upload-ref-label {
        display: none;
    }

    .chat-upload-ref-btn {
        padding: 8px 10px;
    }
}

/* â”€â”€ Phase 3: SRS Progress Stepper (3.1) â”€â”€ */
.srs-progress-stepper {
    background: var(--bg-secondary);
    border-radius: 12px;
    padding: 24px 28px;
    margin: 8px 0;
}

.srs-progress-title {
    font-size: 15px;
    font-weight: 600;
    color: var(--text-main);
    margin-bottom: 20px;
}

.srs-steps {
    display: flex;
    flex-direction: column;
    gap: 12px;
}

.srs-step {
    display: flex;
    align-items: center;
    gap: 10px;
    font-size: 14px;
    padding: 8px 0;
    color: var(--text-secondary);
}

.srs-step.done {
    color: var(--accent-primary);
}

.srs-step.active {
    color: var(--text-main);
    font-weight: 600;
}

.srs-step.pending {
    opacity: 0.4;
}

/* â”€â”€ Phase 3: SRS Guidance Banner (3.3) â”€â”€ */
.srs-guidance-banner {
    display: flex;
    gap: 16px;
    align-items: flex-start;
    background: rgba(255, 185, 30, 0.1);
    border: 1px solid rgba(255, 185, 30, 0.3);
    border-radius: 12px;
    padding: 20px;
    margin: 8px 0;
}

.srs-guidance-icon {
    font-size: 22px;
    color: #F5A623;
    flex-shrink: 0;
    margin-top: 2px;
}

.srs-guidance-body {
    flex: 1;
}

.srs-guidance-body strong {
    display: block;
    font-size: 15px;
    color: var(--text-main);
    margin-bottom: 6px;
}

.srs-guidance-body p {
    font-size: 13px;
    color: var(--text-secondary);
    margin: 0;
}

.srs-guidance-actions {
    display: flex;
    gap: 8px;
    margin-top: 14px;
    flex-wrap: wrap;
}

.srs-guidance-dismiss {
    background: none;
    border: 1px solid var(--border-color);
    color: var(--text-secondary);
    border-radius: 8px;
    cursor: pointer;
    padding: 6px 14px;
    font-size: 13px;
}

/* â”€â”€ Phase 3: Interview Thinking Stage Label (3.2) â”€â”€ */
.thinking-stage-label {
    margin-top: 8px;
    font-size: 12px;
    color: var(--text-tertiary, var(--text-secondary));
    opacity: 0.8;
    display: flex;
    align-items: center;
    gap: 6px;
}

.thinking-stage-label strong {
    color: var(--accent-primary);
    font-weight: 500;
}

/* â”€â”€ Phase 3: Suggestion Card Chips (3.6) â”€â”€ */
.interview-answer-select-wrap {
    margin-top: 14px;
    padding-top: 10px;
    border-top: 1px solid var(--border-color);
}

.suggestion-cards-list {
    display: flex;
    flex-direction: column;
    gap: 10px;
    margin: 4px 0 10px;
}

.suggestion-card {
    display: flex;
    align-items: flex-start;
    gap: 12px;
    padding: 14px 16px;
    border: 1.5px solid var(--border-color);
    border-radius: 14px;
    background: var(--bg-card);
    cursor: pointer;
    box-shadow: 0 1px 4px rgba(0, 0, 0, 0.06);
    transition: border-color 0.18s, background 0.18s, transform 0.15s, box-shadow 0.18s;
}

.suggestion-card:hover:not(.disabled) {
    border-color: var(--accent-primary);
    background: rgba(45, 178, 150, 0.07);
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(45, 178, 150, 0.12);
}

.suggestion-card.selected {
    border-color: var(--accent-primary);
    background: rgba(45, 178, 150, 0.1);
    box-shadow: 0 2px 8px rgba(45, 178, 150, 0.15);
}

.suggestion-card.disabled {
    opacity: 0.45;
    cursor: not-allowed;
    pointer-events: none;
    box-shadow: none;
}

.suggestion-card-radio {
    width: 18px;
    height: 18px;
    border-radius: 50%;
    border: 2px solid var(--border-color);
    flex-shrink: 0;
    margin-top: 1px;
    transition: border-color 0.15s, background 0.15s, box-shadow 0.15s;
}

.suggestion-card.selected .suggestion-card-radio {
    border-color: var(--accent-primary);
    background: var(--accent-primary);
    box-shadow: 0 0 0 3px rgba(45, 178, 150, 0.2);
}

.suggestion-card-text {
    flex: 1;
    font-size: 13.5px;
    line-height: 1.55;
    color: var(--text-main);
}

.interview-mini-btn {
    margin-top: 8px;
    padding: 8px 20px;
    font-size: 13px;
    border-radius: 10px;
    font-weight: 500;
}

.interview-mini-btn:disabled {
    opacity: 0.4;
    cursor: not-allowed;
}

/* â”€â”€ Phase 3: Offline Banner CSS (3.5) â”€â”€ */
.offline-banner {
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    z-index: 9999;
    background: rgba(245, 166, 35, 0.95);
    color: #1a1a1a;
    padding: 10px 20px;
    font-size: 14px;
    font-weight: 500;
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 8px;
    transform: translateY(-100%);
    transition: transform 0.25s ease;
}

.offline-banner.visible {
    transform: translateY(0);
}



################################################################################
# FILE: telegram_bot\__init__.py
################################################################################

"""Telegram bot package initialization."""
__version__ = "1.0.0"

################################################################################
# FILE: telegram_bot\bot.py
################################################################################

import asyncio
import logging
import tempfile
import atexit
from pathlib import Path

import telebot

from telegram_bot.config import bot_settings
import telegram_bot.handlers as handlers

try:
    import msvcrt  # Windows
except Exception:  # noqa: BLE001
    msvcrt = None

try:
    import fcntl  # Unix
except Exception:  # noqa: BLE001
    fcntl = None

# Configure logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

_instance_lock_file = None


def _release_instance_lock() -> None:
    global _instance_lock_file
    if _instance_lock_file is None:
        return
    try:
        _instance_lock_file.seek(0)
        if msvcrt is not None:
            msvcrt.locking(_instance_lock_file.fileno(), msvcrt.LK_UNLCK, 1)
        elif fcntl is not None:
            fcntl.flock(_instance_lock_file.fileno(), fcntl.LOCK_UN)
    except Exception:
        pass
    try:
        _instance_lock_file.close()
    except Exception:
        pass
    _instance_lock_file = None


def _acquire_instance_lock() -> bool:
    global _instance_lock_file
    lock_path = Path(tempfile.gettempdir()) / "tawasul_telegram_bot.lock"
    lock_path.parent.mkdir(parents=True, exist_ok=True)

    fp = open(lock_path, "a+", encoding="utf-8")
    fp.seek(0)

    try:
        if msvcrt is not None:
            msvcrt.locking(fp.fileno(), msvcrt.LK_NBLCK, 1)
        elif fcntl is not None:
            fcntl.flock(fp.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
        else:
            fp.close()
            return True
    except Exception:
        fp.close()
        return False

    fp.seek(0)
    fp.truncate(0)
    fp.write("tawasul telegram bot lock\n")
    fp.flush()
    _instance_lock_file = fp
    atexit.register(_release_instance_lock)
    return True

# Initialize bot
bot = telebot.TeleBot(bot_settings.telegram_bot_token)

def print_bot_link():
    """Print bot link to console."""
    try:
        bot_info = bot.get_me()
        bot_link = f"https://t.me/{bot_info.username}"
        logger.info(f"Bot is running! Share this link: {bot_link}")
    except Exception as e:
        logger.error(f"Error getting bot info: {str(e)}")

def setup_handlers():
    """Register all handlers."""
    # Pass bot instance to handlers
    handlers.set_bot(bot)
    
    # Command handlers
    bot.message_handler(commands=['start'])(handlers.start_command)
    bot.message_handler(commands=['help'])(handlers.help_command)
    bot.message_handler(commands=['projects'])(handlers.projects_command)
    bot.message_handler(commands=['newproject'])(handlers.newproject_command)
    bot.message_handler(commands=['srsnow'])(handlers.srsnow_command)

    # Callback handlers
    bot.callback_query_handler(func=lambda call: str(call.data or '').startswith('project:'))(handlers.handle_project_selection)
    bot.callback_query_handler(func=lambda call: str(call.data or '').startswith('srs:'))(handlers.handle_srs_action)
    bot.callback_query_handler(func=lambda call: str(call.data or '').startswith('session:'))(handlers.handle_session_action)
    
    # Voice/audio handlers
    bot.message_handler(content_types=['voice'])(handlers.handle_voice)
    bot.message_handler(content_types=['audio'])(handlers.handle_voice)

    # Text message handler (for queries)
    bot.message_handler(func=lambda message: True)(handlers.handle_message)

def main():
    """Start the bot."""
    if not _acquire_instance_lock():
        logger.error("Another local Telegram bot instance is already running. Stop it before starting a new one.")
        return

    setup_handlers()
    print_bot_link()
    logger.info("Starting Tawasul Telegram Bot (infinity_polling)...")
    try:
        bot.infinity_polling()
    finally:
        _release_instance_lock()

if __name__ == "__main__":
    main()

################################################################################
# FILE: telegram_bot\config.py
################################################################################

"""
Telegram Bot Configuration.
Loads bot settings from environment.
"""
from pydantic_settings import BaseSettings, SettingsConfigDict
from pydantic import Field
from pathlib import Path

# Get project root directory
ROOT_DIR = Path(__file__).resolve().parent.parent
ENV_FILE = ROOT_DIR / ".env"

class BotSettings(BaseSettings):
    """Bot configuration settings."""
    
    telegram_bot_token: str = Field(default="", alias="TELEGRAM_BOT_TOKEN")
    telegram_admin_id: str = Field(default="", alias="TELEGRAM_ADMIN_ID")
    api_base_url: str = Field(default="http://localhost:8500", alias="API_BASE_URL")
    bot_api_email: str = Field(default="admin@tawasul.com", alias="BOT_API_EMAIL")
    bot_api_password: str = Field(default="admin123", alias="BOT_API_PASSWORD")
    
    model_config = SettingsConfigDict(
        env_file=str(ENV_FILE),
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore"
    )


# Global settings instance
try:
    bot_settings = BotSettings()
except Exception:
    # Fallback to defaults if .env fails
    bot_settings = BotSettings(_env_file=None)

################################################################################
# FILE: telegram_bot\handlers.py
################################################################################

"""
Telegram Bot Handlers.
Command and message handlers for the bot using pyTelegramBotAPI.
"""
import asyncio
import json
from io import BytesIO
import httpx
from typing import Any
from telebot import types
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from backend.config import settings
from backend.database import get_db
from backend.database.models import TelegramSession
from telegram_bot.config import bot_settings
import logging

logger = logging.getLogger(__name__)

NO_PROJECT_EN = "No project is linked to this chat yet. Use /newproject first."
NO_PROJECT_AR = "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø´Ø±ÙˆØ¹ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨Ø¹Ø¯. Ø§Ø³ØªØ®Ø¯Ù… /newproject Ø£ÙˆÙ„Ø§Ù‹."

MENU_NEW_PROJECT_AR = "âž• Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯"
MENU_PROJECTS_AR = "ðŸ“ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹"
MENU_SRS_NOW_AR = "ðŸ“„ ØªÙˆÙ„ÙŠØ¯ SRS"
MENU_HELP_AR = "â“ Ù…Ø³Ø§Ø¹Ø¯Ø©"

MENU_NEW_PROJECT_EN = "âž• New Project"
MENU_PROJECTS_EN = "ðŸ“ Projects"
MENU_SRS_NOW_EN = "ðŸ“„ Generate SRS"
MENU_HELP_EN = "â“ Help"

# Bot instance (will be set from bot.py)
bot = None
_session_fallback: dict[int, int] = {}
_auth_token: str | None = None

def set_bot(bot_instance):
    global bot
    bot = bot_instance


from backend.database.connection import async_session_maker

def set_chat_project(chat_id: int, project_id: int) -> None:
    async def _do():
        try:
            async with async_session_maker() as db:
                stmt = select(TelegramSession).where(TelegramSession.chat_id == chat_id)
                existing = await db.scalar(stmt)
                if existing:
                    existing.project_id = project_id
                else:
                    new_session = TelegramSession(chat_id=chat_id, project_id=project_id)
                    db.add(new_session)
                await db.commit()
        except Exception as exc:  # noqa: BLE001
            logger.warning("Failed to set telegram session in DB: %s", exc)
            _session_fallback[int(chat_id)] = int(project_id)
    asyncio.run(_do())


def get_chat_project(chat_id: int) -> int | None:
    async def _do():
        try:
            async with async_session_maker() as db:
                stmt = select(TelegramSession).where(TelegramSession.chat_id == chat_id)
                session = await db.scalar(stmt)
                if session:
                    return session.project_id
        except Exception as exc:  # noqa: BLE001
            logger.warning("Failed to read telegram session from DB: %s", exc)
        return _session_fallback.get(int(chat_id))
    return asyncio.run(_do())


def _detect_language(text: str) -> str:
    value = str(text or "")
    ar = sum(1 for ch in value if "\u0600" <= ch <= "\u06FF")
    en = sum(1 for ch in value if ("a" <= ch.lower() <= "z"))
    return "ar" if ar >= en else "en"


def _login_headers(force_refresh: bool = False) -> dict:
    global _auth_token
    if _auth_token and not force_refresh:
        return {"Authorization": f"Bearer {_auth_token}"}

    with httpx.Client(timeout=30.0) as client:
        resp = client.post(
            f"{bot_settings.api_base_url}/auth/login",
            json={
                "email": bot_settings.bot_api_email,
                "password": bot_settings.bot_api_password,
            },
        )
        resp.raise_for_status()
        payload = resp.json() if resp.content else {}
        _auth_token = str(payload.get("token") or "").strip() or None

    if not _auth_token:
        raise RuntimeError("Telegram bot login failed: empty token")
    return {"Authorization": f"Bearer {_auth_token}"}


def _api_post(path: str, payload: dict) -> dict:
    with httpx.Client(timeout=60.0) as client:
        headers = _login_headers()
        response = client.post(
            f"{bot_settings.api_base_url}{path}",
            headers=headers,
            json=payload,
        )
        if response.status_code == 401:
            headers = _login_headers(force_refresh=True)
            response = client.post(
                f"{bot_settings.api_base_url}{path}",
                headers=headers,
                json=payload,
            )
        response.raise_for_status()
        return response.json()


def _api_post_file(path: str, files: dict, data: dict = None) -> dict:
    with httpx.Client(timeout=120.0) as client:
        headers = _login_headers()
        response = client.post(
            f"{bot_settings.api_base_url}{path}",
            headers=headers,
            files=files,
            data=data,
        )
        if response.status_code == 401:
            headers = _login_headers(force_refresh=True)
            response = client.post(
                f"{bot_settings.api_base_url}{path}",
                headers=headers,
                files=files,
                data=data,
            )
        response.raise_for_status()
        return response.json()


def _api_get_json(path: str) -> dict:
    with httpx.Client(timeout=60.0) as client:
        headers = _login_headers()
        response = client.get(f"{bot_settings.api_base_url}{path}", headers=headers)
        if response.status_code == 401:
            headers = _login_headers(force_refresh=True)
            response = client.get(f"{bot_settings.api_base_url}{path}", headers=headers)
        response.raise_for_status()
        data = response.json() if response.content else {}
        return data if isinstance(data, dict) else {}


def _api_get_raw(path: str) -> tuple[bytes, str | None]:
    with httpx.Client(timeout=120.0) as client:
        headers = _login_headers()
        response = client.get(f"{bot_settings.api_base_url}{path}", headers=headers)
        if response.status_code == 401:
            headers = _login_headers(force_refresh=True)
            response = client.get(f"{bot_settings.api_base_url}{path}", headers=headers)
        response.raise_for_status()
        return response.content, response.headers.get("content-disposition")


def create_new_project(name: str, description: str = "Created from Telegram bot") -> dict:
    payload = {
        "name": str(name).strip(),
        "description": str(description or "").strip(),
    }
    return _api_post("/projects/", payload)


def _build_create_project_markup() -> types.InlineKeyboardMarkup:
    markup = types.InlineKeyboardMarkup(row_width=1)
    markup.add(types.InlineKeyboardButton(text="âž• Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯", callback_data="project:new"))
    return markup


def _build_persistent_menu_markup(language: str = "ar") -> types.ReplyKeyboardMarkup:
    is_en = language == "en"
    markup = types.ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=False, row_width=2)
    markup.row(MENU_NEW_PROJECT_EN if is_en else MENU_NEW_PROJECT_AR, MENU_PROJECTS_EN if is_en else MENU_PROJECTS_AR)
    markup.row(MENU_SRS_NOW_EN if is_en else MENU_SRS_NOW_AR, MENU_HELP_EN if is_en else MENU_HELP_AR)
    return markup


def _resolve_menu_action(text: str) -> str | None:
    value = str(text or "").strip()
    if value in {MENU_NEW_PROJECT_AR, MENU_NEW_PROJECT_EN}:
        return "newproject"
    if value in {MENU_PROJECTS_AR, MENU_PROJECTS_EN}:
        return "projects"
    if value in {MENU_SRS_NOW_AR, MENU_SRS_NOW_EN}:
        return "srsnow"
    if value in {MENU_HELP_AR, MENU_HELP_EN}:
        return "help"
    return None


def _build_srs_actions_markup(language: str) -> types.InlineKeyboardMarkup:
    markup = types.InlineKeyboardMarkup(row_width=1)
    label = "ðŸ“„ Download SRS PDF" if language == "en" else "ðŸ“„ ØªÙ†Ø²ÙŠÙ„ Ù…Ù„Ù SRS PDF"
    markup.add(types.InlineKeyboardButton(text=label, callback_data="srs:now"))
    return markup


def _build_session_actions_markup(language: str) -> types.InlineKeyboardMarkup:
    """Build inline keyboard with options to add requirements or end session."""
    markup = types.InlineKeyboardMarkup(row_width=1)
    if language == "en":
        markup.add(
            types.InlineKeyboardButton(text="âž• Add More Requirements", callback_data="session:add_more"),
            types.InlineKeyboardButton(text="ðŸ“„ Generate SRS PDF", callback_data="srs:now"),
            types.InlineKeyboardButton(text="âœ… End Session & Book Appointment", callback_data="session:end"),
        )
    else:
        markup.add(
            types.InlineKeyboardButton(text="âž• Ø¥Ø¶Ø§ÙØ© Ù…ØªØ·Ù„Ø¨Ø§Øª Ø£Ø®Ø±Ù‰", callback_data="session:add_more"),
            types.InlineKeyboardButton(text="ðŸ“„ ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ù SRS PDF", callback_data="srs:now"),
            types.InlineKeyboardButton(text="âœ… Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø© ÙˆØ­Ø¬Ø² Ù…ÙˆØ¹Ø¯", callback_data="session:end"),
        )
    return markup


def _resolve_user_language(text_hint: str, telegram_language_code: str | None) -> str:
    hinted = _detect_language(text_hint)
    if hinted in {"ar", "en"}:
        return hinted

    code = str(telegram_language_code or "").strip().lower()
    if code.startswith("ar"):
        return "ar"
    return "en"


def _filename_from_content_disposition(header_value: str | None, fallback: str) -> str:
    raw = str(header_value or "")
    marker = "filename="
    if marker not in raw:
        return fallback
    candidate = raw.split(marker, 1)[1].strip().strip('"').strip("'")
    if not candidate:
        return fallback
    return candidate


def _extract_draft_state(draft_payload: dict | None) -> tuple[dict, dict]:
    if not isinstance(draft_payload, dict):
        return {}, {}
    draft = draft_payload.get("draft") if isinstance(draft_payload.get("draft"), dict) else {}
    summary = draft.get("summary") if isinstance(draft.get("summary"), dict) else {}
    coverage = draft.get("coverage") if isinstance(draft.get("coverage"), dict) else {}
    return summary, coverage


def _no_project_text(language: str) -> str:
    return NO_PROJECT_EN if language == "en" else NO_PROJECT_AR


def _reply_no_project(message, language: str, welcome_prefix: bool = False) -> None:
    body = _no_project_text(language)
    if welcome_prefix and language != "en":
        body = "Ù…Ø±Ø­Ø¨Ø§Ù‹! " + body
    bot.reply_to(message, body, reply_markup=_build_persistent_menu_markup(language))


def _assistant_fallback_text(language: str) -> str:
    return (
        "I need a bit more context to continue."
        if language == "en"
        else "Ù…Ø­ØªØ§Ø¬ Ø´ÙˆÙŠØ© ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ© Ø¹Ù„Ø´Ø§Ù† Ø£ÙƒÙ…Ù„ Ù…Ø¹Ø§Ùƒ Ø¨Ø´ÙƒÙ„ Ø£Ø¯Ù‚."
    )


def _srs_refresh_language(language: str) -> str:
    return "en"  # Always generate SRS in English


def _srs_need_more_details_text(language: str, detail: str) -> str:
    if language == "en":
        base = (
            "âš ï¸ I need more details before generating SRS. "
            "Please continue the conversation with business requirements, workflows, users, constraints, and goals."
        )
        return f"{base}\n\nDetails: {detail}" if detail else base

    base = (
        "âš ï¸ Ù…Ø­ØªØ§Ø¬ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ± Ù‚Ø¨Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù€SRS. "
        "ÙƒÙ…Ù‘Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¨ÙŠØ²Ù†Ø³ØŒ Ø³ÙŠØ± Ø§Ù„Ø¹Ù…Ù„ØŒ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†ØŒ Ø§Ù„Ù‚ÙŠÙˆØ¯ØŒ ÙˆØ§Ù„Ø£Ù‡Ø¯Ø§Ù."
    )
    return f"{base}\n\nØ§Ù„ØªÙØ§ØµÙŠÙ„: {detail}" if detail else base


def _srs_parse_error_text(language: str, detail: str) -> str:
    if language == "en":
        base = (
            "âš ï¸ SRS generation failed due to invalid AI output format (JSON parsing error). "
            "Please retry now; if it repeats, switch the AI provider/model from settings."
        )
        return f"{base}\n\nDetails: {detail}" if detail else base

    base = (
        "âš ï¸ ØªØ¹Ø°Ù‘Ø± ØªÙˆÙ„ÙŠØ¯ SRS Ø¨Ø³Ø¨Ø¨ Ù…Ø®Ø±Ø¬Ø§Øª AI ØºÙŠØ± ØµØ§Ù„Ø­Ø© Ø¨ØµÙŠØºØ© JSON (JSON parsing error). "
        "Ø¬Ø±Ù‘Ø¨ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø§Ù„Ø¢Ù†ØŒ ÙˆØ¥Ø°Ø§ ØªÙƒØ±Ø± Ø§Ù„Ø®Ø·Ø£ ØºÙŠÙ‘Ø± Ù…Ø²ÙˆÙ‘Ø¯/Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª."
    )
    return f"{base}\n\nØ§Ù„ØªÙØ§ØµÙŠÙ„: {detail}" if detail else base


def _is_srs_parse_error(detail: str) -> bool:
    value = str(detail or "").strip().lower()
    if not value:
        return False
    markers = [
        "failed to parse srs json",
        "json parsing",
        "json decode",
        "invalid json",
    ]
    return any(marker in value for marker in markers)


def _safe_http_error_detail(response) -> str:
    if response is None:
        return ""
    try:
        payload = response.json() if response.content else {}
        return str(payload.get("detail") or "").strip()
    except Exception:  # noqa: BLE001
        return ""


def _refresh_srs_or_explain(project_id: int, user_lang: str) -> str | None:
    try:
        _api_post(
            f"/projects/{project_id}/srs/refresh",
            {"language": _srs_refresh_language(user_lang)},
        )
        return None
    except httpx.HTTPStatusError as refresh_exc:
        response = refresh_exc.response
        if response is not None and response.status_code == 400:
            detail = _safe_http_error_detail(response)
            if _is_srs_parse_error(detail):
                return _srs_parse_error_text(user_lang, detail)
            return _srs_need_more_details_text(user_lang, detail)
        raise


def _save_interview_draft(project_id: int, result: dict, language: str) -> None:
    if not isinstance(result, dict):
        return

    payload = {
        "summary": result.get("summary") if isinstance(result.get("summary"), dict) else {},
        "coverage": result.get("coverage") if isinstance(result.get("coverage"), dict) else {},
        "signals": result.get("signals") if isinstance(result.get("signals"), dict) else {},
        "livePatch": result.get("live_patch") if isinstance(result.get("live_patch"), dict) else {},
        "cycleTrace": result.get("cycle_trace") if isinstance(result.get("cycle_trace"), dict) else {},
        "topicNavigation": result.get("topic_navigation") if isinstance(result.get("topic_navigation"), dict) else {},
        "stage": str(result.get("stage") or "discovery"),
        "mode": False,
        "lastAssistantQuestion": str(result.get("question") or ""),
        "lang": language if language in {"ar", "en"} else "ar",
    }
    _api_post(f"/projects/{project_id}/interview/draft", payload)


def _run_interview_turn(project_id: int, query: str, language: str, source: str) -> str:
    msg_metadata = {"source": source, "language": language}
    if source == "telegram_voice":
        msg_metadata["transcript_confirmed"] = True
    _api_post(
        f"/projects/{project_id}/messages",
        {
            "messages": [
                {
                    "role": "user",
                    "content": query,
                    "metadata": msg_metadata,
                }
            ]
        },
    )

    draft_payload = _api_get_json(f"/projects/{project_id}/interview/draft")
    last_summary, last_coverage = _extract_draft_state(draft_payload)
    result = _api_post(
        f"/projects/{project_id}/interview/next",
        {
            "language": language,
            "last_summary": last_summary,
            "last_coverage": last_coverage,
        },
    )

    _save_interview_draft(project_id=project_id, result=result, language=language)

    assistant_text = str(result.get("question") or "").strip() or _assistant_fallback_text(language)

    _api_post(
        f"/projects/{project_id}/messages",
        {
            "messages": [
                {
                    "role": "assistant",
                    "content": assistant_text,
                    "metadata": {"source": "telegram", "language": language},
                }
            ]
        },
    )
    return assistant_text


def _transcribe_message_media(message) -> str:
    voice = getattr(message, "voice", None)
    audio = getattr(message, "audio", None)
    media = voice or audio
    if media is None:
        return ""

    file_info = bot.get_file(media.file_id)
    downloaded = bot.download_file(file_info.file_path)

    filename = "voice.ogg" if voice is not None else (getattr(audio, "file_name", None) or "audio.mp3")
    mime = "audio/ogg" if voice is not None else "audio/mpeg"

    stt_payload = _api_post_file(
        "/stt/transcribe",
        files={"file": (filename, downloaded, mime)},
        data={"language": "auto"},
    )
    return str(stt_payload.get("text") or "").strip()


def _welcome_after_project_created(project_name: str, project_id: int, language: str) -> str:
    safe_name = str(project_name or "Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯").strip() or "Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯"
    if language == "en":
        return (
            "ðŸŽ‰ Welcome to Tawasul AI!\n\n"
            f"âœ… Project ready: {safe_name}\n"
            f"ðŸ†” Project ID: {int(project_id)}\n\n"
            "Iâ€™m Tawasul Chat. I turn your words into clear business requirements.\n"
            "I can help you define scope, priorities, and key user needs step by step.\n\n"
            "ðŸ’¡ Whatâ€™s your idea?"
        )

    return (
        "ðŸŽ‰ Ø£Ù‡Ù„Ø§Ù‹ Ø¨ÙŠÙƒ ÙÙŠ Ø´Ø§Øª ØªÙˆØ§ØµÙ„!\n\n"
        f"âœ… Ù…Ø´Ø±ÙˆØ¹Ùƒ Ø¬Ø§Ù‡Ø²: {safe_name}\n"
        f"ðŸ†” Ø±Ù‚Ù… Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {int(project_id)}\n\n"
        "Ø£Ù†Ø§ Ø´Ø§Øª ØªÙˆØ§ØµÙ„ØŒ Ø¨Ø­ÙˆÙ‘Ù„ ÙƒÙ„Ø§Ù…Ùƒ Ø¥Ù„Ù‰ Business Requirements ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ù†Ø¸Ù…Ø©.\n"
        "Ù‡Ù†Ø´ØªØºÙ„ Ù…Ø¹Ù‹Ø§ Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ØŒ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§ØªØŒ ÙˆØªØ¬Ù…ÙŠØ¹ Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø´ÙƒÙ„ Ø¹Ù…Ù„ÙŠ.\n\n"
        "ðŸ’¡ Ø¥ÙŠÙ‡ Ù‡ÙŠ ÙÙƒØ±ØªÙƒØŸ"
    )


def projects_command(message):
    """Handle /projects command as a create-project CTA (no project listing)."""
    bot.reply_to(
        message,
        "Ø§Ø®ØªØ± Ø£Ù…Ø±Ù‹Ø§ Ù…Ù† Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø¨Ø§Ù„Ø£Ø³ÙÙ„. ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯ ÙÙˆØ±Ù‹Ø§.",
        reply_markup=_build_persistent_menu_markup("ar"),
    )


def handle_project_selection(call):
    """Handle inline keyboard callbacks: project:<id>."""
    data = str(call.data or "")
    if data == "project:new":
        try:
            project_name = "Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯ Ù…Ù† ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…"
            created = create_new_project(project_name)
            created_name = str(created.get("name") or project_name)
            project_id = int(created.get("id"))
            user_lang = _resolve_user_language(
                text_hint=created_name,
                telegram_language_code=getattr(call.from_user, "language_code", None),
            )
            set_chat_project(int(call.message.chat.id), project_id)
            bot.answer_callback_query(call.id, "Done âœ…" if user_lang == "en" else "ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙˆØ±Ø¨Ø·Ù‡ âœ…")
            bot.edit_message_text(
                _welcome_after_project_created(created_name, project_id, user_lang),
                chat_id=call.message.chat.id,
                message_id=call.message.message_id,
            )
        except Exception as exc:  # noqa: BLE001
            logger.error("Failed to create telegram project from callback: %s", exc)
            bot.answer_callback_query(call.id, "ØªØ¹Ø°Ù‘Ø± Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯")
        return

    if not data.startswith("project:"):
        return

    try:
        project_id = int(data.split(":", 1)[1])
        set_chat_project(int(call.message.chat.id), project_id)
        bot.answer_callback_query(call.id, "ØªÙ… Ø±Ø¨Ø· Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© âœ…")
        bot.edit_message_text(
            f"âœ… ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ù†Ø¬Ø§Ø­ (ID: {project_id}).\nØ£Ø±Ø³Ù„ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ø¢Ù†.",
            chat_id=call.message.chat.id,
            message_id=call.message.message_id,
        )
    except Exception as exc:  # noqa: BLE001
        logger.error("Failed to bind telegram chat to project: %s", exc)
        bot.answer_callback_query(call.id, "ØªØ¹Ø°Ù‘Ø± Ø±Ø¨Ø· Ø§Ù„Ù…Ø´Ø±ÙˆØ¹")

def start_command(message):
    """Handle /start command."""
    bot.reply_to(
        message,
        "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Tawasul Bot! ðŸ¤–\n\n"
        "Ù„Ø¨Ø¯Ø§ÙŠØ© Ø³Ø±ÙŠØ¹Ø©ØŒ Ø£Ù†Ø´Ø¦ Ù…Ø´Ø±ÙˆØ¹Ù‹Ø§ Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ø¹Ø¨Ø± /newproject\n"
        "Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ø«Ø§Ø¨ØªØ© Ø£Ø³ÙÙ„ Ø§Ù„Ø´Ø§Øª.",
        reply_markup=_build_persistent_menu_markup("ar"),
    )

def help_command(message):
    """Handle /help command."""
    bot.reply_to(
        message,
        "ðŸ“š Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…\n\n"
        "1) Ø§Ø³ØªØ®Ø¯Ù… /newproject Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯ Ø³Ø±ÙŠØ¹Ø§Ù‹.\n"
        "2) Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… /projects Ù„Ø¥Ø¸Ù‡Ø§Ø± Ø²Ø± Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯.\n"
        "3) Ø§Ø³ØªØ®Ø¯Ù… /srsnow Ù„ØªÙˆÙ„ÙŠØ¯ SRS Ø§Ù„Ø¢Ù† ÙˆØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù PDF ÙÙˆØ±Ù‹Ø§.\n"
        "4) Ø£Ø±Ø³Ù„ Ø³Ø¤Ø§Ù„Ùƒ ÙƒØªØ§Ø¨Ø©Ù‹ ÙˆØ³ÙŠØªÙ… ØªÙˆØ¬ÙŠÙ‡Ù‡ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ù…Ø®ØªØ§Ø±.",
        reply_markup=_build_persistent_menu_markup("ar"),
    )


def newproject_command(message):
    """Create a new project and bind it to current chat.

    Usage:
      /newproject
      /newproject Project Name
    """
    raw = str(getattr(message, "text", "") or "").strip()
    parts = raw.split(maxsplit=1)
    project_name = parts[1].strip() if len(parts) > 1 and parts[1].strip() else "Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯ Ù…Ù† ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…"
    user_lang = _resolve_user_language(
        text_hint=project_name,
        telegram_language_code=getattr(message.from_user, "language_code", None),
    )

    try:
        created = create_new_project(project_name)
        created_name = str(created.get("name") or project_name)
        project_id = int(created.get("id"))
        set_chat_project(int(message.chat.id), project_id)
        bot.reply_to(
            message,
            _welcome_after_project_created(created_name, project_id, user_lang),
            reply_markup=_build_persistent_menu_markup(user_lang),
        )
    except Exception as exc:  # noqa: BLE001
        logger.error("Failed to create project from /newproject: %s", exc)
        try:
            bot.reply_to(
                message,
                "âŒ ØªØ¹Ø°Ù‘Ø± Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯ Ø­Ø§Ù„ÙŠØ§Ù‹. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
                reply_markup=_build_persistent_menu_markup(user_lang),
            )
        except Exception:
            logger.error("Could not send error reply to Telegram")


def srsnow_command(message):
    """Generate latest SRS now and send PDF to user for current chat-bound project."""
    chat_id = int(message.chat.id)
    project_id = get_chat_project(chat_id)
    user_lang = _resolve_user_language(
        text_hint=str(getattr(message, "text", "") or ""),
        telegram_language_code=getattr(message.from_user, "language_code", None),
    )

    if not project_id:
        bot.reply_to(message, _no_project_text(user_lang), reply_markup=_build_persistent_menu_markup(user_lang))
        return

    progress_text = (
        "â³ Generating SRS now and preparing PDF..."
        if user_lang == "en"
        else "â³ Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ SRS Ø§Ù„Ø¢Ù† ÙˆØªØ¬Ù‡ÙŠØ² Ù…Ù„Ù PDF..."
    )
    thinking_msg = bot.reply_to(message, progress_text)

    _generate_and_send_srs_pdf(
        chat_id=message.chat.id,
        project_id=project_id,
        user_lang=user_lang,
        progress_message_id=thinking_msg.message_id,
    )


def _generate_and_send_srs_pdf(*, chat_id: int, project_id: int, user_lang: str, progress_message_id: int) -> None:
    try:
        explain = _refresh_srs_or_explain(project_id, user_lang)
        if explain:
            bot.edit_message_text(explain, chat_id=chat_id, message_id=progress_message_id)
            return

        pdf_bytes, content_disposition = _api_get_raw(f"/projects/{project_id}/srs/export")
        filename = _filename_from_content_disposition(
            content_disposition,
            fallback=f"srs_project_{int(project_id)}.pdf",
        )

        bot.send_document(
            chat_id=chat_id,
            document=BytesIO(pdf_bytes),
            visible_file_name=filename,
            caption=(
                "âœ… SRS generated and PDF is ready."
                if user_lang == "en"
                else "âœ… ØªÙ… ØªÙˆÙ„ÙŠØ¯ SRS Ø¨Ù†Ø¬Ø§Ø­ØŒ ÙˆØ¯Ù‡ Ù…Ù„Ù Ø§Ù„Ù€ PDF."
            ),
        )

        done_text = (
            "Done. You can run /srsnow any time during the conversation."
            if user_lang == "en"
            else "ØªÙ… Ø¨Ù†Ø¬Ø§Ø­. ØªÙ‚Ø¯Ø± ØªØ³ØªØ®Ø¯Ù… /srsnow ÙÙŠ Ø£ÙŠ ÙˆÙ‚Øª Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©."
        )
        bot.edit_message_text(done_text, chat_id=chat_id, message_id=progress_message_id)
    except Exception as exc:  # noqa: BLE001
        logger.error("Failed to generate/export SRS from Telegram command: %s", exc)
        error_text = (
            "âŒ Could not generate SRS/PDF right now. Please try again in a moment."
            if user_lang == "en"
            else "âŒ ØªØ¹Ø°Ù‘Ø± ØªÙˆÙ„ÙŠØ¯ SRS Ø£Ùˆ Ù…Ù„Ù PDF Ø­Ø§Ù„ÙŠØ§Ù‹. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø¹Ø¯ Ù‚Ù„ÙŠÙ„."
        )
        try:
            bot.edit_message_text(error_text, chat_id=chat_id, message_id=progress_message_id)
        except Exception:
            logger.error("Could not send SRS error reply to Telegram")


def handle_srs_action(call):
    """Handle SRS callbacks (currently: srs:now)."""
    data = str(call.data or "")
    if data != "srs:now":
        return

    chat_id = int(call.message.chat.id)
    project_id = get_chat_project(chat_id)
    user_lang = _resolve_user_language(
        text_hint=str(getattr(call.message, "text", "") or ""),
        telegram_language_code=getattr(call.from_user, "language_code", None),
    )

    if not project_id:
        bot.answer_callback_query(call.id, "No project" if user_lang == "en" else "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø´Ø±ÙˆØ¹")
        bot.send_message(chat_id, _no_project_text(user_lang), reply_markup=_build_persistent_menu_markup(user_lang))
        return

    bot.answer_callback_query(call.id, "Generating SRS..." if user_lang == "en" else "Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ SRS...")
    progress = bot.send_message(
        chat_id,
        "â³ Generating SRS now and preparing PDF..."
        if user_lang == "en"
        else "â³ Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ SRS Ø§Ù„Ø¢Ù† ÙˆØªØ¬Ù‡ÙŠØ² Ù…Ù„Ù PDF...",
    )
    _generate_and_send_srs_pdf(
        chat_id=chat_id,
        project_id=project_id,
        user_lang=user_lang,
        progress_message_id=progress.message_id,
    )

def handle_message(message):
    """Handle text messages through interview flow (like web chat)."""
    menu_action = _resolve_menu_action(str(getattr(message, "text", "") or ""))
    if menu_action == "newproject":
        newproject_command(message)
        return
    if menu_action == "projects":
        projects_command(message)
        return
    if menu_action == "srsnow":
        srsnow_command(message)
        return
    if menu_action == "help":
        help_command(message)
        return

    # Ignore commands
    raw_text = str(getattr(message, "text", "") or "")
    if raw_text.startswith('/'):
        return

    chat_id = int(message.chat.id)
    project_id = get_chat_project(chat_id)

    if not project_id:
        _reply_no_project(message, "ar", welcome_prefix=True)
        return

    query = str(message.text or "").strip()
    if not query:
        return

    language = _resolve_user_language(
        text_hint=query,
        telegram_language_code=getattr(message.from_user, "language_code", None),
    )
    
    # Send thinking message
    thinking_msg = bot.reply_to(
        message,
        "ðŸ¤” Thinking..." if language == "en" else "ðŸ¤” Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø±Ø¯Ùƒ ÙˆØ¨Ù†Ø§Ø¡ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ...",
    )
    
    try:
        assistant_text = _run_interview_turn(project_id, query, language, source="telegram")

        answer = f"ðŸ’¬ {assistant_text}"
        
        bot.edit_message_text(
            answer,
            chat_id=message.chat.id,
            message_id=thinking_msg.message_id,
            reply_markup=_build_session_actions_markup(language),
        )
        
    except Exception as e:
        logger.error("Error in telegram interview flow: %s", e)
        try:
            bot.edit_message_text(
                "âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø¹Ø¯ Ù‚Ù„ÙŠÙ„.",
                chat_id=message.chat.id,
                message_id=thinking_msg.message_id
            )
        except Exception:
            logger.error("Could not send interview error reply to Telegram")


def handle_voice(message):
    """Handle Telegram voice/audio using backend STT endpoint, then continue interview flow."""
    chat_id = int(message.chat.id)
    project_id = get_chat_project(chat_id)
    language = _resolve_user_language(
        text_hint="",
        telegram_language_code=getattr(message.from_user, "language_code", None),
    )

    if not project_id:
        _reply_no_project(message, language)
        return

    if getattr(message, "voice", None) is None and getattr(message, "audio", None) is None:
        bot.reply_to(
            message,
            "No voice/audio file found in this message."
            if language == "en"
            else "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù ØµÙˆØª ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø±Ø³Ø§Ù„Ø©.",
        )
        return

    thinking_msg = bot.reply_to(
        message,
        "ðŸŽ¤ Transcribing your voice..."
        if language == "en"
        else "ðŸŽ¤ Ø¬Ø§Ø±ÙŠ ØªÙØ±ÙŠØº Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ...",
    )

    try:
        query = _transcribe_message_media(message)
        if not query:
            bot.edit_message_text(
                "Could not transcribe this audio. Please try again with clearer voice."
                if language == "en"
                else "Ù„Ù… Ø£Ø³ØªØ·Ø¹ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨ØµÙˆØª Ø£ÙˆØ¶Ø­.",
                chat_id=message.chat.id,
                message_id=thinking_msg.message_id,
            )
            return
        assistant_text = _run_interview_turn(project_id, query, language, source="telegram_voice")

        bot.edit_message_text(
            (
                "ðŸ“ You said:\n"
                f"{query}\n\n"
                f"ðŸ’¬ {assistant_text}"
            )
            if language == "en"
            else (
                "ðŸ“ Ø£Ù†Øª Ù‚Ù„Øª:\n"
                f"{query}\n\n"
                f"ðŸ’¬ {assistant_text}"
            ),
            chat_id=message.chat.id,
            message_id=thinking_msg.message_id,
            reply_markup=_build_session_actions_markup(language),
        )
    except Exception as exc:  # noqa: BLE001
        logger.error("Error in telegram voice flow: %s", exc)
        try:
            bot.edit_message_text(
                "âŒ Could not process this voice message right now."
                if language == "en"
                else "âŒ ØªØ¹Ø°Ù‘Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØµÙˆØªÙŠØ© Ø­Ø§Ù„ÙŠØ§Ù‹.",
                chat_id=message.chat.id,
                message_id=thinking_msg.message_id,
            )
        except Exception:
            logger.error("Could not send voice error reply to Telegram")


def handle_session_action(call):
    """Handle session action callbacks: session:add_more, session:end."""
    data = str(call.data or "")
    chat_id = int(call.message.chat.id)
    user_lang = _resolve_user_language(
        text_hint=str(getattr(call.message, "text", "") or ""),
        telegram_language_code=getattr(call.from_user, "language_code", None),
    )

    if data == "session:add_more":
        bot.answer_callback_query(
            call.id,
            "Continue adding requirements" if user_lang == "en" else "Ø£ÙƒÙ…Ù„ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª",
        )
        prompt = (
            "ðŸ’¡ Great! Please continue describing your requirements. "
            "You can type or send a voice message."
            if user_lang == "en"
            else "ðŸ’¡ Ù…Ù…ØªØ§Ø²! ÙƒÙ…Ù‘Ù„ ÙˆØµÙ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù„ÙŠ Ø¹Ø§ÙŠØ²Ù‡Ø§. "
            "ØªÙ‚Ø¯Ø± ØªÙƒØªØ¨ Ø£Ùˆ ØªØ¨Ø¹Øª Ø±Ø³Ø§Ù„Ø© ØµÙˆØªÙŠØ©."
        )
        bot.send_message(
            chat_id,
            prompt,
            reply_markup=_build_persistent_menu_markup(user_lang),
        )
        return

    if data == "session:end":
        project_id = get_chat_project(chat_id)
        bot.answer_callback_query(
            call.id,
            "Ending session..." if user_lang == "en" else "Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø©...",
        )

        # Generate SRS PDF before ending
        if project_id:
            progress = bot.send_message(
                chat_id,
                "â³ Generating your final SRS document before ending the session..."
                if user_lang == "en"
                else "â³ Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ù SRS Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù‚Ø¨Ù„ Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø©...",
            )
            _generate_and_send_srs_pdf(
                chat_id=chat_id,
                project_id=project_id,
                user_lang=user_lang,
                progress_message_id=progress.message_id,
            )

        # Send booking / end session message
        booking_markup = types.InlineKeyboardMarkup(row_width=1)
        if user_lang == "en":
            booking_markup.add(
                types.InlineKeyboardButton(
                    text="ðŸ“… Book an Appointment",
                    url="https://calendly.com",
                ),
                types.InlineKeyboardButton(
                    text="âž• Start a New Project",
                    callback_data="project:new",
                ),
            )
            end_text = (
                "âœ… Session ended successfully!\n\n"
                "ðŸ“„ Your SRS document has been sent above.\n\n"
                "You can book an appointment with our team to discuss "
                "your project further, or start a new project.\n\n"
                "Thank you for using Tawasul AI! ðŸ™"
            )
        else:
            booking_markup.add(
                types.InlineKeyboardButton(
                    text="ðŸ“… Ø­Ø¬Ø² Ù…ÙˆØ¹Ø¯",
                    url="https://calendly.com",
                ),
                types.InlineKeyboardButton(
                    text="âž• Ø¨Ø¯Ø¡ Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯",
                    callback_data="project:new",
                ),
            )
            end_text = (
                "âœ… ØªÙ… Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø© Ø¨Ù†Ø¬Ø§Ø­!\n\n"
                "ðŸ“„ ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„Ù Ø§Ù„Ù€ SRS Ø¨Ø§Ù„Ø£Ø¹Ù„Ù‰.\n\n"
                "ØªÙ‚Ø¯Ø± ØªØ­Ø¬Ø² Ù…ÙˆØ¹Ø¯ Ù…Ø¹ ÙØ±ÙŠÙ‚Ù†Ø§ Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ù…Ø´Ø±ÙˆØ¹Ùƒ Ø¨Ø´ÙƒÙ„ Ø£Ø¹Ù…Ù‚ØŒ "
                "Ø£Ùˆ ØªØ¨Ø¯Ø£ Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯.\n\n"
                "Ø´ÙƒØ±Ø§Ù‹ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ùƒ ØªÙˆØ§ØµÙ„ AI! ðŸ™"
            )

        bot.send_message(chat_id, end_text, reply_markup=booking_markup)

